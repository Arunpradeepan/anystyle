title         | ADAPTIVE MODEL REDUCTION TO ACCELERATE OPTIMIZATION
              | PROBLEMS GOVERNED BY PARTIAL DIFFERENTIAL EQUATIONS
blank         | 
              | 
              | 
              | 
text          |                    A DISSERTATION
              |            SUBMITTED TO THE INSTITUTE FOR
              |    COMPUTATIONAL AND MATHEMATICAL ENGINEERING
              |       AND THE COMMITTEE ON GRADUATE STUDIES
              |               OF STANFORD UNIVERSITY
              |      IN PARTIAL FULFILLMENT OF THE REQUIREMENTS
              |                  FOR THE DEGREE OF
              |                DOCTOR OF PHILOSOPHY
blank         | 
              | 
              | 
              | 
text          |                    Matthew Joseph Zahr
              |                        August 2016
              |                 © 2016 by Matthew Joseph Zahr. All Rights Reserved.
              |          Re-distributed by Stanford University under license with the author.
blank         | 
              | 
              | 
text          |                         This work is licensed under a Creative Commons Attribution-
              |                         Noncommercial 3.0 United States License.
              |                         http://creativecommons.org/licenses/by-nc/3.0/us/
blank         | 
              | 
              | 
              | 
text          | This dissertation is online at: http://purl.stanford.edu/bh989ww6442
blank         | 
              | 
              | 
              | 
meta          |                                           ii
text          | I certify that I have read this dissertation and that, in my opinion, it is fully adequate
              | in scope and quality as a dissertation for the degree of Doctor of Philosophy.
blank         | 
text          |                                                           Charbel Farhat, Primary Adviser
blank         | 
              | 
              | 
text          | I certify that I have read this dissertation and that, in my opinion, it is fully adequate
              | in scope and quality as a dissertation for the degree of Doctor of Philosophy.
blank         | 
text          |                                                                             Michael Saunders
blank         | 
              | 
              | 
text          | I certify that I have read this dissertation and that, in my opinion, it is fully adequate
              | in scope and quality as a dissertation for the degree of Doctor of Philosophy.
blank         | 
text          |                                                                              Per-Olof Persson
blank         | 
              | 
              | 
              | 
text          | Approved for the Stanford University Committee on Graduate Studies.
              |                              Patricia J. Gumport, Vice Provost for Graduate Education
blank         | 
              | 
              | 
              | 
text          | This signature page was generated electronically upon submission of this dissertation in
              | electronic format. An original signed hard copy of the signature page is on file in
              | University Archives.
blank         | 
              | 
              | 
              | 
meta          |                                           iii
title         | Abstract
blank         | 
text          | Optimization problems constrained by Partial Differential Equations (PDEs) are ubiquitous in mod-
              | ern science and engineering. They play a central role in optimal design and control of multiphysics
              | systems, as well as nondestructive evaluation and detection, and inverse problems. Methods to
              | solve these optimization problems rely on potentially many numerical solutions of the underlying
              | equations. For complicated physical interactions taking place on complex domains, these solutions
              | will be computationally expensive—in terms of both time and resources—to obtain, rendering the
              | optimization procedure difficult or intractable.
              |    This dissertation introduces a globally convergent, error-aware trust region algorithm for lever-
              | aging inexpensive approximation models to greatly reduce the cost of solving PDE-constrained
              | optimization problems in increasingly complex scenarios. While the trust region theory is general,
              | in that it is agnostic to the particular form of the approximation model, provided it possesses certain
              | properties, this work employs reduced-order models based on the method of snapshots and Proper
              | Orthogonal Decomposition (POD). The trust region algorithm proceeds by progressively refining
              | the fidelity of the reduced-order model while converging to the optimal solution. Thus, the reduced-
              | order model is trained exactly along the optimization trajectory, circumventing the task of training
              | in a potentially high-dimensional parameter space. The proposed method is shown to find the opti-
              | mal aerodynamic shape of a full aircraft configuration in about half the time required by accepted
              | methods.
              |    The proposed error-aware trust region algorithm is extended to handle the case where uncertain-
              | ties are present in the governing equations. In such situations, the goal is to find a design or control
              | that is risk-averse with respect to some quantity of interest. The objective function and constraints
              | in these problems usually correspond to integrals of quantities of interest over the stochastic space,
              | which will inevitably require many solutions of the underlying partial differential equation. For
              | this reason, dimension-adaptive sparse grids are combined with reduced-order models to define an
              | inexpensive approximation model, which is wrapped in the error-aware trust region framework to
              | ensure convergence to the optimal risk-averse solution. This framework is demonstrated on a model
              | problem from computational mechanics and shown to be several orders of magnitude faster than
              | existing methods.
blank         | 
              | 
              | 
              | 
meta          |                                                    iv
title         | Acknowledgments
blank         | 
text          | First and foremost, I would like to thank my advisor, Professor Charbel Farhat, for his advice
              | and guidance during the past five years. From the time you recruited me out of the AHPCRC
              | Summer Institute as an undergraduate through the completion of my PhD and job search, you
              | have been a professional role model and trusted mentor. I am also grateful for the perfect balance
              | between independence and supervision that you have provided as it has given me the opportunity
              | to explore other areas of computational mathematics and form external collaborations. I am very
              | proud to be able to call myself your student. I also want to thank Professor Per-Olof Persson –
              | my co-author, practicum advisor, thesis reader, coding buddy, and guide into the DG world – for
              | serving so many voluntary roles throughout my PhD. I would also like to thank Professor Sanjay
              | Govindjee and Professor Tarek Zohdi – my trusted mentors since my undergraduate days at UC
              | Berkeley – who have provided crucial support and guidance throughout my PhD and job search. I
              | am also very grateful to rest of my thesis committee – Professor Michael Saunders, Professor Walter
              | Murray, and Professor Louis Durlofsky – and my main source of funding – the Department of Energy
              | Computational Science Graduate Fellowship (DOE CSGF).
              |    I have had the pleasure to be in research lab with many talented and entertaining individuals.
              | I am very thankful to my FRG predecessor Kevin Carlberg who provided invaluable guidance and
              | assistance while I was an AHPCRC undergraduate intern and new graduate student, and remains a
              | trusted mentor and close collaborator to this day. I also want to sincerely thank my FRG labmates
              | – Dr Kyle Washabaugh, Dr Alex Main, Todd Chapman, and Raunak Borker – that made Durand
              | 028 not only an intellectually stimulating place, but also an entertaining and fun one; I will always
              | consider you among my closest friends and collaborators. Finally, I would like to acknowledge the
              | constant support and assistance I received from a number of other FRG-ers: Grace Fontanilla,
              | Tatiana Wilson, William Law, and Dr Philip Avery.
              |    Finally, I would like to thank my family and friends for their love, support, and patience, without
              | which, none of this would be possible. To my sweet dove and soon-to-be wife, Theresa Yates. To my
              | dad, Michael J. Zahr: my closest friend, most trusted mentor, and eternal role model. To my mom,
              | Tamara Bradley: the sweetest, most caring and supportive mother imaginable. To my grandpa and
              | grandma, Robert and Marlene Boranian: you are a constant source of love, encouragement, and sup-
              | port in life and the earliest investors in my education. To my sister, Emily Bradley, and stepfather,
              | Robert Bradley: you are, and always will be, a constant source of enjoyment and entertainment in
blank         | 
              | 
              | 
meta          |                                                   v
text          | my life. To my ski and camping buddies, Michael Gardner and Devon Laduzinsky. To my beloved
              | late uncle John “Jack” Zahr: you are a shining example of success in all phases of life – you will
              | forever be missed and remembered. To my second mother Sharee Eisenga; my soon-to-be in-laws,
              | Michael, Christine, and Rebecca Yates; and the entire Zahr, Hoffmann, and Bradley family.
blank         | 
text          | I dedicate this thesis to my future wife, Theresa Yates; parents, Michael Zahr and Tamara Bradley;
              |  grandparents, Robert and Marlene Boranian; step-father, Robert Bradley; sister, Emily Bradley;
              |                               and my late uncle, John “Jack” Zahr
blank         | 
              | 
              | 
              | 
meta          |                                                 vi
title         | Contents
blank         | 
text          | Abstract                                                                                                  iv
blank         | 
text          | Acknowledgments                                                                                           v
blank         | 
text          | 1 Introduction                                                                                             1
              |   1.1   Motivation    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    1
              |   1.2   Strategy and Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        3
              |   1.3   Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        3
              |         1.3.1   PDE-Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .           3
              |         1.3.2   Trust Region Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         5
              |         1.3.3   Projection-Based Model Reduction . . . . . . . . . . . . . . . . . . . . . . . .           8
              |         1.3.4   Surrogate Methods for PDE-Constrained Optimization . . . . . . . . . . . . .               9
              |   1.4   Thesis Accomplishments and Outline . . . . . . . . . . . . . . . . . . . . . . . . . . .          13
blank         | 
text          | 2 PDE-Constrained Optimization                                                                            16
              |   2.1   Parametrized Partial Differential Equations . . . . . . . . . . . . . . . . . . . . . . .         16
              |         2.1.1   Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      17
              |         2.1.2   Discretization: Parametrization . . . . . . . . . . . . . . . . . . . . . . . . . .       22
              |         2.1.3   Discretization: Governing Equations . . . . . . . . . . . . . . . . . . . . . . .         26
              |         2.1.4   Discretization: Quantities of Interest . . . . . . . . . . . . . . . . . . . . . . .      32
              |   2.2   Parametrized Stochastic Partial Differential Equations . . . . . . . . . . . . . . . . .          34
              |         2.2.1   Risk Measures of Quantities of Interest       . . . . . . . . . . . . . . . . . . . . .   35
              |         2.2.2   Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      36
              |         2.2.3   Finite-Dimensional Approximation . . . . . . . . . . . . . . . . . . . . . . . .          37
              |   2.3   PDE-Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          38
              |         2.3.1   Continuous vs. Discrete Formulation . . . . . . . . . . . . . . . . . . . . . . .         40
              |         2.3.2   Full Space vs. Reduced Space Approach . . . . . . . . . . . . . . . . . . . . .           43
              |         2.3.3   Sensitivity Method for Computing Gradients . . . . . . . . . . . . . . . . . .            44
              |         2.3.4   Adjoint Method for Computing Gradients . . . . . . . . . . . . . . . . . . . .            45
              |         2.3.5   Optimization Problems with Side Constraints . . . . . . . . . . . . . . . . . .           48
blank         | 
              | 
              | 
meta          |                                                    vii
text          | 3 Generalized Multifidelity Trust Region Method                                                        50
              |   3.1   Unconstrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      51
              |         3.1.1   Error-Aware Multifidelity Trust Region Method . . . . . . . . . . . . . . . . .         51
              |         3.1.2   Interior-Point Method for Trust Region Subproblem . . . . . . . . . . . . . .           62
              |         3.1.3   Numerical Experiment: Contrived . . . . . . . . . . . . . . . . . . . . . . . .         63
              |   3.2   Nonlinearly Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .        70
              |         3.2.1   Error-Aware Augmented Lagrangian Multifidelity Trust Region Method . . .                71
              |         3.2.2   Numerical Experiment: Contrived . . . . . . . . . . . . . . . . . . . . . . . .         72
blank         | 
text          | 4 Projection-Based Model Reduction                                                                     77
              |   4.1   Global Reduced-Order Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       78
              |         4.1.1   Primal Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      78
              |         4.1.2   Exact and Minimum-Residual Sensitivity Formulation . . . . . . . . . . . . .            82
              |         4.1.3   Exact and Minimum-Residual Adjoint Formulation . . . . . . . . . . . . . . .            87
              |   4.2   Global Hyperreduced Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        92
              |         4.2.1   Precomputation for Polynomial Nonlinearities . . . . . . . . . . . . . . . . . .        93
              |         4.2.2   Mask and Sample Mesh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        95
              |         4.2.3   Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    96
              |         4.2.4   Minimum-Residual Primal Formulation . . . . . . . . . . . . . . . . . . . . .           98
              |         4.2.5   Exact and Minimum-Residual Sensitivity Formulation . . . . . . . . . . . . . 100
              |         4.2.6   Adjoint Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
              |   4.3   Construction of Reduced-Order Basis and Residual Mask . . . . . . . . . . . . . . . 106
              |   4.4   Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
blank         | 
text          | 5 Optimization via Model Reduction and Residual-Based Trust Regions                                    116
              |   5.1   Residual-Based Trust Region Method . . . . . . . . . . . . . . . . . . . . . . . . . . 117
              |         5.1.1   Multifidelity Trust Region Ingredients . . . . . . . . . . . . . . . . . . . . . . 117
              |         5.1.2   Basis Construction via Proper Orthogonal Decomposition and the Method of
              |                 Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
              |   5.2   Snapshots from Partially Converged Solutions . . . . . . . . . . . . . . . . . . . . . . 128
              |   5.3   Efficient Trust Region Assessment with Partially Converged Solutions . . . . . . . . 130
              |   5.4   Extension to Hyperreduced Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
              |   5.5   Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
              |         5.5.1   Optimal Control of 1D Inviscid Burgers’ Equation . . . . . . . . . . . . . . . 135
              |         5.5.2   Optimal Control of 1D Viscous Burgers’ Equation . . . . . . . . . . . . . . . 144
              |         5.5.3   Shape Optimization of Airfoil in Inviscid, Subsonic Flow . . . . . . . . . . . . 150
              |         5.5.4   Shape Optimization of the Common Research Model in Viscous, Turbulent Flow158
blank         | 
              | 
              | 
              | 
meta          |                                                   viii
text          | 6 Model Reduction and Sparse Grids for Efficient Stochastic Optimization                             170
              |   6.1   Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
              |         6.1.1   Stochastic High-Dimensional Model . . . . . . . . . . . . . . . . . . . . . . . 171
              |         6.1.2   Stochastic Reduced-Order Model . . . . . . . . . . . . . . . . . . . . . . . . . 173
              |         6.1.3   Anisotropic Sparse Grids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
              |   6.2   Two Levels of Approximation of Risk-Averse Measures . . . . . . . . . . . . . . . . . 180
              |   6.3   Multifidelity Trust Region Method Based on Two-Level Approximation . . . . . . . 183
              |         6.3.1   Trust Region Ingredients     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
              |         6.3.2   Greedy Construction of Sparse Grid and Reduced Basis . . . . . . . . . . . . 187
              |         6.3.3   Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
              |   6.4   Numerical Experiment: Optimal Control of the Viscous Burgers’ Equation with Un-
              |         certain Coefficients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
blank         | 
text          | 7 Conclusions                                                                                        205
              |   7.1   Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
              |   7.2   Prospective Future Work      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
blank         | 
text          | A Global Convergence Proof: Error-Aware Trust Region Method                                          209
blank         | 
text          | B Residual-Based Error Bounds                                                                        215
blank         | 
text          | C Adaptive State and Parameter Space Reduction for Large-Scale Optimization 224
              |   C.1 Two-Level Nested Reduction of Parametrized Partial Differential Equations . . . . . 226
              |         C.1.1 Outer Layer of Reduction: Restriction of Parameter Space . . . . . . . . . . . 226
              |         C.1.2 Inner Layer of Reduction: Projection-Based Model Reduction . . . . . . . . . 229
              |   C.2 Globally Convergent Multifidelity Trust Region Method . . . . . . . . . . . . . . . . 231
              |         C.2.1 Outer Iteration: Globally Convergent Parameter Space Adaptation . . . . . . 231
              |         C.2.2 Inner Iteration: Multifidelity Optimization with Reduced-Order Models . . . 234
blank         | 
text          | D Time-Dependent PDE-Constrained Optimization under Periodicity Constraints240
              |   D.1 Governing Equations and Discretization . . . . . . . . . . . . . . . . . . . . . . . . . 240
              |         D.1.1 System of Conservation Laws on Deforming Domain: Arbitrary Lagrangian-
              |                 Eulerian Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
              |         D.1.2 Arbitrary Lagrangian-Eulerian Discontinuous Galerkin Method . . . . . . . . 242
              |   D.2 Fully Discrete, Time-Dependent Adjoint Equations . . . . . . . . . . . . . . . . . . . 245
              |         D.2.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
              |         D.2.2 Parametrization of Initial Condition . . . . . . . . . . . . . . . . . . . . . . . 248
              |         D.2.3 Benefits of Fully Discrete Framework . . . . . . . . . . . . . . . . . . . . . . . 249
              |         D.2.4 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
              |         D.2.5 Numerical Experiment: Energetically Optimal Trajectory of 2D Airfoil in
              |                 Compressible, Viscous Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
blank         | 
              | 
meta          |                                                    ix
text          |        D.2.6 Numerical Experiment: Energetically Optimal Shape and Flapping Motion of
              |                2D Airfoil at Constant Impulse . . . . . . . . . . . . . . . . . . . . . . . . . . 263
              |   D.3 Computing Time-Periodic Solutions of Partial Differential Equations . . . . . . . . . 272
              |        D.3.1 Numerical Solvers: Shooting Methods . . . . . . . . . . . . . . . . . . . . . . 273
              |        D.3.2 Stability of Periodic Orbits of Fully Discrete Partial Differential Equations . 276
              |   D.4 Fully Discrete, Time-Periodic Adjoint Method . . . . . . . . . . . . . . . . . . . . . . 278
              |        D.4.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
              |        D.4.2 Numerical Solver: Matrix-Free Krylov Method . . . . . . . . . . . . . . . . . 280
              |        D.4.3 Generalized Reduced-Gradient Method for PDE Optimization with Time-
              |                Periodicity Constraints    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
              |        D.4.4 Numerical Experiment: Time-Periodic Solutions of the Compressible Navier-
              |                Stokes Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
              |        D.4.5 Numerical Experiment: Energetically Optimal Flapping with Thrust and Time-
              |                Periodicity Constraints    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
              |   D.5 Conclusion     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
              |   D.6 Existence and Uniqueness of Solutions of the Adjoint Equations of the Fully Discrete,
              |        Time-Periodically Constrained Partial Differential Equations . . . . . . . . . . . . . 299
blank         | 
text          | Bibliography                                                                                         301
blank         | 
              | 
              | 
              | 
meta          |                                                   x
title         | List of Tables
blank         | 
text          |  2.1   Butcher Tableau for s-stage diagonally implicit Runge-Kutta scheme . . . . . . . . .                 31
blank         | 
text          |  3.1   Convergence history of Algorithm 1 applied to the Rosenbrock problem. . . . . . . .                  69
              |  3.2   Convergence history of Algorithm 1 applied to the constrained problem (3.49). Iter-
              |        ations 0 − 9: τ0 = 10−4 , iterations 10 − 19: τ1 = 10−5 , iterations 20 − 29: τ2 = 10−6 .
              |        The norm of the gradient of Lτj (µ), for fixed τj , decreases 3 − 4 orders of magnitude
              |        throughout the iterations despite the values of Lτj (µk ) and mk (µk ) or Lτj (µ̂k ) and
              |        mk (µ̂k ) not being close until near convergence. . . . . . . . . . . . . . . . . . . . . .          76
blank         | 
text          |  5.1   Variants of the multifidelity trust region method based on projection-based reduced-
              |        order models introduced in Algorithms 11 and 12. The first three methods are not
              |        guaranteed to be globally convergent since they do not necessarily satisfy the gradient
              |        condition (3.15). The methods that employ the traditional trust region employ two
              |        trust region subproblem solvers: an exact solver based on the interior point method
              |        in Algorithm 3 and the inexact Steihaug-Toint CG solver. The methods that employ
              |        the residual-based trust region rely on the exact interior point solver in Algorithm 3.
              |        The interior point solver considered in this section uses Newton-CG to solve the un-
              |        constrained subproblem (instead of BFGS) for fair comparison with the second-order
              |        Steihaug-Toint CG. The snapshot matrices Uk , Wk , Zk consist of state, sensitiv-
              |        ity, and adjoint snapshots, respectively, of the high-dimensional model at all previous
              |        trust region centers, i.e., µ0 , . . . , µk−1 .    . . . . . . . . . . . . . . . . . . . . . . . . . 141
              |  5.2   Convergence history of Algorithm 11 applied to optimal control of the inviscid Burg-
              |        ers’ equation using method ‘sens-etr-intpt’ using reduced-order models based on a
              |        Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
              |  5.3   Convergence history of Algorithm 12 applied to optimal control of the inviscid Burg-
              |        ers’ equation using method ‘sens-etr-intpt’ using reduced-order models based on a
              |        Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
              |  5.4   Convergence history of Algorithm 11 applied to optimal control of the inviscid Burg-
              |        ers’ equation using method ‘sens-ctr-stcg’ using reduced-order models based on a
              |        Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
blank         | 
              | 
              | 
meta          |                                                          xi
text          | 5.5   Convergence history of Algorithm 12 applied to optimal control of the inviscid Burg-
              |       ers’ equation using method ‘sens-ctr-stcg’ using reduced-order models based on a
              |       Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
              | 5.6   Convergence history of Algorithm 11 applied to optimal control of the viscous Burg-
              |       ers’ equation using method ‘dual-etr-intpt’ using reduced-order models based on a
              |       Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
              | 5.7   Convergence history of Algorithm 11 applied to optimal control of the viscous Burg-
              |       ers’ equation using method ‘dual-ctr-intpt’ using reduced-order models based on a
              |       Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
              | 5.8   Performance of the HDM- and ROM-based optimization methods. . . . . . . . . . . 169
blank         | 
text          | 6.1   Convergence history of Algorithm 15 applied to the optimal control of the stochastic
              |       Burgers’ equation in (6.67). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
              | 6.2   Convergence history of Algorithm 16 applied to the optimal control of the stochastic
              |       Burgers’ equation in (6.67). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
blank         | 
text          | D.1 Butcher Tableau for 3-stage, 3rd order DIRK scheme [3]
              |                                          2
              |                                                              6α2 −20α+5
              |       α = 0.435866521508459, γ = − 6α        −16α+1
              |                                               4     ,   ω=        4     .   . . . . . . . . . . . . . . . 254
              | D.2 Summary of parametrizations considered in Section D.2.5. The number of clamped
              |       cubic spline knots used to discretize x(t), y(t), and θ(t) are mx + 1, my + 1, and
              |       mθ , respectively. PI freezes the rigid body translation (mx = my = 0) and optimizes
              |       over only the rotation (mθ 6= 0). PII optimizes over all rigid body degrees of freedom
              |       (mx = my = mθ 6= 0). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
              | D.3 Table summarizing integrated quantities of interest at optimal solution of (D.44) for
              |       each parametrization (PI, PII) for each level of refinement. The total work monoton-
              |       ically increases as Nµ increases for a given parametrization, which is expected due to
              |       the nested search spaces. For a fixed ID, the optimal total work for parametrization
              |       PII is larger than that for PI since the search space for PI is a subset of that of
              |       PII. The other integrated quantities are included for completeness, but do not exhibit
              |       trends (except for converging to a fixed value as Nµ increases) since they were not
              |       included in the optimization problem. . . . . . . . . . . . . . . . . . . . . . . . . . . 261
              | D.4 Summary of parametrizations considered in Section D.2.6. The number of periodic
              |       cubic spline knots used to discretize y(t), θ(t), and ¸(t) are my + 1, mθ + 1, and
              |       mc +1, respectively. FI freezes the airfoil shape and considers only rigid body motions
              |       (my = mθ 6= 0, mc = 0). FII parametrizes both shape and kinematic motion (my =
              |       mθ = mc 6= 0). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
blank         | 
              | 
              | 
              | 
meta          |                                                   xii
text          | D.5 Table summarizing integrated quantities of interest at optimal solution of each op-
              |      timization problem for each impulse level. In all cases, the desired value of Jx is
              |      achieved to greater than 4 digits of accuracy. The optimal solution for larger values
              |      of the impulse constraint require more total work to complete flapping motion, i.e.,
              |      work monotonically increases in magnitude as value of impulse constraint increases.
              |      Smaller values of total work are achievable if airfoil is allowed to morph its shape
              |      in addition its rigid body motion. The other integrated quantities are included for
              |      completeness, but do not exhibit trends since they were not in the optimization problem.268
              | D.6 Table summarizing performance of numerical solvers for fully discrete time-periodic
              |      partial differential equations, considering nonlinear preconditioning via m fixed point
              |      iterations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
              | D.7 Comparison of non-zero derivatives of total energy, W , and x-impulse, Jx , computed
              |      with the adjoint method and a second-order finite difference approximation with step
              |      size τ = 10−6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
blank         | 
              | 
              | 
              | 
meta          |                                                 xiii
title         | List of Figures
blank         | 
text          |  1.1   The adaptive approach to accelerate PDE-constrained optimization with projection-
              |        based reduced-order models. Top left: block schematic of the workflow where few
              |        High-Dimensional Model (HDM) samples are compressed to build the Reduced-Order
              |        Basis (ROB) and the resulting Reduced-Order Model (ROM) is used in the optimiza-
              |        tion procedure, as long as it maintains accuracy. When the accuracy degrades, an
              |        additional sample of the HDM is taken at the new point in the parameter space and
              |        the ROB is enriched. Top right: schematic of parameter space (µ-space) where the
              |        black dot and star are the initial guess and solution of the optimization problem,
              |        respectively, the red circles indicate HDM samples, the gray regions are the “trust
              |        regions” for the ROM constructed at each iteration, the blue line is the trajectory
              |        of the ROM optimization procedure, and the blue star is the optimal solution found
              |        by the ROM optimization. Bottom: schematic of the computational cost where the
              |        expensive (HDM evaluations and ROB construction) and inexpensive components
              |        are intermixed throughout the algorithm. These methods are usually equipped with
              |        global convergence theory that guarantee convergence to a local optimum of the PDE-
              |        constrained optimization problem, as indicated in the top right plot. . . . . . . . . .   11
blank         | 
              | 
              | 
              | 
meta          |                                                xiv
text          | 1.2   The offline-online approach to accelerate PDE-constrained optimization with projection-
              |       based reduced-order models. Top left: block schematic of the workflow where a
              |       number of High-Dimensional Model (HDM) samples are compressed to build the
              |       Reduced-Order Basis (ROB) in an offline phase; the resulting inexpensive Reduced-
              |       Order Model (ROM) is repeatedly queried in the online optimization phase. Top
              |       right: schematic of parameter space (µ-space) where the black dot and star are the
              |       initial guess and solution of the optimization problem, respectively, the red circles
              |       indicate HDM samples, the blue line is the trajectory of the ROM optimization pro-
              |       cedure, and the blue star is the optimal solution found by the ROM optimization.
              |       Bottom: schematic of the computational cost where there is a clear distinction be-
              |       tween the expensive components (HDM evaluations and ROB construction) that are
              |       done once-and-for-all in the offline phase and the inexpensive components (ROM eval-
              |       uations) that are repeatedly queried in the online phase. In general, these methods are
              |       not guaranteed to converge to a local optimum of the PDE-constrained optimization
              |       problem, as indicated in the top right plot. . . . . . . . . . . . . . . . . . . . . . . .       12
              | 1.3   Organization of thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     15
blank         | 
text          | 2.1   Left: Undeformed NACA0012 airfoil and surrounding triangular mesh. Right: Defor-
              |       mation of R2 according to mapping ϕ in (2.33) that deforms the NACA0012 geometry
              |       and surrounding mesh. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      25
              | 2.2   Top left: Undeformed geometry of a circle (blue) and a FFD lattice (gray). Top
              |       center : Perturbation of FFD control nodes according to an x-directed elongation
              |       mode and resulting shape of the circle. Top right: Perturbation of FFD control
              |       nodes according to a bending mode and resulting shape of the circle. Bottom: Local
              |       perturbations to individual FFD control nodes in the y direction and the resulting
              |       shape of the circle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   26
              | 2.3   Free form deformation lattices and Volkswagen Passat geometry: (left) undeformed
              |       configuration, (top right) deformed configuration with lowered roof, and (bottom right)
              |       deformed configuration with steeply tapered trunk. . . . . . . . . . . . . . . . . . . .         27
              | 2.4   Free form deformation lattice and Common Research Model (CRM) geometry: (left)
              |       undeformed configuration and (right) deformed configuration with positive dihedral.              27
              | 2.5   Shape parametrization of a NACA0012 airfoil using a cubic design element. Blue
              |       nodes and lines designate the undeformed design element and shape and black nodes
              |       and lines designate the deformed design element and shape. . . . . . . . . . . . . . .           28
              |                                                       2
              | 2.6   Left: Quadrilateral mesh of a subset of R corresponding to a rectangle (160 × 100
              |       elements) whose topology is parametrized by a density-based method. Right: An
              |       example of an admissible topology of the density-based topological parametrization—
              |       an optimized cantilever designed to maximize the global stiffness of the structure
              |       under a vertical load at the right end. . . . . . . . . . . . . . . . . . . . . . . . . . .      29
blank         | 
              | 
              | 
meta          |                                                  xv
text          | 2.7   Left: Quadrilateral mesh of a subset of R2 corresponding to a rectangle (160×100 ele-
              |       ments) with a hole whose topology is parametrized by a density-based method. Right:
              |       An example of an admissible topology of the density-based topological parametrization—
              |       a Michell structure [37, 94] designed to maximize the global stiffness of the structure
              |       under a vertical load at the right end. . . . . . . . . . . . . . . . . . . . . . . . . . .         29
              |                                                  3
              | 2.8   Left: Hexahedral mesh of a subset of R corresponding to a cube (35×35×35 elements)
              |       whose topology is parametrized by a density-based method. Right: An example of
              |       an admissible topology of the density-based topological parametrization—a trestle
              |       designed to maximize the global stiffness of the structure under a vertical load. . . .             29
              | 2.9   Left: Tetrahedral mesh of a subset of R3 corresponding to an unoptimized lacrosse
              |       head (475, 666 elements) whose topology is parametrized by a density-based method.
              |       Right: An example of an admissible topology of the density-based topological parametrization—
              |       an unconverged maximum stiffness topology. The entire object is included in the top
              |       row and the bottom row is a slice to show internal voids in the optimized shape. . . .              30
blank         | 
text          | 3.1   Geometry of trust region constraint in special case where ϑk = kAk (µ − µk )k2 =
              |       kµ − µk kAT Ak . The eigenvalue decomposition of ATk Ak is ATk Ak = Qk Λk QTk with
              |                    k
blank         | 
text          |       eigenvectors qi = Qk ei and eigenvalues λi = eTi Λk ei . . . . . . . . . . . . . . . . . . .        53
              |                                                                               4     3
              | 3.2   Logarithmic barrier function (3.27) corresponding to mk (x) = x − x (                ), ϑk (x) =
              |        2
              |       x , ∆k = 1 with γ = 0.1 (        ) and γ = 0.0001 (        ).   . . . . . . . . . . . . . . . . .   62
              | 3.3   Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33). The contours
              |       represent the true function F (µ), the red dots indicate trust region centers, and the
              |       blue line is the trajectory of the trust region subproblem. . . . . . . . . . . . . . . .           66
              | 3.4   Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33); iterations
              |       proceed from left to right then top to bottom. The contours represent the true
              |       function F (µ), the red dots indicate trust region centers µk , the blue dots are the
              |       candidate for the next trust region center µ̂k , and the green region indicates the
              |       feasible set for the trust region subproblem. . . . . . . . . . . . . . . . . . . . . . . .         67
              | 3.5   Convergence history of the objective quantities using Algorithm 1: F (µk ) (                  ),
              |       F (µ̂k ) (       ), mk (µk ) (   ), mk (µ̂k ) (      ). Steady progress is made toward the
              |       optimal solution, despite the objective and model only agreeing at iteration 0. . . . .             68
              | 3.6   Convergence history of gradient quantities using Algorithm 1: k∇F (µk )k (                    ),
              |       k∇F (µ̂k )k (        ), k∇mk (µk )k (     ). The gradient of the true objective function
              |       decreases 6 orders of magnitude. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          68
              | 3.7   Trajectory of Algorithm 1 as applied to the constrained problem (3.49). The contours
              |       represent the true function F (µ), the red dots indicate trust region centers, and the
              |       blue line is the trajectory of the trust region subproblem. . . . . . . . . . . . . . . .           73
blank         | 
              | 
              | 
              | 
meta          |                                                      xvi
text          | 3.8   Trajectory of Algorithm 1 as applied to the constrained problem (3.49) embedded
              |       in the augmented Lagrangian framework. The contours represent the true function
              |       F (µ), the red dots indicate trust region centers µk , the blue dots are the candidate
              |       for the next trust region center µ̂k , and the green region indicates the feasible set for
              |       the trust region subproblem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   74
              | 3.9   Convergence history of the augmented Lagrangian objective quantities using Algo-
              |       rithm 1: Lτj (µk ) (      ), Lτj (µ̂k ) (   ), mk (µk ) (   ), mk (µ̂k ) (   ). The three
              |       augmented Lagrangian iterations are separated by a vertical dashed line with the
              |       following penalty parameters: τ0 = 10−4 (iterations 0 − 9), τ1 = 10−5 (iterations
              |       10 − 19), τ2 = 10−6 (iterations 20 − 29). . . . . . . . . . . . . . . . . . . . . . . . . .    75
              | 3.10 Convergence history of the augmented Lagrangian gradient quantities using Algo-
              |       rithm 1: k∇Lτj (µk )k (       ), k∇Lτj (µ̂k )k (    ), k∇mk (µk )k (     ). The three aug-
              |       mented Lagrangian iterations are separated by a vertical dashed line with the follow-
              |       ing penalty parameters: τ0 = 10−4 (iterations 0 − 9), τ1 = 10−5 (iterations 10 − 19),
              |       τ2 = 10−6 (iterations 20−29). For each augmented Lagrangian iteration, the gradient
              |       of the true augmented Lagrangian (for fixed τj ) decreases 3 − 4 orders of magnitude.          75
blank         | 
text          | 5.1   Control (left) and corresponding solution (right) of the inviscid Burgers’ equation
              |       in (5.56) at: the initial condition µ = (1.0, 1.0, 0.0) (      ), the target solution µ =
              |       (2.5, 0.02, 0.0425) (   ), and solution of the baseline optimization method (         ). . . 136
              | 5.2   Contours of the objective function f (u(µ), µ) in (5.55) in the µ1 − µ2 plane corre-
              |       sponding to a slice at µ3 = 0.0. The initial condition for the optimization problem
              |       and target solution are shown with a red circle and blue square, respectively. . . . . 138
              | 5.3   Contour of the reduced objective function f (Φur (µ; Φ, Ψ), µ) in (5.55) in the µ1 −µ2
              |       plane corresponding to a slice at µ3 = 0.0. The reduced-order model employs a
              |       Galerkin projection and the trial basis is constructed from: (top) the primal solution
              |       at µ0 , i.e., col(Φ) = span{u(µ0 )}; (middle) the primal and adjoint solution at µ0 , i.e.,
              |       col(Φ) = span{u(µ
              |                       0 ), λ(µ0 )}; (bottom)
              |                                              the primal and sensitivity solution at µ0 , i.e.,
              |                               ∂u
              |       col(Φ) = span u(µ0 ),       (µ ) . The green shaded region indicates the areas where:
              |                               ∂µ 0
              |       (left) the Euclidean ball is bounded by 0.5, i.e., kµ − µ0 k ≤ 0.5, (center) the error
              |       between the true and reduced objective function is bounded by 100, i.e., |f (u(µ), µ)−
              |       f (Φur (µ; Φ, Ψ), µ)| ≤ 100, and (right) the residual norm of the reconstructed ROM
              |       solution is bounded by 10, i.e., kr(Φur (µ; Φ, Ψ), µ)k ≤ 10. The initial condition for
              |       the optimization problem and target solution are shown with a red circle and blue
              |       square, respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
blank         | 
              | 
              | 
              | 
meta          |                                                   xvii
text          | 5.4   Contour of the reduced objective function f (Φur (µ; Φ, Ψ), µ) in (5.55) in the µ1 −µ2
              |       plane corresponding to a slice at µ3 = 0.0. The reduced-order model employs a
              |       LSPG projection and the trial basis is constructed from: (top) the primal solution at
              |       µ0 , i.e., col(Φ) = span{u(µ0 )}; (middle) the primal and adjoint solution at µ0 , i.e.,
              |       col(Φ) = span{u(µ
              |                       0 ), λ(µ0 )}; (bottom)
              |                                              the primal and sensitivity solution at µ0 , i.e.,
              |                               ∂u
              |       col(Φ) = span u(µ0 ),       (µ ) . The green shaded region indicates the areas where:
              |                               ∂µ 0
              |       (left) the Euclidean ball is bounded by 0.5, i.e., kµ − µ0 k ≤ 0.5, (center) the error
              |       between the true and reduced objective function is bounded by 100, i.e., |f (u(µ), µ)−
              |       f (Φur (µ; Φ, Ψ), µ)| ≤ 100, and (right) the residual norm of the reconstructed ROM
              |       solution is bounded by 10, i.e., kr(Φur (µ; Φ, Ψ), µ)k ≤ 10. The initial condition for
              |       the optimization problem and target solution are shown with a red circle and blue
              |       square, respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
              | 5.5   Convergence history of various optimization solvers for optimal control of the invis-
              |       cid Burgers’ equation when Galerkin reduced-order model defines the approximation
              |       model. Optimization solvers considered: L-BFGS solver with only HDM evaluations
              |       (    ), prim-etr-intpt (      ), prim-ctr-intpt (      ), prim-ctr-stcg (      ), sens-etr-intpt
              |       (    ), sens-ctr-intpt (       ), sens-ctr-stcg (      ), adj-etr-intpt (       ), adj-ctr-intpt
              |       (    ), adj-ctr-stcg (      ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
              | 5.6   Convergence history of various optimization solvers for optimal control of the inviscid
              |       Burgers’ equation when LSPG reduced-order model defines the approximation model.
              |       Optimization solvers considered: L-BFGS solver with only HDM evaluations (                      ),
              |       prim-etr-intpt (      ), prim-ctr-intpt (      ), prim-ctr-stcg (      ), sens-etr-intpt (      ),
              |       sens-ctr-intpt (      ), sens-ctr-stcg (    ), adj-etr-intpt (      ), adj-ctr-intpt (   ), adj-
              |       ctr-stcg (      ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
              | 5.7   Left: Cumulative number of primal ROM queries as a function of major iteration in
              |       the trust region algorithm based on reduced-order models (Algorithm 11) as applied
              |       to optimal control of the inviscid Burgers’ equation. Right: Histogram of the number
              |       of primal ROM queries at a given basis size. Data separated into the top and bottom
              |       rows to deal with the disparate x-scales. All reduced-order models use a Galerkin pro-
              |       jection. Optimization solvers considered: prim-etr-intpt (            ), prim-ctr-intpt (       ),
              |       prim-ctr-stcg (       ), sens-etr-intpt (     ), sens-ctr-intpt (       ), sens-ctr-stcg (      ),
              |       adj-etr-intpt (      ), adj-ctr-intpt (     ), adj-ctr-stcg (     ). . . . . . . . . . . . . . . 143
              | 5.8   Convergence of the objective function (left) and gradient (right) as a function of
              |       the cost metric in (5.58) for several values of the speedup factor of the reduced-
              |       order model: τ = 20 (top row), τ = 50 (middle row), τ = ∞ (bottom row) for
              |       optimal control of the inviscid Burgers’ equation. All reduced-order models use a
              |       Galerkin projection. Optimization solvers considered: L-BFGS solver with only HDM
              |       evaluations (      ), sens-etr-intpt (      ), sens-ctr-intpt (     ), sens-ctr-stcg (       ). . . 145
blank         | 
              | 
              | 
              | 
meta          |                                                   xviii
text          | 5.9   Convergence history of the objective quantities for optimal control of the inviscid
              |       Burgers’ equation using Algorithm 11 (left – fully converged solutions as snapshots and
              |       in the evaluation of trust region steps) and Algorithm 12 (right – partially converged
              |       solutions as snapshots and in the evaluation of trust region steps): F (µk ) (                        ),
              |       F (µ̂k ) (      ), mk (µk ) (       ), mk (µ̂k ) (         ). The variant ‘sens-etr-intpt’ (Table 5.1)
              |       of the multifidelity trust region algorithm with Galerkin-based reduced-order models
              |       is used. Since the approximation model in the left plot is first-order consistent at
              |       trust region centers, mk (µk ) is omitted. . . . . . . . . . . . . . . . . . . . . . . . . . 146
              | 5.10 Convergence history of the gradient quantities for optimal control of the inviscid
              |       Burgers’ equation using Algorithm 11 (left – fully converged solutions as snapshots and
              |       in the evaluation of trust region steps) and Algorithm 12 (right – partially converged
              |       solutions as snapshots and in the evaluation of trust region steps): k∇F (µk )k (                     ),
              |       k∇F (µ̂k )k (       ), k∇mk (µk )k (         ), k∇mk (µ̂k )k (         ). The variant ‘sens-etr-intpt’
              |       (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based reduced-
              |       order models is used. Since the approximation model in the left plot is first-order
              |       consistent at trust region centers, k∇mk (µk )k is omitted. . . . . . . . . . . . . . . . 146
              | 5.11 Convergence history of the constraint quantities for optimal control of the inviscid
              |       Burgers’ equation using Algorithm 11 (left – fully converged solutions as snapshots and
              |       in the evaluation of trust region steps) and Algorithm 12 (right – partially converged
              |       solutions as snapshots and in the evaluation of trust region steps): ϑk (µk ) (                       ),
              |       ϑk (µ̂k ) (     ), ∆k (         ). The variant ‘sens-etr-intpt’ (Table 5.1) of the multifidelity
              |       trust region algorithm with Galerkin-based reduced-order models is used. . . . . . . 147
              | 5.12 Control (left) and corresponding solution (right) of the viscous Burgers’ equation
              |       in (5.60) at: the initial guess for the optimization problem (                      ) and the optimal
              |       solution of (5.59) (        ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
              | 5.13 Convergence history of various optimization solvers for optimal control of the vis-
              |       cous Burgers’ equation when Galerkin reduced-order model defines the approximation
              |       model. Optimization solvers considered: L-BFGS solver with only HDM evaluations
              |       (     ), adj-etr-intpt (         ), adj-ctr-intpt (          ), adj-ctr-stcg (     ). . . . . . . . . . . 148
              | 5.14 Left: Cumulative number of primal ROM queries as a function of major iteration in
              |       the trust region algorithm based on reduced-order models (Algorithm 11) as applied
              |       to optimal control of the viscous Burgers’ equation. Right: Histogram of the number
              |       of primal ROM queries at a given basis size. Data separated into the top and bottom
              |       rows to deal with the disparate x-scales. All reduced-order models use a Galerkin
              |       projection. Optimization solvers considered: adj-etr-intpt (                     ), adj-ctr-intpt (   ),
              |       adj-ctr-stcg (       ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
blank         | 
              | 
              | 
              | 
meta          |                                                            xix
text          | 5.15 Convergence of the objective function (left) and gradient (right) as a function of
              |      the cost metric in (5.61) for several values of the speedup factor of the reduced-
              |      order model: τ = 50 (top row), τ = 100 (middle row), τ = ∞ (bottom row) for
              |      optimal control of the viscous Burgers’ equation. All reduced-order models use a
              |      Galerkin projection. Optimization solvers considered: L-BFGS solver with only HDM
              |      evaluations (      ), adj-etr-intpt (    ), adj-ctr-intpt (    ), adj-ctr-stcg (    ).   . . . 151
              | 5.16 Convergence history of the objective (left) and gradient (right) quantities for optimal
              |      control of the viscous Burgers’ equation using Algorithm 11 (fully converged solutions
              |      as snapshots and in the evaluation of trust region steps). Left: |F (µk ) − F (µ∗ )|
              |      (    ), |F (µ̂k ) − F (µ∗ )| (   ), |mk (µ̂k ) − F (µ∗ )| (   ). Right: k∇F (µk )k (       ),
              |      k∇F (µ̂k )k (     ), k∇mk (µ̂k )k (     ). The variant ‘adj-etr-intpt’ (Table 5.1) of the
              |      multifidelity trust region algorithm with Galerkin-based reduced-order models is used.
              |      Since the approximation model is first-order consistent at trust region centers mk (µk )
              |      and k∇mk (µk )k are omitted. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
              | 5.17 Shape parametrization of a NACA0012 airfoil using a cubic design element (the no-
              |      tation µi designates the i-th component of the vector µ which refers to the i-th
              |      displacement degree of freedom of the shape parametrization) . . . . . . . . . . . . . 153
              | 5.18 NACA0012 mesh and pressure distribution at Mach 0.5 and zero angle of attack. . . 154
              | 5.19 Cub-RAE2822 mesh and pressure isolines computed at Mach 0.5 and zero angle of
              |      attack. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
              | 5.20 Progression of the objective function during the HDM-based optimization. The initial
              |      guess is defined as the 0th optimization iteration. . . . . . . . . . . . . . . . . . . . . 156
              | 5.21 Subsonic inverse design of the airfoil Cub-RAE2822: initial shape (NACA0012) and
              |      associated Cp function, and final shape (Cub-RAE2822) and associated Cp functions
              |      delivered by the HDM- and ROM-based optimizations, respectively. . . . . . . . . . 157
              | 5.22 Objective function versus number of queries to the HDM: ROM-based optimization
              |      (red) and HDM-based optimization (black). . . . . . . . . . . . . . . . . . . . . . . . 158
              | 5.23 Progression of reduced objective function: dashed line indicates an HDM sample and
              |      a subsequent update of the ROB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
              | 5.24 Progression of HDM residual: dashed line indicates an HDM sample and a subsequent
              |      update of the ROB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
              | 5.25 Parametrization of CRM. Left: Undeformed CRM configuration. Right: Deformed
              |      CRM configuration with positive perturbation to the wingspan µ1 (top row), localized
              |      sweep µ2 (second row), twist µ3 (third row), and localized dihedral µ4 (bottom row). 161
              | 5.26 Two different views of the initial guess (gray) and solution (red) of the optimization
              |      problem in (5.64). The displacement from the undeformed configuration to the opti-
              |      mal solution (red) is magnified by 2×. There is a 2.2 drag count reduction from the
              |      initial to optimized shape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
blank         | 
              | 
              | 
              | 
meta          |                                                  xx
text          | 5.27 Left: Initial guess for optimization problem in (5.64). Right: Solution of optimization
              |       problem in (5.64). Both plots are colored by the coefficient of pressure Cp . There is
              |       a 2.2 drag count reduction from the initial to optimized shape. . . . . . . . . . . . . 162
              | 5.28 Convergence history of the baseline PDE-constrained optimization solver without
              |       model reduction (        ) and proposed trust region method based on hyperreduced
              |       approximation models (        ). A yellow square ( ) indicates an augmented Lagrangian
              |       update. The reduction in drag count is taken as the performance metric and the
              |       number of primal HDM queries is the cost model. With respect to this cost metric,
              |       the ROM-based optimization solver converges 2× faster than the HDM-based solver. 163
              | 5.29 Convergence history of the baseline PDE-constrained optimization solver without
              |       model reduction (        ) and proposed trust region method based on hyperreduced
              |       approximation models (        ). A yellow square ( ) indicates an augmented Lagrangian
              |       update. The reduction in drag count is taken as the performance metric and the total
              |       wall time of the optimization procedure (normalized by the wall time of a single
              |       primal HDM solve) is the cost model. With respect to this cost metric, the ROM-
              |       based optimization solver converges 1.6× faster than the HDM-based solver. . . . . . 164
              | 5.30 The sample mesh (72×103 nodes) used at an intermediate iteration of the trust region
              |       method based on hyperreduced (collocation) approximation models. . . . . . . . . . 164
blank         | 
text          | 6.1   Full tensor product based on Clenshaw-Curtis (levels 1, 3, 5) . . . . . . . . . . . . . 178
              | 6.2   Isotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5) . . . . . . . . . . . . . 178
              | 6.3   Anisotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5) . . . . . . . . . . . 179
              | 6.4   Anisotropic sparse grid based on Clenshaw-Curtis with all (including non-admissible)
              |       forward neighbors (levels 1, 3, 5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
              | 6.5   Left: the control defining the initial guess for the optimization problem (              ), the
              |       solution of the deterministic optimal control problem, i.e., with the stochastic variables
              |       fixed at their mean value y = 0 (            ), and the solution of the stochastic optimal
              |       control problem (       ). Right: the mean solution of the viscous Burgers’ equation
              |       in (6.68) at the initial control (       ), optimal deterministic control (        ), and the
              |       optimal stochastic control. One (         ) and two (      ) standard deviations about the
              |       mean solution corresponding to the optimal stochastic control are also included.             . . 199
              | 6.6   Convergence history of the objective error quantities using MI (left) and MII (right):
              |       |F (µk ) − F (µ∗ )| (   ), |F (µ̂k ) − F (µ∗ )| (   ), |mk (µk ) − F (µ∗ )| (    ), |mk (µ̂k ) −
              |            ∗
              |       F (µ )| (      ). Rapid progress is made toward the optimal solution, despite poor
              |       agreement between the objective and model at early iterations. . . . . . . . . . . . . 200
              | 6.7   Convergence history of the gradient quantities using MI (left) and MII (right):
              |       k∇F (µk )k (      ), k∇F (µ̂k )k (     ), k∇mk (µk )k (      ), k∇mk (µ̂k )k (      ). . . . . . 201
              | 6.8   Cumulative number of HDM primal and adjoint evaluations as the major iterations
              |       in the various trust region algorithms progress: BII (           ), BIII (       ), MI (      ),
              |       MII (       ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
blank         | 
              | 
meta          |                                                    xxi
text          | 6.9   Left: Cumulative number of primal and adjoint ROM evaluations as the major iter-
              |       ations in the various trust region algorithms progress. Right: Number of primal and
              |       adjoint ROM queries organized according to the size of the reduced-order basis (ku ).
              |       Trust region methods considered: MI (           ), MII (    ). . . . . . . . . . . . . . . . . 202
              | 6.10 Convergence of the objective function (left) and gradient (right) as a function of the
              |       cost metric in (6.69) for method MIII for several values of the speedup factor of the
              |       reduced-order model: τ = 1 (       ), τ = 10 (       ), τ = 100 (    ), τ = ∞ (     ). The
              |       baseline methods used for comparison: BI (           ) and BIII (     ). . . . . . . . . . . 203
blank         | 
text          | C.1 Schematic of restriction of parameter space RNµ to affine subspace A(µ̄, Υ) of di-
              |       mension kµ , in the special case where Nµ = 2 and kµ = 1. The optimal solution µ∗
              |       in the parameter space, as well as the optimal solution over A(µ̄, Υ) are also depicted.227
blank         | 
text          | D.1 Time-dependent mapping between reference and physical domains. . . . . . . . . . . 241
              | D.2 Airfoil kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
              | D.3 Verification of adjoint-based gradient with fourth-order centered finite difference ap-
              |       proximation, for a range of finite intervals, τ , for the total work W —the objective
              |       function in (D.44)—for parametrization PII (Table D.2). The computed gradient
              |       match the finite difference approximation to about 10 digits of accuracy before round-
              |       off errors degrade the accuracy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
              | D.4 Trajectories of x(t), y(t), and θ(t) at initial guess (         ), solution of (D.44) under
              |       parametrization PI (      ), and solution of (D.44) under parametrization PII (           )
              |       for ID = 7. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
              | D.5 Time history of instantaneous quantities of interest (x-directed force – Fxh (u, µ, t),
              |       y-directed force – Fyh (u, µ, t), total power – P h (u, µ, t), x-translational power –
              |       Pxh (u, µ, t), y-translational power – Pyh (u, µ, t), rotational power – Pθh (u, µ, t)) at
              |       initial guess (    ), solution of (D.44) under parametrization PI (         ), and solution
              |       of (D.44) under parametrization PII (       ) for ID = 7. . . . . . . . . . . . . . . . . . 259
              | D.6 Left: Convergence of total work W with optimization iteration for parametrization PI
              |       (    ) and PII (     ) for ID = 7. Both optimization problems converge to a motion
              |       with significantly lower required total work; PII finds a better motion than PI (in
              |       terms of total work) due to the enlarged search space, at the cost of additional iter-
              |       ations. Each optimization iteration requires a primal flow computation—to evaluate
              |       the quantities of interest—and its corresponding adjoint—to evaluate the gradient of
              |       the quantity of interest. Right: Convergence of optimal value of total work W as
              |       parameter space is refined for parametrization PI (           ) and PII (     ). This im-
              |       plies convergence to an optimal, smooth trajectory that is not polluted by its discrete
              |       parametrization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
blank         | 
              | 
              | 
              | 
meta          |                                                xxii
text          | D.7 Flow vorticity around airfoil undergoing motion corresponding to initial guess for
              |      optimization, i.e., pure heaving (       ). Flow separation off leading edge implies a
              |      large amount of work required to complete mission. Snapshots taken at times t =
              |      0.0, 0.8, 1.6, 2.4, 3.2, 4.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
              | D.8 Flow vorticity around airfoil undergoing motion corresponding to optimal pitching
              |      motion for fixed translational motion, i.e., solution of (D.44) under parametrization
              |      PI (       ). The pitching motion greatly reduces the degree of flow separation and
              |      vortex shedding compared to the initial guess, and requires less work to complete the
              |      mission. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0. . . . . . . . . . . . 262
              | D.9 Flow vorticity around airfoil undergoing motion corresponding to optimal rigid body
              |      motion, i.e., solution of (D.44) under parametrization PII (            ). This rigid body
              |      motion further reduces the degree of flow separation and required work to complete
              |      the mission. This motion differs from the solution of PI as it has a larger pitch
              |      amplitude and slightly overshoots the final vertical position before settling to the
              |      required position. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0. . . . . . 263
              | D.10 Airfoil kinematics and deformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
              | D.11 Trajectories of y(t), θ(t), and c(t) at initial guess (         ), solution of (D.49) under
              |      parametrization FI (q = 0.0:          , q = 1.0:       , q = 2.5:        ), and solution of
              |      (D.49) under parametrization FII (q = 0.0:           , q = 1.0:       , q = 2.5:     ) from
              |      Table D.4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
              | D.12 Time history of total power, P h (u, µ, t), and x-directed force, Fxh (u, µ, t), imparted
              |      onto foil by fluid at initial guess (      ), solution of (D.49) under parametrization
              |      FI (q = 0.0:         , q = 1.0:       , q = 2.5:        ), and solution of (D.49) under
              |      parametrization FII (q = 0.0:        , q = 1.0:     , q = 2.5:       ) from Table D.4.   . . 266
              | D.13 Convergence of quantities of interest, W and Jx , with optimization iteration for
              |      parametrization FI (q = 0.0:       , q = 1.0:      , q = 2.5:      ) and FII (q = 0.0:    ,
              |      q = 1.0:       , q = 2.5:    ) from Table D.4. Each optimization iteration requires the
              |      a primal flow computation—to evaluate quantities of interest—and its corresponding
              |      adjoint—to evaluate the gradient of quantities of interest. . . . . . . . . . . . . . . . 267
              | D.14 Flow vorticity around flapping airfoil undergoing motion corresponding to initial guess
              |      for optimization problem (D.49), i.e., pure heaving (       ). Flow separation off leading
              |      edge implies a large amount of work required for flapping motion. Snapshots taken
              |      at times t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0. . . . . . . . . . . . . . . . . . . . . 269
              | D.15 Flow vorticity around flapping airfoil undergoing optimal rigid body motion corre-
              |      sponding to the solution of (D.49) under parametrization FI. The x-directed im-
              |      pulse is Jx = 2.5. The pitching motion greatly reduces the degree of flow separation
              |      and vortex shedding compared to the initial guess, and requires less work to com-
              |      plete the flapping motion and generate desired impulse. Snapshots taken at times
              |      t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0. . . . . . . . . . . . . . . . . . . . . . . . . . 270
blank         | 
              | 
              | 
meta          |                                               xxiii
text          | D.16 Flow vorticity around flapping airfoil undergoing optimal deformation and kinematic
              |      motion, corresponding to the solution of (D.49) under parametrization FII. The x-
              |      directed impulse is Jx = 2.5. The morphing further reduces the flow separation
              |      and work required to complete the flapping motion and generate desired impulse.
              |      Snapshots taken at times t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.                . . . . . . . . . . 271
              | D.17 Trajectories of h(µ, t) and θ(µ, t) that define the motion of the airfoil in Figure D.27
              |      and will be used to study primal and dual time-periodic solvers. . . . . . . . . . . . . 285
              | D.18 Flow vorticity around heaving/pitching airfoil for simulation initialized from steady
              |      state flow. Non-physical transients are introduced at the beginning of the time interval
              |      that result in non-trivial errors in integrated quantities of interests. Snapshots taken
              |      at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . . . . . . . . . . . . . . . . . . . . . . . . . 286
              | D.19 Time-periodic flow vorticity around heaving/pitching airfoil, i.e., initialized from pe-
              |      riodic initial condition. The time-periodic initial condition ensures transients are not
              |      introduced at the beginning of the simulation; the result is a seamless transition be-
              |      tween periods, as would be experienced in-flight, and trusted integrated quantities of
              |      interest. Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . . . . . . . . . . . 286
              | D.20 Convergence comparison for numerical solvers for fully discrete time-periodically con-
              |      strained partial differential equations (D.52), (D.54), nonlinearly preconditioned with
              |      m fixed point iterations. Left: m = 0, middle: m = 1, right: m = 5. Solvers:
              |      fixed point iteration (     ), steepest decent (               ), L-BFGS (    ), Newton-GMRES:
              |              −2                   −3                       −4
              |      ∆ = 10       (   ), ∆ = 10        (    ), ∆ = 10           (      ), where ∆ is the GMRES con-
              |      vergence tolerance. The optimization algorithms (steepest decent and L-BFGS) were
              |      not included in the m = 0 study due to lack of convergence issues. . . . . . . . . . . 287
              | D.21 Linear and nonlinear convergence of Newton-GMRES method for determining fully
              |      discrete time-periodic solutions with various linear system tolerances, ∆, i.e., kJ x − Rk <
              |      ∆, where r and J are defined in (D.61) and (D.62). Tolerances considered: ∆ = 10−2
              |      (    ), ∆ = 10−3 (        ), ∆ = 10−4 (         ). . . . . . . . . . . . . . . . . . . . . . . . . 289
              | D.22 Time history of power, Fxh (u, µ, t), and x-directed force, P h (u, µ, t), after k Newton-
              |      GMRES iterations (linear system convergence tolerance ∆ = 10−2 ) starting from
              |      steady-state. Values of k: 0 (        ), 1 (      ), and 8 (         ). . . . . . . . . . . . . . . . 289
              | D.23 Convergence of fully discrete quantities of interest to their values at the time-periodic
              |      solution, W ∗ and Jx∗ , for various solvers, without nonlinear preconditioning. Solvers:
              |      Newton-GMRES: ∆ = 10−2 (               ), ∆ = 10−3 (              ), ∆ = 10−4 (     ), where ∆ is
              |      the GMRES convergence tolerance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
              |                                     ∂u(Nt )
              | D.24 First 200 eigenvalues ( ) of    ∂u0 —evaluated         at periodic solution—with largest mag-
              |      nitude. All eigenvalues lie in unit circle, thus the periodic orbit is stable. . . . . . . . 290
blank         | 
              | 
              | 
              | 
meta          |                                                     xxiv
text          | D.25 GMRES convergence for determining solution of adjoint equations corresponding to
              |      fully discrete time-periodic partial differential equation, i.e., a linear two-point bound-
              |                                                          ∂W                   ∂Jx
              |      ary value problem. A defined in (D.81), b1 =               , and b2 =           from (D.80),
              |                                                         ∂u(Nt )              ∂u(Nt )
              |      where W is fully discrete approximation of the total work done by fluid on airfoil
              |      and Jx is the x-directed impulse. Solvers: fixed point iteration (         ) and GMRES
              |      (    ). The linearization is performed about the time-periodic solution obtained with
              |      Newton-Krylov (∆ = 10−4 ) method. . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
              | D.26 Verification of periodic adjoint-based gradient with second-order centered finite differ-
              |      ence approximation, for a range of finite intervals, τ . The computed gradient match
              |      the finite difference approximation to nearly 7 digits before round-off errors degrade
              |      the accuracy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
              | D.27 Kinematic description of body under consideration, NACA0012 airfoil (right). . . . . 293
              | D.28 Trajectories of h(µ, t) and θ(µ, t) at initial guess (      ) and optimal solution (       )
              |      for optimization problem in (D.93). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
              | D.29 Time history of the power, P h (u, µ, t), and x-directed force, Fxh (u, µ, t), imparted
              |      onto foil by fluid at initial guess (    ) and optimal solution (       ) for optimization
              |      problem in (D.93). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
              | D.30 Convergence of quantities of interest, W and Jx , with optimization iteration. Each
              |      optimization iteration requires a periodic flow computation and its corresponding
              |      adjoint to evaluate the quantities of interest and their gradients. . . . . . . . . . . . 297
              | D.31 Trajectory of airfoil and flow vorticity at initial guess for optimization (pure heaving
              |      motion, see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . 297
              | D.32 Trajectory of airfoil and flow vorticity at energetically optimal, zero-impulse flapping
              |      motion (see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . 298
blank         | 
              | 
              | 
              | 
meta          |                                                xxv
title         | Chapter 1
blank         | 
title         | Introduction
blank         | 
title         | 1.1     Motivation
text          | Optimization problems governed by partial differential equations, or PDE-constrained optimiza-
              | tion problems, arise in nearly every branch of engineering and science. The most classical PDE-
              | constrained optimization problems arise in the context of design and control of engineering systems.
              | The solutions of these problems promise to deliver engineering systems with superior performance
              | (in some chosen metric) than otherwise possible, and will have the greatest impact in highly complex
              | situations where intuition breaks down and prototyping and experimentation are expensive, difficult,
              | or dangerous. Topological optimization can lead to lightweight, highly optimized designs intended to
              | operate in volatile multiphysics environments [180], which can be realized using 3D printing or addi-
              | tive manufacturing technology [111, 203]. Topology optimization also promises to have widespread
              | impact in medicine, specifically with regard to medical implants, since optimized, patient-specific
              | implants can be realized [213]. Shape optimization has been used to design aircraft and automobiles
              | with superior aerodynamic performance [164, 163, 123, 55] and reduced environmental and noise
              | [50, 73] impact. Shape optimization has also been used in biological applications, e.g., to design the
              | shape of the incoming branch of the aorto-coronaric bypass [160, 171]. Boundary and volumetric
              | control have been used to drive the state of an engineering system toward some desired using source
              | terms, e.g., diverting heat from a microprocessor using a fan.
              |    PDE-constrained optimization problems also arise in the context of material and initial condition
              | inversion. In material inversion problems, an unknown material distribution must be inferred from
              | the response of a system to known inputs. Nondestructive evaluation [39, 72] seeks to determine
              | the material distribution of a solid object in situ, i.e., without extracting a sample and performing
              | laboratory tests, from the response measured from structural and acoustic inputs to detect structural
              | defects prior to operation. A similar PDE-constrained optimization problem underlies the emerging
              | technology of full waveform inversion [195] where the material properties of the Earth’s crust are
              | sought in order to detect the location and size of oil reservoirs. In initial condition inversion, the
blank         | 
              | 
              | 
meta          |                                                   1
              | CHAPTER 1. INTRODUCTION                                                                              2
blank         | 
              | 
              | 
text          | initial state of a system must be inferred from measurements at later times. An important instance
              | of such a PDE-constrained optimization problem occurs in the determination of the source of a
              | contaminant given its current configuration.
              |    The problems considered to this point have been posed in the ideal setting of certain knowl-
              | edge of the data defining the coefficients (material properties) and boundary conditions (loads) of
              | the partial differential equation describing the physical system of interest. This is not realistic as
              | all physical systems, particularly those characterized by a high degree of volatility, are plagued
              | with uncertainties. An important consideration in any discipline in science and engineering is the
              | robustness of a particular system with respect to these uncertainties. PDE-constrained optimiza-
              | tion problems also arise when attempting to quantify the uncertainty in quantities of interest of
              | PDEs as a result of uncertain data or input. For example, in the Bayesian framework, locating the
              | Maximum A Posteriori (MAP) point is a required step in importance sampling [134], i.e., where
              | samples are efficiently drawn from the posterior distribution of the uncertain PDE, and amounts to
              | a PDE-constrained optimization problem. Beyond simply using PDE-constrained optimization to
              | facilitate uncertainty quantification, it is important to incorporate uncertainty quantification into
              | the optimization problem to obtain designs and controls that are risk-averse with respect to the un-
              | certainties. This leads to PDE-constrained optimization under uncertainty—optimization problems
              | constrained by stochastic partial differential equations with objective and constraints defined as risk
              | or hazard measures of its quantities of interests. These hazard measures usually penalize variance
              | from the mean or rare, catastrophic events.
              |    While the potential benefit of widespread adoption of PDE-constrained optimization in engi-
              | neering and scientific practice are profound, a number of factors prevent this, most notably the
              | large computational cost, in terms of computing time and resources, associated with these problems.
              | Often, particularly in relevant 3D applications, partial differential equations require a massive dis-
              | cretization to accurately resolve the underlying physics and the solution of the resulting (sequence
              | of) nonlinear equations requires significant computing time on a supercomputer. PDE-constrained
              | optimization problems require potentially many queries to the underlying discretized primal and dual
              | PDE to iteratively progress toward the optimal solution. Since even a single PDE solve constitutes a
              | significant investment in computational resources, the many-query setting of PDE-constrained opti-
              | mization exacerbates this problem and, in some cases, can be prohibitively expensive. This situation
              | is further complicated from the presence of uncertainty that exists in every physical setting, particu-
              | larly those with a high degree of volatility. In reality, the boundary conditions, material properties,
              | and sources terms of a system are not known with certainty and cannot be modeled as such if one
              | wishes to discover solutions that are robust with respect to these uncertainties. Depending on the
              | number of stochastic parameters incorporated into the PDE, the quantification of uncertainty in an
              | optimization problem will increase the computational cost by potentially many orders of magnitude.
              | This effectively makes these problems infeasible for all except the smallest academic problems.
meta          | CHAPTER 1. INTRODUCTION                                                                                  3
blank         | 
              | 
              | 
title         | 1.2      Strategy and Objectives
text          | The primary objective of this thesis is to develop a series of optimization methods to solve large-scale
              | deterministic and stochastic PDE-constrained optimization problems that can largely circumvent the
              | prohibitive cost of repeatedly running computational physics simulations without compromising the
              | quality of the resulting optimum. Focus will be placed on methods that apply to complex, nonlinear
              | partial differential equations that do not possess significant structure, i.e., linearity, ellipticity, and
              | coercivity, that can be used to develop inexpensive, computable error bounds. The strategy taken
              | to accomplish this objective is modular in the sense that two independent technologies will be
              | developed and later combined and specialized to the context of deterministic and stochastic PDE-
              | constrained optimization. The two foundational technologies that are developed for this purpose
              | are: (1) a globally convergent, generalized trust region method for the management of approximation
              | models in the context of optimization and (2) minimum-residual, projection-based reduced-order and
              | hyperreduced models as low-dimensional approximations of the discretized PDE.
              |    In the context of deterministic PDE-constrained optimization, these technologies, along with
              | the concept of a partially converged PDE solution, will be combined to yield an efficient, globally
              | convergent optimization procedure. In the context of stochastic PDE-constrained optimization,
              | projection-based reduced-order models and dimension-adaptive sparse grids will define an efficient
              | approximation model based on two levels of inexactness. This two-level approximation will be nested
              | in the generalized trust region method to produce an efficient, globally convergent optimization
              | procedure.
blank         | 
              | 
title         | 1.3      Literature Review
text          | This work builds on several foundational technologies including PDE-constrained optimization, trust
              | region methods, projection-based model reduction, and surrogate methods for PDE-constrained
              | optimization. This section presents a brief literature review of each technology and outlines the
              | contributions of this thesis to the state-of-the-art.
blank         | 
              | 
title         | 1.3.1     PDE-Constrained Optimization
text          | This work is primarily concerned with the efficient solution of optimization problems governed by
              | partial differential equations, in both the deterministic and stochastic setting. This section provides
              | a brief literature review of deterministic and stochastic PDE-constrained optimization with an ex-
              | tensive mathematical formulation of PDE-constrained optimization problems provided in Chapter 2.
              |    PDE-constrained optimization has been extensively studied in the case where the underlying
              | partial differential equation is deterministic. A thorough review of the topic is provided in the
              | references [78, 96]. The PDE-constrained optimization problem is naturally posed in a continuous
              | setting [100, 135], that is, the PDE itself is a constraint of the optimization problem and the ob-
              | jective function and “side” constraints are defined by integrating the PDE solution over (portions)
meta          | CHAPTER 1. INTRODUCTION                                                                             4
blank         | 
              | 
              | 
text          | of the spatio-temporal domain. The corresponding optimality conditions are a system of partial
              | differential equations that must be discretized to be solvable in a computational setting. A more
              | common and practical approach, particularly in large-scale implementations, defines an optimization
              | problem constrained by the discretized PDE, resulting in an optimality system consisting of a system
              | of discrete nonlinear equations [138, 187, 130, 140]. Once the optimization setting has been chosen,
              | the optimization problem can be solved using a full space [147, 69, 91, 116, 2] or reduced space
              | [100, 197, 138, 108, 109, 210] approach. The reduced space approach uses a PDE solver to eliminate
              | the PDE constraint from the optimization problem while the full space approach considers the PDE
              | as a constraint and directly solves the complete optimization problem. If the reduced space method
              | is employed and a gradient-based optimizer is used, the sensitivity [80, 138, 129, 127] or adjoint
              | [100, 197, 187, 130, 123] method are required to compute the required gradients of the optimiza-
              | tion functionals. The relative efficiency of these methods depends on the number of optimization
              | variables and constraints: the sensitivity method is more efficient if there are more constraints than
              | variables and the adjoint method is more efficient in the opposite case. These concepts regarding
              | the continuous versus discrete formulation, full space versus reduced space approach, and sensitiv-
              | ity versus adjoint method for computing gradients apply whether the partial differential equation
              | under consideration is static or transient [88, 116, 136, 124, 205, 140, 55, 132, 211, 212]. The case
              | where the PDE is unsteady represents a significant increase in computational expense as there are
              | substantially more optimization variables in the full space approach (one for each spatial degree of
              | freedom at each timestep) or the full transient PDE must be resolved at each optimization iteration
              | in the reduced space approach. This work will solely consider the discrete formulation of the PDE-
              | constrained optimization problem, which will be solved using the reduced space approach. Both the
              | sensitivity and adjoint methods will be used to compute gradients of quantities of interest.
              |    In addition to the many considerations involved in the formulation and solution of deterministic
              | PDE-constrained optimization problems, the case where the underlying PDE depends on random
              | data [13, 141, 142, 199, 204, 12] involves an additional component—treatment of the stochastic
              | variables. Stochastic Galerkin [13] and collocation [12] are popular techniques for discretizing the
              | stochastic space associated with the PDE. This work will solely consider the non-intrusive approach
              | of stochastic collocation [24, 23, 22, 178, 188, 107] and the collocation nodes will be defined using
              | sparse grids [184, 144, 66, 145, 156, 18, 157, 67, 29, 146]. While there has been work considering
              | random optimization variables [24, 30], this work will consider the optimization variables to be
              | deterministic quantities, with the data underlying the PDE (boundary conditions, coefficients) as
              | uncertain. These instances of PDE-constrained optimization problems under uncertainty can be
              | many orders of magnitude more expensive than the deterministic counterpart since the PDE solution
              | must be resolved over the stochastic space, which may be high dimensional. While these problems
              | have been solved in a number of relevant applications [30, 49, 178], they are impractically expensive
              | for many important engineering and science applications.
meta          | CHAPTER 1. INTRODUCTION                                                                                    5
blank         | 
              | 
              | 
title         | 1.3.2     Trust Region Methods
text          | One of the foundational technologies that this thesis builds upon and extensively utilizes are trust
              | region methods for numerical optimization. Trust region methods are a popular and robust globaliza-
              | tion strategy for numerical optimization solvers, that is, a framework for ensuring a local minimum is
              | obtained, regardless of the starting point. While they are not usually considered as efficient as line-
              | search methods [71, 143], they are popular due to their robustness and flexibility. Let F : RNµ → R
              | define a function to be minimized and suppose evaluations of F (µ) and ∇F (µ) are expensive. Early
              | trust region methods replaced the potentially expensive optimization problem
blank         | 
text          |                                               minimize F (µ)
              |                                                µ∈RNµ
blank         | 
              | 
text          | with the inexpensive quadratic program
blank         | 
text          |                                                                  1
              |              minimize     mk (µ) := F (µk ) + ∇F (µk )(µ − µk ) + (µ − µk )T Bk (µ − µk )
              |               µ∈RNµ                                              2
              |              subject to    kµ − µk k ≤ ∆k ,
blank         | 
text          | where Bk is a symmetric positive-definite approximation of the Hessian ∇2 F (µk ). The solution of
              | this trust region subproblem provides a candidate for the new trust region center and, depending on
              | how well the reduction actually achieved by accepting the step compares to the reduction predicted
              | by the quadratic model, the step is accepted or rejected and the trust region radius ∆k is modified
              | accordingly. The quality of the trust region step is assessed by comparing the actual-to-predicted
              | reduction ratio (ρk ) to unity
              |                                                 F (µk ) − F (µ̂k )
              |                                         ρk =                        ,
              |                                                mk (µk ) − mk (µ̂k )
              | where µ̂k is the candidate step, defined as the solution of the trust region subproblem. Once the
              | details of the step acceptance and radius modification are complete, it can be shown [48] that the
              | sequence of trust region centers {µk } converges to a first-order critical point
blank         | 
text          |                                             lim k∇F (µk )k = 0.
              |                                            k→∞
blank         | 
              | 
text          | The pivotal work in [159, 133] established convergence under only mild conditions—known as the
              | fraction of Cauchy decrease—on the candidate step produced by the quadratic program. A slew
              | of specialized, efficient solvers have been developed that generate steps guaranteed to satisfy the
              | fraction of Cauchy decrease; see [133, 48] for an extensive overview.
              |    In many applications, it may be expensive or impossible to evaluate the objective function or
              | its gradient to construct the quadratic approximation, e.g., if F (µ) corresponds to the quantity of
              | interest of a partial differential equation or an iterative linear solver [93, 166, 92] is used to compute to
              | compute F (µ) or ∇F (µ), and a host of work [133, 189, 34, 35, 36, 48, 93, 216, 108, 109] has been done
              | to allow for inexact gradient evaluations to be used in the definition of the trust region subproblem
meta          | CHAPTER 1. INTRODUCTION                                                                               6
blank         | 
              | 
              | 
text          | and inexact objective evaluations in the computation of ρk . Moré [133] introduced an inexact gradient
              | condition that requires the gradient approximation at the trust region center, gk , asymptotically
              | approaches the true gradient, i.e., k∇F (µk ) − gk k → 0 for any convergent sequence {µk }. While
              | this provides substantial flexibility over previous work that requires first-order consistency of the
              | approximation and model (gk = ∇F (µk )), this condition does not suggest an accuracy condition on
              | gk at a particular iteration. Carter [34, 35, 36] suggested the relative gradient error condition
blank         | 
text          |                                 k∇F (µk ) − gk k ≤ η kgk k       η ∈ (0, 1),
blank         | 
text          | which has served as the basis for many trust region model management methods, including the
              | popular Trust Region Proper Orthogonal Decomposition [10] method. The Carter condition is
              | useful because it does not require gk be recomputed to higher accuracy after a failed step; however,
              | it requires the evaluation of the gradient error (or a tight bound), which may be impractical in many
              | situations. Toint [189] suggested the gradient condition
blank         | 
text          |                            k∇F (µk ) − gk k ≤ min{κ1 ∆k , κ2 }          κ1 , κ2 > 0
blank         | 
text          | that requires increased accuracy as ∆k decreases, i.e., after failed iterations, but relies on arbitrary
              | constants κ1 , κ2 . Heinkenschloss and Vincent [93] suggested a similar gradient condition in the
              | context of a Sequential Quadratic Programming (SQP) method
blank         | 
text          |                              k∇F (µk ) − gk k ≤ ξ min{kgk k , ∆k }         ξ>0
blank         | 
text          | that requires increased accuracy after failed iterations or near convergence and also depends on an
              | arbitrary constant. The arbitrary constants required by the Toint [189] and Heinkenschloss-Vincent
              | [93] bounds are significant as they permit the use of error indicators that can completely circumvent
              | the need to compute or tightly bound the gradient error. Suppose an error indicator ϕk : RNµ → R
              | can be derived such that
              |                                  k∇F (µk ) − gk k ≤ ξϕk (µk )       ξ > 0,
blank         | 
text          | where ξ > 0 is an arbitrary constant. Then the Heinkenschloss-Vincent [93] gradient condition will
              | be satisfied if the error indicator satisfies
blank         | 
text          |                                        ϕk (µk ) ≤ κ min{kgk k , ∆k },
blank         | 
text          | where κ > 0 is any user-defined constant. Since the error indicator is solely used to enforce the
              | required gradient condition, the constant ξ > 0 is never computed and may depend on quantities
              | that, in general, cannot be computed such as Lipschitz constants or bounds on various quantities.
              | This work employs the Heinkenschloss-Vincent condition due to the required generality in handling
              | complex, nonlinear PDE-constrained optimization problems where tight gradient error bounds are
              | not readily available.
meta          | CHAPTER 1. INTRODUCTION                                                                               7
blank         | 
              | 
              | 
text          |    Similar to the inexact gradient condition used in the trust region subproblem, conditions have
              | been developed [34, 216, 109] for using inexact objective function evaluations in the actual-to-
              | predicted reduction ratio, ρk . The asymptotic condition in [109] allows for the same flexibility as
              | the Heinkenschloss-Vincent gradient condition and will be used in this work. Kouri [109] replaced
              | the computation of ρk with
              |                                                  ψk (µk ) − ψk (µ̂k )
              |                                          ρ̃k =                        ,
              |                                                  mk (µk ) − mk (µ̂k )
              | where ψk : RNµ → R is the inexact objective model that satisfies
blank         | 
text          |                                                                                           1/ω
              |        |F (µk ) − F (µ̂k ) + ψk (µ̂k ) − ψk (µk )| ≤ σ [η min{mk (µk ) − mk (µ̂k ), rk }]       σ>0
blank         | 
text          | and σ is an arbitrary constant, {rk }∞
              |                                      k=1 is a forcing sequence such that rk → 0, and µ̂k is the
              | candidate step at iteration k. This condition permits the use of an error indicator θk : RNµ → R
              | such that
              |                         |F (µk ) − F (µ) + ψk (µ) − ψk (µk )| ≤ σθk (µ)          σ > 0.
blank         | 
text          | The true error can be disregarded and the inexact objective condition enforced solely based on the
              | error indicator
              |                                 θk (µ̂k )ω ≤ η min{mk (µk ) − mk (µ̂k ), rk },
blank         | 
text          | where ω, η ∈ (0, 1) are algorithmic constants. When the Heinkenschloss-Vincent gradient condition
              | [93] and Kouri objective condition [109] are combined into a single trust region method, as seen in
              | [109], the entire algorithm proceeds without requiring queries to F (µ) or ∇F (µ) and guarantees
              | global convergence—this flexibility will be built upon and leveraged in this work as I look to develop
              | methods that address large-scale, expensive problems where inexpensive, tight error bounds are not
              | available.
              |    In many cases, it is possible to obtain an approximation that is superior to the basic quadratic
              | model, which can be used to provide a better approximation model mk (µ) in the trust region
              | framework. Alexandrov [4, 6] introduced the trust region model management framework that al-
              | lows for this and proves global convergence, provided the approximation model satisfies first-order
              | consistency at trust region centers
blank         | 
text          |                                mk (µk ) = F (µk )       ∇mk (µk ) = ∇F (µk ).
blank         | 
text          | These requirements can be weakened by introducing the inexact gradient conditions of [189, 35, 93]
              | and inexact objective condition of [34, 109]. This flexibility has been leveraged in a number of
              | contexts, most notably the Trust Region Proper Orthogonal Decomposition method [10, 57, 1,
              | 170, 186] where the approximation model is taken as the projection-based reduced-order model
              | whose reduced basis is computed via Proper Orthogonal Decomposition (POD) and the method
              | of snapshots [183] and the Carter condition [35] is employed. It was also leveraged in [108, 109]
              | in the context of PDE-constrained optimization uncertainty where the model problem employed
meta          | CHAPTER 1. INTRODUCTION                                                                            8
blank         | 
              | 
              | 
text          | dimension-adaptive sparse grids to approximate the integral of the PDE quantity of interest over
              | the stochastic space.
blank         | 
              | 
title         | 1.3.3    Projection-Based Model Reduction
text          | Another pivotal technology in this work is projection-based model reduction, which will be used to
              | define inexpensive approximation models for the expensive PDE discretization and solver underlying
              | the PDE-constrained optimization problem of interest. The concepts underlying modern reduced-
              | order models have been used in the context of modal decomposition for linear structural dynamics
              | for several decades [65]. In this approach, the dynamics of a particular structure are approximated
              | using its dominant modes, which are computed via an eigenvalue decomposition of the system mass
              | and stiffness matrices. Generalization to the case of a nonlinear structure exist [98], but have not
              | seen the same widespread adoption as the linear case.
              |    Modern approaches to projection-based model reduction include the reduced basis method [121,
              | 122, 17, 173] and methods based on Proper Orthogonal Decomposition (POD) [21, 101] and the
              | method of snapshots [183]. The reduced basis method employs a variational framework and con-
              | structs a reduced basis from solutions of the underlying PDE that are greedily sampled in the
              | parameter space at locations where an inexpensive error bound on the reduced-order model is max-
              | imized [149, 173]. This is usually embedded in an offline-online framework [17, 149, 173] where all
              | expensive operations related to sampling the PDE and construction of the reduced basis are con-
              | fined to an offline phase and the inexpensive reduced-order model is repeatedly queried in the online
              | phase. While this method possesses a beautiful mathematical framework, it relies on properties
              | of the underlying PDE such as linearity and ellipticity for the derivation of the error bounds and
              | the efficient offline-online decomposition. POD-based model reduction is a general framework that
              | uses POD to compress “snapshots” of the PDE solution at particular time instances and parameter
              | configurations to generate a physics-based basis that will be used to approximate the solution. The
              | governing equations are restricted to a low-dimensional “trial” subspace and projected onto an ap-
              | propriate “test” subspace. The result is a small nonlinear system of equations—few unknowns from
              | the introduction of the trial subspace and few equations from the projection onto the test subspace.
              | An increasing popular approach in model reduction is to choose the test basis such that the resulting
              | reduced-order model minimizes the residual in some norm [31, 89]. Such reduced-order models have
              | been called “optimal” for a given trial subspace in this particular norm [31]. These “optimal” or
              | minimum-residual reduced-order models have been extensively studied in [115, 28, 31, 89]. Chapter 4
              | details several properties of minimum-residual reduced-order models, some of which are new, that
              | will be used in Chapters 5–6 in the construction of optimization methods based on reduced-order
              | models. An important contribution of this work is the extension of the concept of minimum-residual
              | reduced-order models from the primal setting to sensitivity and adjoint PDEs. It will be shown that
              | this approach to compute reduced sensitivities and adjoints will circumvent many difficulties that
              | arise in directly considering the sensitivity or adjoint of a minimum-residual reduced-order model.
              | Furthermore, conditions will be provided under which the minimum-residual sensitivity/adjoint
meta          | CHAPTER 1. INTRODUCTION                                                                              9
blank         | 
              | 
              | 
text          | reduced-order models agree with the sensitivity/adjoint of the primal reduced-order model. These
              | contributions are provided in Chapter 4.
              |    Both the reduced basis method and POD-based methods construct the trial basis from solution
              | snapshots. A number of works have considered snapshots based on other types of information,
              | including sensitivities [87, 86, 32, 85, 52, 210, 198], adjoints [57, 74], unconverged solutions [198],
              | residuals at unconverged solutions [198], and Krylov vectors from the linear system solve that arises
              | at each Newton-Raphson iteration [198]. However, it is well-known that the singular value decom-
              | position underlying POD is sensitive to the relative scaling of the columns in the data matrix and
              | this heterogeneous collection of snapshots should not be carelessly lumped into a single data matrix
              | for compression. That is, when incorporating fundamentally different types of snapshots (entries
              | have different physical units and likely different scales), care must be taken to ensure the resulting
              | decomposition is useful. The work in [32] weighs sensitivities by increment in the parameter to make
              | their units consistent with primal snapshots. A more general approach taken in [74, 210, 198] is to
              | use POD to compress on each type of snapshot individually then concatenate the resulting basis. In
              | [52] a separate basis was constructed for each sensitivity in the construction of a sensitivity ROM,
              | each of which was computed based on POD of snapshots of the corresponding sensitivity. This work
              | builds on the approach in [74, 210, 198] by using POD to build a basis from homogeneous snapshot
              | types and combining the results into a single basis. I further generalize this method to ensure partic-
              | ular snapshots are preserved in the resulting subspace. This will be pivotal in guaranteeing required
              | accuracy at trust region centers when the model reduction technology is combined with the trust
              | region method of Chapter 3 to produce globally convergent, efficient deterministic and stochastic
              | PDE-constrained optimization solvers in Chapters 5–6.
              |    Finally, partial differential equations that do not possess an affine dependence on their parameters
              | or state vector require an additional level of approximation for online efficiency. This additional
              | approximation, referred to as hyperreduction, is required to reduce the complexity of evaluating
              | nonlinear terms that are not amenable to precomputation [17, 175, 115, 41, 31, 59]. An overview
              | of the most popular hyperreduction methods are provided in Section 4.2.3. Nonlinearities that are
              | polynomial do not strictly require hyperreduction since they are amenable to precomputation and
              | all dependence on the large dimension of the underlying PDE can be confined to the offline phase
              | [149, 14, 16]. However, the approach quickly scales poorly with the size of the reduced-order model
              |                                                                            m+1
              | as the highest polynomial degree increases, e.g., they usually scale as O(ku   ) where ku is the
              | ROM size and m is the polynomial order. Section 4.2.1 provides a detailed formulation of fully
              | discrete PDEs with polynomial nonlinearities in the state and parameter, as well as the details of
              | the precomputation of the monomial terms.
blank         | 
              | 
title         | 1.3.4    Surrogate Methods for PDE-Constrained Optimization
text          | The methods introduced and developed in this thesis fall into the class of surrogate-based optimiza-
              | tion methods, whereby the expensive, high-fidelity model that defines the “true” objective F (µ) and
              | gradient ∇F (µ) is replaced by an inexpensive approximation. The surrogate models can be based
meta          | CHAPTER 1. INTRODUCTION                                                                              10
blank         | 
              | 
              | 
text          | on response surfaces [63], adaptive spatial discretizations [216], loose tolerances on linear solvers
              | [166, 216], partially converged solutions [62], projection-based reduced-order models [10, 171, 210],
              | and many other approximation models. This section provides a brief overview of methods that
              | use projection-based reduced-order models as a surrogate as they are most relevant to the methods
              | developed in this thesis. For a thorough review of surrogate-based optimization methods, see [63].
              | The methods reviewed in this section fall into two main categories: (1) those that adhere to a strict
              | offline-online decomposition and (2) those that do not. For chronological accuracy, methods that do
              | not distinguish between an offline and online phase are considered first.
              |    Alexandrov developed the Trust Region Model Management (TRMM) framework that uses a
              | general approximation model that satisfies first-order consistency in the context of unconstrained
              | [4] and nonlinearly constrained optimization [6]. The famous Trust Region Proper Orthogonal
              | Decomposition (TRPOD) method [10, 57] was among the first methods to leverage projection-
              | based reduced-order models in a globally convergent optimization algorithm. This method does
              | not exactly fit into Alexandrov’s TRMM framework as TRPOD uses the Carter condition [35] to
              | define the accuracy required of the reduced-order model to ensure convergence. This condition is
              | considerably weaker than fist-order consistency and allows a relatively small reduced-order model
              | to be used. In the TRPOD method, at the control corresponding to the trust region center, the
              | snapshots are collected from the full-order PDE simulation and compressed using POD. The size of
              | the reduced basis is selected to ensure the Carter condition is satisfied, which involves computing
              | the true gradient error at reduced-order models of increasing size. Later work on TRPOD also
              | collected adjoint snapshots and built a separate POD-based ROM for the adjoint PDE [57]. While
              | this leads to gradients that are not consistent with the quantities of interest computed from the
              | primal reduced-order model, it did not hinder convergence in the numerical experiments in [57].
              | TRPOD was originally developed for unconstrained problems and was later applied to problems with
              | nonlinear constraints [1, 186] following Alexandrov’s work [6]. A method similar to TRPOD is called
              | Optimality System POD (OS-POD) [113], which attempts to build a reduced-order model at the
              | optimal control. It formulates an optimization problem that consists of the unreduced optimization
              | problem, the reduced optimization problem, and the POD system. The monolithic optimization
              | problem is solved using a simple splitting method that results in a method similar to TRPOD
              | with two main exceptions: a trust region framework is not used to manage the approximation
              | model and OS-POD involves an intermediate step with the true gradient of the objective function.
              | Another surrogate optimization solver similar to TRPOD was developed in [208]. This method used
              | projection-based reduced-order models based on a Krylov-Pade approximation (and therefore specific
              | to linear PDEs) as the approximation model. Two significant contributions of this work are: (1) the
              | flexibility to handle generalized (non-quadratic) trust region constraints and (2) a new method to
              | assess the trust region step without evaluating the HDM. This thesis considers similar generalizations
              | over the standard TRPOD method with the most significant difference being the proposed methods
              | are built on the flexible trust region method of [109]. This flexibility is leveraged to use unconverged
              | solutions as snapshots and in the evaluation of the trust region step. It will also be used later to
meta          | CHAPTER 1. INTRODUCTION                                                                                                      11
blank         | 
              | 
              | 
              | 
text          |                               HDM                                Optimizer
blank         | 
text          |       HDM
              |                              ROB Φ
              |                  Compress
              |       HDM
              |                              ROM                                   ROM
              |                                    ROM
              |                                          ROM
blank         | 
              | 
text          |                                                      ROM
              |                                                            ROM
blank         | 
              | 
              | 
              | 
text          |                                                                                    ROM
              |                                                                                          ROM
              |                                                                                                ROM
blank         | 
              | 
text          |                                                                                                            ROM
              |                                                                                                                  ROM
              |               HDM      ROB                     ···                   HDM     ROB                     ···               ···
blank         | 
              | 
text          | Figure 1.1: The adaptive approach to accelerate PDE-constrained optimization with projection-
              | based reduced-order models. Top left: block schematic of the workflow where few High-Dimensional
              | Model (HDM) samples are compressed to build the Reduced-Order Basis (ROB) and the resulting
              | Reduced-Order Model (ROM) is used in the optimization procedure, as long as it maintains accuracy.
              | When the accuracy degrades, an additional sample of the HDM is taken at the new point in the
              | parameter space and the ROB is enriched. Top right: schematic of parameter space (µ-space) where
              | the black dot and star are the initial guess and solution of the optimization problem, respectively, the
              | red circles indicate HDM samples, the gray regions are the “trust regions” for the ROM constructed
              | at each iteration, the blue line is the trajectory of the ROM optimization procedure, and the blue star
              | is the optimal solution found by the ROM optimization. Bottom: schematic of the computational
              | cost where the expensive (HDM evaluations and ROB construction) and inexpensive components are
              | intermixed throughout the algorithm. These methods are usually equipped with global convergence
              | theory that guarantee convergence to a local optimum of the PDE-constrained optimization problem,
              | as indicated in the top right plot.
blank         | 
              | 
text          | build a two-level approximation to accelerate stochastic PDE-constrained optimization. All of the
              | methods based on Alexandrov’s TRMM framework, as well as the other variants described here,
              | are categorized as adaptive optimization procedures—see Figure 1.1—since the surrogate model is
              | not built once-and-for-all in an offline phase and repeatedly queried in the online phase; rather, the
              | surrogates are adaptively built on-the-fly during the optimization procedure.
              |    In contrast to the method that do not distinguish between offline and online cost are the reduced
              | basis methods that do make such a distinction [172, 174, 114, 125, 52]—see Figure 1.2. These
              | methods sample the parameter space in an offline phase to collect snapshots, build a reduced
              | basis, and precompute PDE operators contracted with the reduced basis. In the online phase, the
              | reduced-order model is queried many times as the PDE-constrained optimization problem is solved
              | with the ROM in place of the original PDE. Due to the strict offline-online decomposition, global
              | convergence usually cannot be established. However, since these methods usually consider linear,
              | elliptic PDEs and a quadratic objective function (resulting in a convex optimization problem), error
              | bounds between the computed solution and unique optimum can be derived and computed. As the
meta          | CHAPTER 1. INTRODUCTION                                                                                    12
blank         | 
              | 
              | 
              | 
text          |              HDM
              |                                                      Optimizer
              |              HDM
              |               ..
              |                .                         ROB Φ
              |                         Compress
              |              HDM
              |                                                          ROM
              |              HDM
              |                                          Offline
blank         | 
              | 
              | 
              | 
text          |                                                                  ROM
              |                                                                        ROM
              |                                                                              ROM
              |                                                                                    ROM
              |                                                                                          ROM
              |                                                                                                ROM
              |                                                                                                      ROM
              |               HDM      HDM         ···     HDM     HDM     ROB
blank         | 
              | 
text          | Figure 1.2: The offline-online approach to accelerate PDE-constrained optimization with projection-
              | based reduced-order models. Top left: block schematic of the workflow where a number of High-
              | Dimensional Model (HDM) samples are compressed to build the Reduced-Order Basis (ROB) in
              | an offline phase; the resulting inexpensive Reduced-Order Model (ROM) is repeatedly queried in
              | the online optimization phase. Top right: schematic of parameter space (µ-space) where the black
              | dot and star are the initial guess and solution of the optimization problem, respectively, the red
              | circles indicate HDM samples, the blue line is the trajectory of the ROM optimization procedure,
              | and the blue star is the optimal solution found by the ROM optimization. Bottom: schematic of
              | the computational cost where there is a clear distinction between the expensive components (HDM
              | evaluations and ROB construction) that are done once-and-for-all in the offline phase and the
              | inexpensive components (ROM evaluations) that are repeatedly queried in the online phase. In
              | general, these methods are not guaranteed to converge to a local optimum of the PDE-constrained
              | optimization problem, as indicated in the top right plot.
blank         | 
              | 
text          | assumptions on these methods are too strong for the applications of interest in this thesis, they will
              | not be considered further.
              |    To this point, only methods developed for deterministic PDE-constrained optimization problems
              | have been considered. In the context of PDE-constrained optimization under uncertainty, Kouri [108,
              | 109] used dimension-adaptive sparse grids to define the quadrature nodes in a stochastic collocation
              | method to define an inexpensive surrogate model (due to a quadrature rule with fewer points than
              | would be required by an isotropic sparse grid or tensor product rule). This approximation model was
              | embedded in the trust region method developed in those papers that allows for inexact gradient and
              | objective evaluations. Chen [44, 42, 43] introduced an additional level of approximation by using
              | reduced-order models in addition to sparse grids. This work focused on simple PDEs and employed
              | an offline-online framework (instead of a globally convergent trust region framework that breaks
              | the offline-online decomposition). The method developed in this work for efficient PDE-constrained
              | optimization under uncertainty is a crossover between these two methods: I develop a two-level
              | approximation based on projection-based model reduction and dimension-adaptive sparse grids and
              | embed the approximation model in a globally convergent trust region framework. This enables the
meta          | CHAPTER 1. INTRODUCTION                                                                             13
blank         | 
              | 
              | 
text          | framework to handle general PDEs, leverage the efficiency benefits of reduced-order models, and
              | ensure global convergence; see Chapter 6 for details.
              |    The methods developed in this thesis most resemble TRPOD in that snapshots of the HDM
              | at the trust region center will define the reduced-order model. A crucial difference that leads to
              | improved efficiency and flexibility is that the proposed methods will be built on a more general and
              | flexible trust region theory that permits the use of inexact gradient and objective evaluations and
              | allows for more general trust region constraints. While the present work mostly focuses on problems
              | with a relatively small parameter space compared to the state space, Appendix C uses concepts from
              | linesearch [71, 143] and subspace [54, 119, 137, 143, 207] methods to remove this restriction. Other
              | research that has considered the more difficult case of a large parameter space employs surrogate
              | models with a variable parametrization [168, 167] in the TRMM framework. Other work that
              | applies reduction to the parameter space include [117, 120]; however, these are not embedded in an
              | adaptation algorithm and cannot establish global convergence.
blank         | 
              | 
title         | 1.4     Thesis Accomplishments and Outline
text          | The contributions of this thesis are divided into two primary contributions and two auxiliary contri-
              | butions. The two primary contributions are: (1) the development of an efficient solver for determin-
              | istic PDE-constrained optimization problems that leverages projection-based reduced-order models
              | and partially converged PDE solutions and (2) the development of an efficient solver for stochastic
              | PDE-constrained optimization problems that leverages projection-based reduced-order models and
              | anisotropic sparse grids. The primary contributions were built on two independent auxiliary contri-
              | butions that have applications that extend well beyond the scope of this thesis: (1) the introduction
              | of a globally convergent, generalized trust region method for managing efficient approximation mod-
              | els and (2) the generalization and extension of minimum-residual projection-based reduced-order
              | models [115, 28, 31, 89] to sensitivity and adjoint PDEs.
              |    The proposed multifidelity trust region method extends the trust region method introduced in
              | [109] by allowing a generalized trust region constraint to be used, provided the approximation model
              | and trust region constraint are related by an asymptotic error bound that mirrors the inexact objec-
              | tive condition in [109]. The asymptotic error conditions on the gradient and objective evaluations
              | are identical to those in [109]. It will be shown that the traditional trust region constraint, i.e.,
              | the ball in RNµ with center µk , trivially satisfies the required asymptotic relationship and therefore
              | the proposed trust region method exactly reduces to the method in [109] under this choice. Global
              | convergence of the proposed generalized trust region method is established and closely follows the
              | convergence theory in [133, 108, 109]. Unlike traditional trust region methods, the non-quadratic
              | trust region constraint eliminates the possibility of using specialized methods to solve the trust
              | region subproblem that automatically satisfy the fraction of Cauchy decrease [48]. As a result,
              | an interior-point method is outlined to solve the trust region subproblem (an optimization prob-
              | lem with a single nonlinear inequality constraint) exactly. While the method is established in the
meta          | CHAPTER 1. INTRODUCTION                                                                             14
blank         | 
              | 
              | 
text          | unconstrained setting, an augmented Lagrangian approach for extending it to nonlinear equality
              | constraints is detailed. This multifidelity trust region method constitutes one of the pillars of this
              | thesis that will be extensively used throughout. The second pillar is the primary PDE approximation
              | technology employed in this work: projection-based model reduction.
              |    While the concept of projection-based model reduction is not new, this work contributes to the
              | understanding of minimum-residual reduced-order models and extends it to apply to sensitivity
              | and adjoint PDEs. In particular, the concept of a minimum-residual reduced-order model for the
              | fully discrete sensitivity and adjoint PDE is introduced and important properties are established.
              | In particular, conditions are established that guarantee the reduced sensitivity and adjoint models
              | agree with the sensitivity and adjoint of the primal reduced-order model and exactly reconstruct the
              | high-dimensional model counterpart. These properties are crucial when the reduced-order model
              | is embedded into the trust region framework as they will be used to establish the error conditions
              | required for convergence. These minimum-residual sensitivity and adjoint reduced-order models, and
              | the surrounding theory, represent a significant contribution as it will be shown they are significantly
              | easier to implement in a large code-base and compute than the sensitivity and adjoint of the primal
              | reduced-order model.
              |    These two technologies—the generalized trust region method and minimum-residual projection-
              | based reduced-order models—serve as pillars for the primary contributions of the thesis: efficient
              | optimization methods for deterministic and stochastic PDE-constrained optimization. The proposed
              | method for deterministic PDE-constrained optimization uses projection-based reduced-order models
              | as the approximation model in the generalized trust region method and residual-based error indi-
              | cators. For additional efficiency, partially converged primal and sensitivity/adjoint solutions are
              | used as snapshots in the construction of the reduced-order models and partially converged primal
              | solutions are used to evaluate the trust region step. The flexibility of the underlying trust region
              | framework is leveraged to ensure the use of partially converged solutions does not hinder conver-
              | gence. The proposed method for stochastic PDE-constrained optimization employs an additional
              | level of inexactness to efficiently integrate quantities of interest over the stochastic space to form
              | risk measures. This lead to the development of the two-level approximation of risk measures of
              | PDE quantities of interest that uses dimension-adaptive anisotropic sparse grids to perform efficient
              | integration in the stochastic space and model reduction for efficient PDE queries at each colloca-
              | tion node. This approximation is embedded in the multifidelity trust region method and global
              | convergence is established by employing a two-level, dimension-adaptive greedy algorithm to simul-
              | taneously construct the sparse grid and reduced-order basis to satisfy required error conditions. The
              | proposed method directly extends the work in [108, 109] that only defines the approximation model
              | using dimension-adaptive sparse grids with PDE queries at collocation nodes performed using the
              | high-dimensional model. It is also similar to [42, 43] that employs the same two-level approximation,
              | but embeds it in an offline-online framework and claims regarding convergence only apply to simple
              | PDEs.
              |    This thesis is organized as follows (Figure 1.3). Chapter 2 provides necessary background on
meta          | CHAPTER 1. INTRODUCTION                                                                                       15
blank         | 
              | 
              | 
text          |                                                Partially Converged
              |                                                                                 Residual-Based Error Bounds
              |                                                  PDE Solutions
              |                                                                                        (Appendix B)
              |                                                    (Chapter 5)
blank         | 
              | 
              | 
              | 
text          |  Deterministic PDE Optimization              Deterministic PDE Op-                 Stochastic PDE Opti-
              |  with ROMs – Many Parameters                 timization with ROMs                 mization with SG-ROMs
              |          (Appendix C)                             (Chapter 5)                           (Chapter 6)
blank         | 
              | 
              | 
              | 
text          |                                   Multifidelity Trust-       Projection-Based
              |  Subspace Optimization Methods                                                    Anisotropic Sparse Grids
              |                                    Region Method             Model Reduction
              |          (Appendix C)                                                                   (Chapter 6)
              |                                      (Chapter 3)               (Chapter 4)
blank         | 
              | 
text          |                                                Trust Region Global              High-Order, Time-Dependent
              |                                                Convergence Theory                    PDE Optimization
              |                                                   (Appendix A)                         (Appendix D)
blank         | 
              | 
text          |                                    Figure 1.3: Organization of thesis
blank         | 
              | 
text          | partial differential equations, their discretization, and PDE-constrained optimization. Chapter 3
              | discusses some necessary elements of optimization theory and introduces the proposed generalized
              | trust region method for leveraging approximation models in an optimization setting. The con-
              | vergence proof for the proposed method is provided in Appendix A. This method will serve as a
              | cornerstone for the efficient solvers developed in Chapters 5 and 6 for deterministic and stochas-
              | tic PDE-constrained optimization. Chapter 4 introduces projection-based model reduction—the
              | approximation models that will eventually define the trust region subproblems—including some
              | novel contributions pertaining to minimum-residual sensitivity and adjoint reduced-order models.
              | Chapter 5 introduces one of the primary contributions of this thesis: the use projection-based
              | reduced-order models in the generalized trust region framework to yield an efficient solver for de-
              | terministic PDE-constrained optimization problems. The potential of the method is demonstrated
              | on a number of problems in computational mechanics, including a large-scale industrial examples
              | of aerodynamic shape design of a full aircraft configuration. The second primary contribution of
              | this thesis is presented in Chapter 6: the extension of the method in Chapter 5 to handle stochastic
              | PDE-constrained optimization problems where an additional level of inexactness is introduced into
              | the approximation model through the use of anisotropic sparse grids to efficiently integrate risk
              | measures of PDE quantities of interest over the stochastic space. While the methods introduced in
              | these chapters implicitly assume the number of parameters is small in comparison to the size of the
              | PDE discretization, Appendix C develops a method to generalize these algorithms to the case where
              | the number of parameters and state variables are comparable. Finally, Chapter 7 offers conclusions
              | and ideas for future research and Appendix D introduces an adjoint method for optimization of
              | time-dependent PDEs, possibly with periodicity constraints, discretized with high-order methods.
title         | Chapter 2
blank         | 
title         | PDE-Constrained Optimization
blank         | 
text          | This chapter provides an overview of parametrized partial differential equations and PDE-constrained
              | optimization that will be used extensively in the remainder of this document. The focus is primarily
              | on static, nonlinear PDEs with either deterministic or stochastic coefficients and boundary condi-
              | tions. The various elliptic and hyperbolic PDEs encountered in this document are also introduced,
              | which include: the 1D viscous and inviscid Burgers’ equation, linear elasticity, the total Lagrangian
              | form of the finite deformation continuum equations, the compressible Euler equations, and the
              | compressible Navier-Stokes equations. The chapter concludes with relevant concepts pertaining to
              | PDE-constrained optimization including: the continuous and discrete version of the optimization
              | problem, full-space and reduced-space solvers, gradient computations in the reduced-space approach
              | via the sensitivity and adjoint method, and approaches to handle side constraints, i.e., optimization
              | constraints other than the PDE constraint itself.
blank         | 
              | 
title         | 2.1     Parametrized Partial Differential Equations
text          | Consider a system of partial differential equations of the form: find U such that
blank         | 
text          |                             ∂U
              |                                + G(U, ∇U ) = g(x, t)       x ∈ B,   t∈T
              |                             ∂t
              |                                 H(U, ∇U ) = h(x, t)        x ∈ ∂B, t ∈ T                        (2.1)
blank         | 
text          |                                     U (x, t0 ) = U0 (x)    x∈B
blank         | 
text          | where T = (t0 , tf ) ⊂ R+ is the temporal domain, B ⊂ Rnsd is the spatial domain with boundary ∂B,
              | U (x, t) is the unknown state vector with nc components, G and H are first-order spatial differential
              | operators, g and h are volumetric and boundary source terms, and U0 : B → Rnsv is the initial
              | data. In the most general case, the domain B can be time-dependent, leading to rigid and deforming
              | domain problems. In such settings, an arbitrary Lagrangian-Eulerian description of the PDE can
              | be employed to transform the equations to a fixed domain; see Appendix D for additional details.
blank         | 
              | 
              | 
meta          |                                                   16
              | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                         17
blank         | 
              | 
              | 
text          |    From the solution of the partial differential equation (U ), relevant Quantities of Interest (QoIs)1
              | are defined as space-time integrals of various solution-dependent quantities over the domain. Quan-
              | tities of interest are essential from a practical perspective as they provide metrics to quantify the
              | performance and behavior of the system under consideration. In this work, QoIs will take the form
              |                                 Z Z                   Z Z
              |                       F(U ) =         fB (U ) dV dt +      f∂B (U ) dA dt,                    (2.2)
              |                                        T   B                   T   ∂B
blank         | 
              | 
text          | where fB and f∂B are relevant pointwise quantities. The form in (2.2) is general in that it encom-
              | passes integrals over subsets of the spatial and temporal domains, as well as pointwise quantities at
              | fixed spatial locations or times. This results from the lack of regularity imposed on fB and f∂B that
              | allows for the use of indicator or Dirac functions.
              |    In the remainder of this document, the primary interest will be in the behavior of solutions
              | (U ) and QoIs (F(U )) of the PDE under perturbations to data of the problem—the domain (B)
              | and boundary (∂B), source terms (g and h), initial condition (U0 ), or coefficients defining the
              | differential operators G and H. In subsequent sections, these parameters of the partial differential
              | equation will be the optimization variables whose values will be sought such that the objective QoI is
              | minimized and other QoI-based constraints are satisfied. Before proceeding to the discussion of PDE-
              | constrained optimization, the various PDEs considered in this document are introduced and details
              | regarding the discretization of parametrized partial differential equations and the corresponding
              | quantities of interest are discussed.
blank         | 
              | 
title         | 2.1.1     Examples
text          | This section provides specific examples of partial differential equations (2.1) and quantities of interest
              | (2.2) that will be encountered in this thesis. While the examples are mostly from the fields of solid
              | and fluid mechanics, this is not a fundamental restriction in any of the subsequent developments.
blank         | 
title         | Linear Elasticity
blank         | 
text          | Consider a solid body B ⊂ Rnsd subject to distributed body forces b(x, t) with boundary ∂B de-
              | composed into two parts: ∂Bu and ∂Bt such that ∂B = ∂Bu ∪ ∂Bt . Displacements are prescribed
              | along ∂Bu and ∂Bt is subject to prescribed traction forces. Under the assumption that the result-
              | ing deformations are infinitesimal and the pointwise stress and strain are related through a linear
              | relationship, the deformation of the body is governed by the following system of partial differential
              | equations
              |                                        ρü = ∇ · σ + b      x ∈ B, t ∈ T
              |                                         u = ū              x ∈ ∂Bu , t ∈ T                                  (2.3)
              |                                     σ · n = t̄              x ∈ ∂Bt , t ∈ T ,
              |   1 Quantity of Interest (singular) will be abbreviated QoI and Quantities of Interest (plural) will be abbreviated
blank         | 
meta          | QoIs.
              | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                  18
blank         | 
              | 
              | 
text          | where u(x, t) ∈ Rnsd is the pointwise deformation and state vector of the PDE, σ(x, t) ∈ Rnsd ×nsd
              | is the symmetric stress tensor, ρ(x, t) ∈ R+ is the density of the material that comprises B, b(x, t) ∈
              | Rnsd is the body force, ū(x, t) ∈ Rnsd is the prescribed displacement on ∂Bu , t̄(x, t) ∈ Rnsd is the
              | prescribed traction on ∂Bt , and n(x) ∈ Rnsd is the pointwise outward normal to the boundary. The
              | system of PDEs is closed with the stress-strain relationship (Hooke’s law)
blank         | 
text          |                                                 σ = C : ,                                            (2.4)
blank         | 
text          | where C ∈ Rnsd ×nsd ×nsd ×nsd is the elasticity tensor with major and minor symmetry and (x, t) ∈
              | Rnsd ×nsd is the strain tensor. The kinematic constraint relates the deformation to strain
blank         | 
text          |                                                 1
              |                                                   ∇u + ∇uT .
blank         |                                                           
text          |                                            =                                                         (2.5)
              |                                                 2
blank         | 
text          | Remark. The system in (2.3) does not strictly fit into the form in (2.1) due to presence of the
              | second-order temporal derivative, i.e., the inertial term. This can be remedied by introducing the
              | velocity v = u̇ and defining the state vector U = (u, v). This will not preserve the structure of the
              | governing equations and they are usually treated directly in their second-order form.
blank         | 
text          |    There are number of relevant quantities of interest in linear elasticity including: (1) pointwise
              | displacement magnitude, (2) pointwise stress measures, (3) mass/volume, and (4) global stiffness,
              | to name a few. The volume of the structure and its global stiffness are defined as
              |                        Z                           Z              Z
              |                    V =    dV       and     S(u) =     uk bk dV +      uk t̄k dA,                      (2.6)
              |                              B                            B              ∂B
blank         | 
text          | respectively, where summation from 1 to nsd over repeated indices is implied. The volume is a
              | purely geometric quantity of interest as it does not depend on the solution of the partial differential
              | equation.
blank         | 
title         | Finite Deformation Continuum Mechanics
blank         | 
text          | The system of partial differential equations that governs the physical setup of the previous section
              | in the general case, that is, without the linearity assumption, is
blank         | 
text          |                                     ρü = ∇ · P + b      X ∈ B, t ∈ T
              |                                       u = ū             X ∈ ∂Bu , t ∈ T                              (2.7)
              |                                  P · N = t̄              X ∈ ∂Bt , t ∈ T
blank         | 
text          | where ρ, u, b, ū, t̄ are defined in the previous section on linear elasticity, P (X, t) is the first Piola-
              | Kirchhoff stress tensor, and N (X) is the pointwise outward normal to the boundary in the reference
              | configuration (B). The equations are closed with a general constitutive relationship
blank         | 
text          |                                                 P = P (F ),                                           (2.8)
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                                    19
blank         | 
              | 
              | 
text          |                 ∂u
              | where F = I +       is the deformation gradient. The governing equations in (2.7), posed on the
              |                 ∂X
              | reference or undeformed configuration (B), are called the total Lagrangian form [19]. The equations
              | can be transformed to the current or physical configuration using the diffeomorphism: x(X, t) :=
              | X + u(X, t), but the so-called updated Lagrangian form will not be considered in this document.
              | The quantities of interest from the previous section (2.6) will also be used here.
blank         | 
title         | General Conservation Laws
blank         | 
text          | The next sequence of partial differential equations considered take the form viscous or inviscid
              | conservation laws
blank         | 
text          |                              ∂U
              |                                 + ∇ · FI (U ) + ∇ · FV (U, ∇U ) = g(x, t)                 x∈B                            (2.9)
              |                              ∂t
blank         | 
text          | where FI is the inviscid flux, FV is the viscous flux, and g is a source term. Hyperbolic systems
              | of partial differential equations of this form describe propagation phenomena such as those in fluid
              | dynamics and electromagnetics.
blank         | 
title         | 1D Inviscid Burgers’ Equation
blank         | 
text          | The first and simplest conservation law considered is the 1D inviscid Burgers’ equation with an
              | inflow boundary condition, which describes shock propagation of a conserved variable, u
blank         | 
text          |                                   ∂u      ∂u
              |                                      +u        = g(x, t)         x ∈ (xl , xr ), t ∈ (t0 , tf )
              |                                   ∂t      ∂x                                                                            (2.10)
              |                                      u(xl , t) = h(t)            t ∈ (t0 , tf ).
blank         | 
text          | This constitutes a conservation law of the form (2.9) where the conserved variable, inviscid flux, and
              | viscous flux are
              |                                                                 u2
              |                              U := u                FI (U ) :=                 FV (U, ∇U ) := 0.
              |                                                                 2
              | Two quantities of interest for Burgers’ equation are: the regularized tracking-type functional and
              | the amount of the conserved variable that exits the domain through the outflow boundary
blank         | 
text          |                              tf       xr                                                           tf
              |                      1
              |                          Z        Z                                                          Z
              |                                             (u − ū)2 + αg 2 dx dt
blank         |                                                            
text          |            T (u) =                                                        and      R(u) =               u(xr , t) dt,
              |                      2   t0       xl                                                              t0
blank         | 
              | 
text          | respectively, where ū is a target state and α > 0 is a prescribed regularization constant.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                     20
blank         | 
              | 
              | 
title         | Compressible Navier-Stokes Equations
blank         | 
text          | The compressible Navier-Stokes equations govern viscous fluid flow in a domain B and take the form
blank         | 
text          |                                            ∂ρ      ∂
              |                                               +       (ρui ) = 0,                                        (2.11)
              |                                            ∂t     ∂xi
              |                               ∂             ∂                     ∂τij
              |                                  (ρui ) +     (ρui uj + p) = +           for i = 1, 2, 3,                (2.12)
              |                               ∂t          ∂xi                     ∂xj
              |                             ∂            ∂                        ∂qj      ∂
              |                                (ρE) +        (uj (ρE + p)) = −         +      (uj τij ),                 (2.13)
              |                             ∂t          ∂xi                       ∂xj     ∂xj
blank         | 
text          | where ρ is the fluid density, u1 , u2 , u3 are the velocity components, and E is the total energy. The
              | viscous stress tensor and heat flux are given by
blank         |                                                                                             
text          |                    ∂ui   ∂uj   2 ∂uk                                    µ ∂            p 1
              |      τij = µ           +     −       δij            and          qj = −             E + − uk uk .        (2.14)
              |                    ∂xj   ∂xi   3 ∂xk                                    Pr ∂xj         ρ 2
blank         | 
text          | Here, µ is the viscosity coefficient and Pr = 0.72 is the Prandtl number which we assume to be
              | constant. For an ideal gas, the pressure p has the form
blank         |                                                               
text          |                                                          1
              |                                          p = (γ − 1)ρ E − uk uk ,                                        (2.15)
              |                                                          2
blank         | 
text          | where γ is the adiabatic gas constant. All walls have no-slip boundary conditions, i.e., ui = 0 for
              | i = 1, 2, 3. Equations (2.11)-(2.13) can be written in conservation form as
              |                                                                                                   
              |                             ρ                                  ρu1           ρu2               ρu3
              |                                                                                     
              |                         ρu1                         p + ρu2     ρu1 u2     ρu1 u3 
              |                                                            1                        
              |                     U = ρu2              FI (U ) =  ρu1 u2     p + ρu22    ρu2 u3                    (2.16)
              |                                                                                     
              |                                                                                     
              |                         ρu                          ρu u                         2 
              |                            3                            1 3     ρu 2 u 3  p  + ρu3  
              |                           ρE                          u1 (E + p) u2 (E + p) u3 (E + p)
blank         | 
text          |                                                                                           
              |                                                    0                 0             0
              |                                                                                           
              |                                            −τ11                 −τ21          −τ31        
              |                                                                                           
              |                             FV (U, ∇U ) =  −τ12                 −τ22          −τ32        .            (2.17)
              |                                                                                           
              |                                                                                           
              |                                            −τ                   −τ23          −τ33        
              |                                                13                                         
              |                                            q1 − ui τi1         q2 − ui τi2   q3 − ui τi3
blank         | 
text          | While there are a plethora of quantities of interest in fluid dynamics, the most relevant quantities
              | tend to be time-averaged integrated forces and moments on surfaces, particularly in aerodynamics
              | applications. The time-averaged force in the ith direction on a surface ∂Bw takes the form
blank         | 
text          |                                  1
              |                                     Z Z
              |                           Fi =              (p + ρui uj nj − τji nj ) dA dt.                             (2.18)
              |                                |T | T ∂Bw
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             21
blank         | 
              | 
              | 
title         | Compressible Euler Equations
blank         | 
title         | The compressible Euler equations
blank         | 
text          |                                             ∂ρ      ∂
              |                                                +       (ρui ) = 0,                               (2.19)
              |                                             ∂t     ∂xi
              |                                ∂             ∂
              |                                   (ρui ) +     (ρui uj + p) = 0 for i = 1, 2, 3,                 (2.20)
              |                                ∂t          ∂xi
              |                              ∂            ∂
              |                                 (ρE) +        (uj (ρE + p)) = 0                                  (2.21)
              |                              ∂t          ∂xi
blank         | 
text          | model an inviscid fluid. The Navier-Stokes equations in (2.11)-(2.13) reduce to the compressible
              | Euler equations above in the limit of no viscosity, i.e., µ → 0. The conservation form is identical
              | to the Navier-Stokes case with FV (U, ∇U ) := 0 and the time-averaged force on a surface ∂Bw are
              | defined as
              |                                              1
              |                                                    Z Z
              |                                      Fi =                    (p + ρui uj nj ) dA.                (2.22)
              |                                             |T |   T   ∂Bw
blank         | 
              | 
title         | Compressible Navier-Stokes Equations—Isentropic Assumption
blank         | 
text          | In situations where the entropy in the system is constant, i.e., adiabatic and reversible, the Navier-
              | Stokes equations can be simplified to its isentropic form. For a perfect gas, the entropy is defined
              | as
              |                                              s = p/ργ = constant,                                (2.23)
blank         | 
text          | which explicitly relates the pressure and density of the flow, rendering the energy equation redundant
              | and leads to
blank         | 
text          |                                      ∂ρ     ∂
              |                                          +      (ρui ) = 0,                                      (2.24)
              |                                      ∂t    ∂xi
              |                          ∂            ∂                     ∂τij
              |                             (ρui ) +     (ρui uj + p) = +                 for i = 1, 2, 3.       (2.25)
              |                          ∂t          ∂xi                    ∂xj
blank         | 
text          | This effectively reduces the square system of PDEs of size nsd + 2 to one of size nsd + 1. It can
              | be shown, under suitable assumptions, that the solution of the isentropic approximation of the
              | Navier-Stokes equations converges to the solution of the incompressible Navier-Stokes equations as
              | the Mach number approaches zero [118, 51, 64]. In conservation form, the compressible, isentropic
              | Navier-Stokes equations are
              |                                                                                           
              |                              ρ                                   ρu1        ρu2        ρu3
              |                                                          p + ρu21
              |                                                                                           
              |                        ρu1                                              ρu1 u2     ρu1 u3 
              |                     U =                      FI (U ) =                                       (2.26)
              |                        ρu 
              |                         2
              |                                                           ρu u
              |                                                           1 2           p + ρu22    ρu2 u3 
              |                         ρu3                                ρu1 u3         ρu2 u3    p + ρu23
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                22
blank         | 
              | 
              | 
text          |                                                                         
              |                                                     0      0         0
              |                                                −τ11     −τ21      −τ31 
              |                                                                        
              |                                  FV (U, ∇U ) = 
              |                                                −τ
              |                                                                         .                         (2.27)
              |                                                 12      −τ22      −τ32 
              |                                                                         
              |                                                 −τ13     −τ23      −τ33
blank         | 
text          | and the time-averaged, integrated forces are computed according to (2.18).
blank         | 
              | 
title         | 2.1.2     Discretization: Parametrization
text          | The primary interest in this document is not the study of partial differential equations themselves,
              | rather the behavior of PDE solutions (U ) and QoIs (F(U )) under perturbations to the PDE itself,
              | e.g., the domain (B), source terms (g and h), and coefficients of the differential operators (usually
              | manifest as material properties in physical problems). This will lead naturally to the discussion of
              | optimization in the next section where we seek to find the PDE domain, source term, and coefficients
              | that minimizes some QoI and meets performance constraints on other QoIs.
              |    In general, the quantities defining the PDE lie in infinite-dimensional function spaces, which are
              | not convenient or practical to work with in a computational setting. Furthermore, it is difficult to
              | design practical and relevant perturbation strategies in these spaces that will be useful in engineering
              | and scientific applications. Accordingly, the remainder of this section discusses the finite-dimensional
              | parametrization of the partial differential equation in (2.1). This will entail the definition of a vector
              | of Nµ parameters, µ ∈ RNµ , and a precise description of the dependence of the PDE on µ. In
              | general, the parameters can be decomposed as µ = (µB , µg , µh , µG , µH , µU0 ), where
blank         | 
text          |                            B = B(µB )                           U0 (x) = U0 (x, µU0 )
              |                       g(x, t) = g(x, t, µg )                   h(x, t) = h(x, t, µh )              (2.28)
              |                   G(U, ∇U ) = G(U, ∇U, µG )              H(U, ∇U ) = H(U, ∇U, µH ).
blank         | 
text          | All quantities are assumed to be continuously differentiable with respect to their respective param-
              | eters. This level of granularity is not significant for this document, but must be exploited in a
              | computational setting for an efficient implementation. Therefore, only the monolithic vector µ is
              | considered and the PDE dependence on this parameter takes the form
blank         | 
text          |                              B = B(µ)                           U0 (x) = U0 (x, µ)
              |                        g(x, t) = g(x, t, µ)                    h(x, t) = h(x, t, µ)                (2.29)
              |                     G(U, ∇U ) = G(U, ∇U, µ)              H(U, ∇U ) = H(U, ∇U, µ).
blank         | 
text          | The various quantities in (2.28)-(2.29) are fundamentally different and specialized techniques have
              | been developed to parametrize each. In the remainder of this section, a few techniques are discussed
              | that are relevant to the shape and topology parametrization of B and parametrization of space-time
              | functions such as the source terms and initial conditions, as they will be most relevant to problems
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              23
blank         | 
              | 
              | 
text          | encountered in subsequent chapters.
blank         | 
title         | Parametrization of spatial functions
blank         | 
text          | The first type of operator that arises in PDE applications, particularly in the context of optimal
              | and distributed control [214, 190], that requires parametrization are spatial functions such as the
              | source terms in (2.1). This section seeks to define a parameter vector µ such that the set {g(x, µ) |
              | µ ∈ RNµ } includes a relevant set of scalar-valued functions with some level of regularity for a given
              | application. Only scalar-valued functions will be considered in this section as vector-valued function
              | can be parametrized through the parametrization each component—with either the same or different
              | parameters for each.
              |    Local interpolation is a general strategy for parametrizing spatial functions in nsd dimensions.
              | In this setting, the domain B is decomposed into elements of standard shapes—such as simplices
              | or hyper-rectangles—and a set of polynomials of a given degree are introduced over each element.
              | The coefficients of each polynomial in the discretization of B comprise the parameter vector and the
              | parametrized spatial function takes the form
blank         | 
text          |                                                     Nµ
              |                                                     X
              |                                         g(x, µ) =         µI NI (x)                              (2.30)
              |                                                     I=1
blank         | 
              | 
text          | where NI are the shape functions and µI are the components of µ. While a parametrization of this
              | form can be applied in any number of spatial dimensions and has a built-in refinement mechanism (by
              | subdividing the elements in the discretization and defining polynomials over the new elements), it
              | can lead to large parameter vectors, i.e., Nµ  1. An additional benefit of such a parametrization is
              | the use of an unstructured discretization of B to refine regions where increased resolution is required,
              | if such information is known.
              |    On the other end of the spectrum lie global interpolation methods where the interpolant is
              | defined based on information from the entire domain B or parameter vector µ. Cubic splines in one
              | dimension fall into this category—the parameter vector defines the value of the spline at “knots”
              | and an interpolant is constructed that passes through these values and satisfies boundary conditions.
              | In higher dimensions, radial basis functions [27] serve a similar purpose.
blank         | 
title         | Parmetrization of spatio-temporal functions
blank         | 
text          | In time-dependent applications, spatio-temporal functions are often used to describe source terms,
              | boundary conditions, or even domain deformations. To optimize over these types of terms, they must
              | be parametrized with a finite number of parameters. One option is to consider the domain B × T
              | as a domain in nsd + 1 dimensions and apply the local interpolation method of the previous section.
              | Alternatively, the spatio-temporal function can be defined and parametrized using a separation of
              | variables approach
              |                                     g(x, t, µ) = gs (x, µ)gt (t, µ),                             (2.31)
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                               24
blank         | 
              | 
              | 
text          | where gs is a parametrized spatial function in Rnsd and gt is a parametrized univariate function. Any
              | of the methods discussed in the previous section can be employed to parametrize gs and gt . This
              | approach enables certain spatial or temporal requirements—such as periodicity—to be explicitly
              | enforced in the parametrization of g(x, t, µ) through the selection of gs and gt . For example, if gt (t)
              | is taken as a periodic function of period T , then g( · , t, · ) is guaranteed to be periodic with period
              | T.
blank         | 
title         | Shape parametrization of domain, B
blank         | 
text          | Parametrization of the shape (at a fixed topology) of two- and three-dimensional objects with a finite
              | number of intuitive parameters is essential in computer graphics as well as a number of engineering
              | disciplines, usually in the context of design. A plethora of shape parametrization techniques exist
              | [99, 177, 9, 61], each with strengths and weaknesses. These methods can be divided into two
              | distinct classes: (1) those that parametrize B directly and (2) those that parametrize the boundary
              | ∂B := B \ B and extend the deformation to the interior—usually by solving an auxiliary PDE.
              |      Methods that parametrize B directly define an analytical mapping
blank         | 
text          |                                         ϕ : Rnsd × RNµ → Rnsd                                     (2.32)
blank         | 
text          |                                                        0                   0
              | that maps the reference domain B to the new shape B , i.e., ϕ(B, µ) = B . These methods are usually
              | easily parallelized as they involve local operations, i.e., given µ, any subset v ⊂ B gets transformed
              |                                                                                          0
              | as ϕ(v, µ) independent of the action of ϕ on B \ v. Smoothness of the new shape, B is guaranteed
              | from the smoothness of the original shape and mapping. In many cases, a mapping of the form (2.32)
              | can be defined analytically given a geometry of interest and requirements of the parametrization.
              | For example, the camber of the NACA0012 airfoil in Figure 2.1 can be parametrized with three
              | parameters using a Gaussian of the form
blank         | 
text          |                                                                     2
              |                                       ϕ(X, µ) = µ1 e−µ2 (X1 −µ3 )                                 (2.33)
blank         | 
text          | where µ1 , µ2 , µ3 control the magnitude, sharpness, and center of the camber, respectively; see
              | the shape corresponding to µ1 = 0.2, µ2 = 2.0, µ3 = 0.0 in Figure 2.1. While this approach is
              | trivial to parallelize and can lead to highly intuitive parameters, it can be cumbersome for complex
              | 3D geometries and requires considerable expertise in designing parameters. Another approach for
              | parametrizing B directly that is extremely popular in the computer graphics community is known
              | as Free Form Deformation (FFD) [179]. In this method, a nsd -dimensional lattice of control points
              | define an analytic function on the interior of the lattice, which can be extended to the entire space.
              | In this setting, the displacement of the control nodes of the lattice are the parameters, which induce
              | a deformation on the volume enclosed by the lattice and thus any body embedded in it. Figure 2.2
              | shows a circle parametrized with FFD based on B-splines, including the undeformed and deformed
              | geometry and FFD lattice. While FFD is more general and flexible than manual parametrization
              | and nearly as parallelizable, it may quickly lead to a large number of parameters, which may lead to
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                               25
blank         | 
              | 
              | 
              | 
text          | Figure 2.1: Left: Undeformed NACA0012 airfoil and surrounding triangular mesh. Right: Deforma-
              | tion of R2 according to mapping ϕ in (2.33) that deforms the NACA0012 geometry and surrounding
              | mesh.
blank         | 
              | 
text          | slower convergence in the context of optimization. When gradient-based optimization techniques are
              | employed, this trade-off is usually worthwhile, particularly in aerodynamic applications. The number
              | of parameters may be reduced by combining the manual parametrization with FFD techniques, that
              | is, introduce a FFD lattice to control the underlying geometry and a manual parametrization that
              | controls the FFD lattice nodes. Figures 2.3 shows the parametrization of a model of a Volkswagen
              | Passat using FFD with two relevant and intuitive shape parameters—the height of the roof and
              | taper of the trunk. Figure 2.4 shows the parametrization of the Common Research Model (CRM)
              | geometry with one intuitive parameter—the dihedral of the wing.
              |    The other class shape parametrization methods defines a parametrization of the boundary ∂B and
              | propagates the deformation to the interior B, usually via the solution of a partial differential equation
              | such as linear or nonlinear elasticity with prescribed displacement on ∂B [58, 155]. Analytical
              | methods such as splines (nsd = 2) or Non-Uniform Rational B-Splines (NURBS) patches (nsd = 3)
              | are commonly used for the surface parametrization. Another popular method uses the design element
              | concept where a finite element mesh is defined such that it encloses the geometry of interest, ∂B,
              | and the finite element shape functions define the deformation of the enclosed volume2 . Figure 2.5
              | provides an example of a NACA0012 airfoil parametrized with a single cubic design element.
              |    All of the parametrization methods considered in this section are useful in parametrizing the
              | shape of an object with a fixed topology. Methods for parametrizing the topology of a domain will
              | be discussed in the next section—they are fundamentally different and inevitably lead to a large
              | number of parameters, Nµ  1.
blank         | 
title         | Topology parametrization of domain, B
blank         | 
text          | Two prevailing methods are available for parametrizing the topology of a domain, B: (1) density-
              | based methods [181, 20] and (2) level set methods [192]. Density methods define the topology of the
              | domain using an indicator function
              |                                                 χ : Rnsd → {0, 1},                                (2.34)
              |   2 The   design element concept can also be used to directly parametrize B.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                          26
blank         | 
              | 
              | 
              | 
text          | Figure 2.2: Top left: Undeformed geometry of a circle (blue) and a FFD lattice (gray). Top center :
              | Perturbation of FFD control nodes according to an x-directed elongation mode and resulting shape
              | of the circle. Top right: Perturbation of FFD control nodes according to a bending mode and
              | resulting shape of the circle. Bottom: Local perturbations to individual FFD control nodes in the y
              | direction and the resulting shape of the circle.
blank         | 
              | 
text          | where χ(x) = 1 if x ∈ B and χ(x) = 0 otherwise. The topology of B is then parametrized by
              | parametrizing the function χ using any of the methods previously discussed. The most common
              | approach to parametrize χ is to partition a subset of Rnsd into Nµ elements or patches of finite
              | volume and define χ to be constant within each element k with value µk ∈ {0, 1}3 . Figures 2.6 –
              | 2.9 show the topology of a cantilever, cube, and lacrosse head parametrized with a density-based
              | approach that uses a constant value of χ in each element. This approach has the advantage of a
              | simple implementation, but smooth topologies can only be obtained if an extremely large number
              | of elements are used, i.e., Nµ  1.
              |    Conversely, level set methods define the topology implicitly by identifying all surfaces or interfaces
              | as the zero level-set of an implicit function,
blank         | 
text          |                                                      φ : Rnsd → R,                                            (2.35)
blank         | 
text          | where B = {x ∈ Rnsd | φ(x) ≤ 0} and ∂B = {x ∈ Rnsd | φ(x) = 0}. The parametrization
              | of the spatial function φ using any of the techniques previously discussed leads to the topology
              | parametrization.
blank         | 
              | 
title         | 2.1.3        Discretization: Governing Equations
text          | With the techniques described in the previous section, the parametrization of the PDE can be
              | encoded in the finite-dimensional vector µ ∈ RNµ that contains all types of parameters considered.
              |   3 It   is often necessary to relax the range of µk to [0, 1] to obtain a continuous optimization problem.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              27
blank         | 
              | 
              | 
              | 
text          | Figure 2.3: Free form deformation lattices and Volkswagen Passat geometry: (left) undeformed
              | configuration, (top right) deformed configuration with lowered roof, and (bottom right) deformed
              | configuration with steeply tapered trunk.
blank         | 
              | 
              | 
              | 
text          | Figure 2.4: Free form deformation lattice and Common Research Model (CRM) geometry: (left)
              | undeformed configuration and (right) deformed configuration with positive dihedral.
blank         | 
              | 
text          | The partial differential equation in (2.1) under the finite-dimensional parametrization takes the form:
              | for any µ ∈ RNµ , find U such that
blank         | 
text          |                         ∂U
              |                            + G(U, ∇U, µ) = g(x, t, µ)         x ∈ B(µ),   t∈T
              |                         ∂t
              |                             H(U, ∇U, µ) = h(x, t, µ)          x ∈ ∂B(µ), t ∈ T                   (2.36)
blank         | 
text          |                                 U (x, t0 , µ) = U0 (x, µ)     x ∈ B(µ).
blank         | 
text          | At this point, the parametrized PDE in (2.36) will be discretized in the usual two-step manner:
              | discretization in space, i.e., semi-discretization, to yield a system of Ordinary Differential Equations
              | (ODEs) and subsequent temporal discretization. A less commonly used alternative is to employ a
              | monolithic space-time discretization. Given the generality of the differential operators in (2.36), it
              | is inappropriate to commit to a single spatial discretization method given the myriad of possibilities
              | including finite differences, finite volumes, finite elements, and discontinuous Galerkin and spectral
              | methods. The most appropriate method depends on a number of factors including the properties
              | of the spatial operators in (2.36), regularity of the solution U , and the complexity of the domain
              | B. Finite volume, finite element, and discontinuous Galerkin methods will be used to discretize the
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              28
blank         | 
              | 
              | 
              | 
text          |                       (a) µ1 = 0.1                                   (b) µ2 = 0.1
blank         | 
              | 
              | 
              | 
text          |                       (c) µ3 = 0.1                                   (d) µ4 = 0.1
blank         | 
              | 
              | 
              | 
text          |                       (e) µ5 = 0.1                                   (f) µ6 = 0.1
blank         | 
              | 
              | 
              | 
text          |                       (g) µ7 = 0.1                                   (h) µ8 = 0.1
blank         | 
text          | Figure 2.5: Shape parametrization of a NACA0012 airfoil using a cubic design element. Blue nodes
              | and lines designate the undeformed design element and shape and black nodes and lines designate
              | the deformed design element and shape.
blank         | 
              | 
text          | various PDEs that arise in this work. At this point, an unspecified spatial discretization is applied
              | to the parametrized PDEs in (2.36) to yield the nonlinear system of ODEs: for any µ ∈ RNµ , find
              | u such that
              |                                      M u̇ = r(u, t, µ)         t∈T
              |                                                                                                  (2.37)
              |                                      u(0) = u0 (µ)
blank         | 
text          | where u( · ) ∈ RNu is the semi-discrete state vector, M ∈ RNu ×Nu is the mass matrix, r : RNu ×R+ ×
              | RNµ → RNu is the nonlinear function that encodes the spatial discretization of the partial differential
              | equation and boundary conditions in (2.36), and u0 (µ) ∈ RNu is the parameter-dependent initial
              | condition that arises from the spatial discretization of U0 (x, µ). If the partial differential equation
              | in (2.36) is steady or static, i.e., U,t = 0, (2.37) becomes
blank         | 
text          |                                               r(u, µ) = 0                                        (2.38)
blank         | 
text          | and the discretization is complete.
blank         | 
text          | Remark. As written, the mass matrix M is time- and parameter-independent, which will not be
              | the case if the domain is time- or parameter-dependent or, for example, if corotator-based shell
              | elements are used in a finite element discretization of structural problems [19]. In the first case, the
              | mass matrix can be completely fixed by using an arbitrary Lagrangian-Eulerian mapping to a fixed
              | reference domain [211]; see Appendix D for details.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                            29
blank         | 
              | 
              | 
              | 
text          | Figure 2.6: Left: Quadrilateral mesh of a subset of R2 corresponding to a rectangle (160 × 100
              | elements) whose topology is parametrized by a density-based method. Right: An example of an
              | admissible topology of the density-based topological parametrization—an optimized cantilever de-
              | signed to maximize the global stiffness of the structure under a vertical load at the right end.
blank         | 
              | 
              | 
              | 
text          | Figure 2.7: Left: Quadrilateral mesh of a subset of R2 corresponding to a rectangle (160 × 100
              | elements) with a hole whose topology is parametrized by a density-based method. Right: An example
              | of an admissible topology of the density-based topological parametrization—a Michell structure
              | [37, 94] designed to maximize the global stiffness of the structure under a vertical load at the right
              | end.
blank         | 
              | 
              | 
              | 
text          | Figure 2.8: Left: Hexahedral mesh of a subset of R3 corresponding to a cube (35 × 35 × 35 elements)
              | whose topology is parametrized by a density-based method. Right: An example of an admissible
              | topology of the density-based topological parametrization—a trestle designed to maximize the global
              | stiffness of the structure under a vertical load.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                       30
blank         | 
              | 
              | 
              | 
text          | Figure 2.9: Left: Tetrahedral mesh of a subset of R3 corresponding to an unoptimized lacrosse
              | head (475, 666 elements) whose topology is parametrized by a density-based method. Right: An
              | example of an admissible topology of the density-based topological parametrization—an unconverged
              | maximum stiffness topology. The entire object is included in the top row and the bottom row is a
              | slice to show internal voids in the optimized shape.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                   31
blank         | 
              | 
              | 
text          |           Table 2.1: Butcher Tableau for s-stage diagonally implicit Runge-Kutta scheme
blank         | 
text          |                                            c1     a11
              |                                            c2     a21     a22
              |                                            ..      ..      ..     ..
              |                                             .       .       .       .
              |                                            cs     as1     as2     ···        ass
              |                                                   b1      b2      ···        bs
blank         | 
              | 
text          |    For time-dependent problems, the system of ODEs is discretized to yield the complete dis-
              | cretization: a sequence of algebraic, nonlinear systems of equations. The two prevailing classes of
              | high-order implicit temporal integration schemes are: (a) Backward Differentiation Formulas (BDF)
              | and (b) Implicit Runge-Kutta (IRK). BDF schemes are multistep schemes that have the general
              | form
              |                                            n−1
              |                                            X
              |                                M u(n) −           αi M u(i) = κ∆tr(u(n) , tn , µ)                     (2.39)
              |                                             i=0
blank         | 
text          | where αi and κ are constants that define different schemes, such as (1) BDF1 (backward Euler):
              | κ = αn−1 = 1 and α0 = · · · αn−2 = 0, (2) BDF2: κ = 2/3, αn−1 = 4/3, αn−2 = −1/3, α0 =
              | · · · = αn−3 = 0, and (3) BDF3: κ = 6/11, αn−1 = 18/11, αn−2 = −9/11, αn−3 = 2/11, α0 =
              | · · · = αn−4 = 0. They are popular since high-order accuracy can be achieved at the cost of a single
              | nonlinear solve of size Nu at each time step. However, they suffer from initialization issues and are
              | limited to second-order accuracy, if A-stability is required. In contrast, IRK schemes are single-step
              | methods that can be A-stable and arbitrarily high-order, at the cost of solving an enlarged nonlinear
              | system of equations of size s · Nu , for an s-stage scheme, at each time step. For practical problems,
              | this can be prohibitively expensive, in terms of memory and CPU time.
              |    A particular subclass of the IRK schemes, known as Diagonally Implicit Runge-Kutta (DIRK)
              | schemes [3], are capable of achieving high-order accuracy with the desired stability properties, with-
              | out requiring the solution of an enlarged system of equations. The DIRK schemes are defined by a
              | lower triangular Butcher tableau (Table 2.1) and take the following form when applied to (2.37)
blank         | 
text          |                                     u(0) = u0 (µ)
              |                                                            s
              |                                                                        (n)
              |                                                            X
              |                                     u(n) = u(n−1) +              bi ki                                (2.40)
              |                                                            i=1
blank         |                                                                         
text          |                                      (n)            (n)
              |                                  M ki      = ∆tn r ui , tn−1 + ci ∆tn , µ ,
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s, where Nt are the number of time steps in the temporal discretiza-
              | tion and s is the number of stages in the DIRK scheme. The temporal domain T is discretized into
              | Nt segments with endpoints {t0 , t1 , . . . , tNt }, with the nth segment having length ∆tn = tn − tn−1
              |                                                     (n)
              | for n = 1, . . . , Nt . Additionally, in (2.40), ui       is used to denote the approximation of u(n) at the
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             32
blank         | 
              | 
              | 
text          | ith stage of time step n
blank         | 
text          |                                                                                i
              |                         (n)       (n)           (n)                                      (n)
              |                                                                                X
              |                     ui        = ui (u(n−1) , k1 , . . . , ks(n) ) = u(n−1) +         aij kj .   (2.41)
              |                                                                                j=1
blank         | 
              | 
text          | From (2.40), a complete time step requires the solution of a sequence of s nonlinear systems of
              | equation of size Nu .
              |    From this exposition on the spatio-temporal discretization of the parametrized partial differential
              | equation in (2.36), finding the solution of the continuous form of the equations—a task that requires
              | searching an infinite-dimensional trial space for the solution U —has been reduced the task of finding
              | the solution of the algebraic nonlinear system of equations in (2.38) or a sequence of such equations.
              | The solution, u, of the algebraic equations can be used, along with the shape functions underlying
              | the spatio-temporal discretization, to reconstruct an approximation to the solution U (x, t).
blank         | 
text          | Remark. One option to treat second-order temporal problems, such as those in (2.3) and (2.7) is to
              | recast them in first-order form, as discussed in the previous section, and apply a BDF or IRK/DIRK
              | scheme, as developed in this section. However, it is usually better to apply specialized integrators
              | that work directly on the second-order form of the equation, such as the Newmark scheme [139]
              | or generalized α-method [45], as these schemes are constructed with tunable damping to promote
              | stability—a particularly important consideration in these problems.
blank         | 
text          |    At this point, the governing equation and its parameters have been discretized. The final dis-
              | cretization task is to treat the quantity of interest. To ensure the truncation error of the governing
              | equation and quantity of interest exactly match, a solver-consistent discretization [211] is employed
              | and detailed in the next section.
blank         | 
              | 
title         | 2.1.4    Discretization: Quantities of Interest
text          | Quantities of interest are among the most important aspects of a computational physics simulation,
              | particularly in engineering applications. Optimization problems, the main focus of this work, are
              | completely driven by quantities of interest as these comprise the objective and constraint functions.
              | Therefore, care must be taken in the discretization of the integrals in (2.2) since this will introduce
              | an additional error, i.e., on top of the error in the discretization of the PDE itself. To ensure
              | the quantity of interest discretization does not dominate, thereby lowering the global order of the
              | scheme, it is necessary that its discretization order matches that of the the governing equations.
              | Clearly, it is wasteful to discretize this to a higher order than the state equation, using a similar
              | argument.
              |    For these reasons, discretization of (2.2) will be done in a solver-consistent manner, i.e., the
              | spatial and temporal discretization used for the governing
              |                                                       Z     equation Zwill also be used for the quan-
              | tities of interest. Define fh as the approximation of   fB (U ) dV +      f∂B (U ) dA using the shape
              |                                                              B                  ∂B
              | functions underlying the spatial discretization of the governing equations. This ensures the spatial
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                            33
blank         | 
              | 
              | 
text          | integration error in the quantity of interest exactly matches that of the governing equations. Next,
              | define                                                            Z   t
              |                                          Fh (u, µ, t) :=                  fh (u, µ, τ ) dτ,                     (2.42)
              |                                                                     t0
blank         | 
text          | where the temporal domain is taken to be T = (t0 , tf ). Before the temporal discretization of the
              | governing equations can be applied to discretize the integral in (2.42), it must be converted to an
              | ODE. This is accomplished via differentiation of (2.42) with respect to t to yield
blank         | 
text          |                                                Ḟh (u, µ, t) = fh (u, µ, t).                                    (2.43)
blank         | 
text          | Augmenting the semi-discrete governing equations with this ODE results in the enlarged system of
              | ODEs                                     "          #" #                  "                 #
              |                                           M        0  u̇                      r(u, µ, t)
              |                                                                    =                            .               (2.44)
              |                                             0      1       Ḟh              fh (u, µ, t)
blank         | 
text          | At this point, the same temporal discretization used for the governing equations in the previous
              | section can be applied to discretize (2.44). A monolithic discretization of this form ensures the
              | temporal truncation error of the governing equations and quantity of interest will exactly match.
              | The development will proceed assuming a DIRK scheme is used—the same procedure would apply
              | if BDF or another first-order temporal discretization was applied. Application of the DIRK scheme
              | yields the fully discrete governing equations and corresponding solver-consistent discretization of the
              | quantity of interest (2.2)
blank         | 
text          |                                                            s
              |                                                                       (n)
              |                                                            X
              |                                 u(n) = u(n−1) +                  bi ki
              |                                                            i=1
              |                                                              s
              |                                                                                                                 (2.45)
blank         |                                                                                             
text          |                                   (n)       (n−1)                       (n)
              |                                                            X
              |                                 Fh      = Fh           +         bi fh ui , µ, tn−1 + ci ∆tn
              |                                                            i=1
blank         |                                                                             
text          |                                (n)                     (n)
              |                             M ki        = ∆tn r       ui ,   µ, tn−1 + ci ∆tn .
blank         | 
text          |                                                   (n)
              | for n = 1, . . . , Nt , i = 1, . . . , s, and ui         is defined in (2.41). Finally, the functional in (2.42) is
              | evaluated at time t = tf to yield the solver-consistent approximation of Fh (u, µ, tf )
blank         | 
text          |                                                    (1)                              (Nt )
              |                        F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) ) := Fh              ≈ Fh (u, µ, tf ).   (2.46)
blank         | 
text          |     While the spatially solver-consistent discretization of QoIs is widely used, particularly in the con-
              | text of finite element methods, temporal discretization is commonly done via low-order quadrature
              | rules, usually the trapezoidal rule [193, 130, 124, 205, 102]. The main advantage of this solver-
              | consistent discretization is the asymptotic discretization order of the governing equation and quan-
              | tity of interest are guaranteed to exactly match, which ensures there is no wasted error in “over-
              | integrating” one of the terms. The solver-consistent discretization also has the advantage of a natural
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              34
blank         | 
              | 
              | 
text          | and convenient implementation given the spatial and temporal discretization implementation. Fi-
              | nally, this method has the additional convenience
              |                                            Z tn     of keeping a high-order accurate “current” value
              |                                        (n)
              | of the integral, i.e. at time step n, Fh ≈      fh (τ ) dτ to high-order accuracy. This property does
              |                                             t0
              |                                                        Z tn
              | not hold for high-order numerical quadrature since          fh (τ ) dτ will involve u(n+j) , where j ≥ 1
              |                                                         t0
              | depends on the quadrature rule used.
              |    This completes the discussion of parametrized deterministic partial differential equations. Before
              | proceeding to the main topic of this work, PDE-constrained optimization, the notion of parametric
              | stochastic partial differential equations is introduced and relevant details discussed, such as mean-
              | ingful risk measures and methods to discretize the stochastic space. This will lead to the discussion
              | of PDE-constrained optimization that will be applicable in both the deterministic and stochastic
              | setting.
blank         | 
              | 
title         | 2.2        Parametrized Stochastic Partial Differential Equations
text          | This section generalizes the concepts discussed in Section 2.1 to the case where uncertainty is present
              | in the parametrized partial differential equation—for simplicity only static problems are considered.
              | The ultimate goal is to setup the stochastic PDE-constrained optimization problem. The discussion
              | begins with the formulation of parametrized, Stochastic Partial Differential Equations (SPDEs) and
              | introduces the concept of risk measures of PDE quantities of interest. These risk measures will
              | comprise the objective and constraint functions in stochastic (risk-averse) optimization problems.
              | The SPDE will be discretized in space using the techniques introduced in Section 2.1 and collocation
              | will be used to discretize the stochastic space. Finally, the deterministic and stochastic PDE-
              | constrained optimization problems will be collectively detailed in Section 2.3.
              |    Let B be a bounded domain in Rnsd and let (Ω, F, P ) be a complete probability space. Here
              | Ω is the set of outcomes, F ⊂ 2Ω is the σ-algebra of events, and P : F → [0, 1] is a probability
              | measure. Consider the stochastic boundary value problem: find U such that P -almost everywhere
              | in Ω
              |                            G(U, ∇U, µ, ω) = g(x, µ, ω)        x ∈ B(µ, ω)
              |                                                                                                  (2.47)
              |                            H(U, ∇U, µ, ω) = h(x, µ, ω)        x ∈ ∂B(µ, ω),
blank         | 
text          | where B ⊂ Rnsd is the spatial domain with boundary ∂B, U is the unknown solution, µ ∈ RNµ
              | are deterministic parameters, G and H are first-order spatial differential operators, and g and h
              | are volumetric and boundary source terms. For generality, the domain, boundary, source terms,
              | and differential operators are all taken as stochastic. The presence of the parameter vector µ
              | indicates that the differential operators, source terms, and boundary conditions have already been
              | parametrized using the techniques in Section 2.1.2. Each realization of the parametrized, stochastic
              | PDE in (2.47), i.e., for a given ω ∈ Ω, constitutes a deterministic PDE of the form (2.1), which
              | can be discretized according to the methods outlined in that section. The following finite-noise
              | assumption [13, 12] allows the source of randomness to be approximated using a finite number of
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                 35
blank         | 
              | 
              | 
text          | independent random variables.
blank         | 
text          | Assumption 2.1 (Finite-dimensional noise). The stochastic terms in (2.47) depend on a finite
              | number of real-valued random variables, i.e.,
blank         | 
text          |                               G( · , · , · , ω) = G( · , · , · , Y1 (ω), . . . , YNy (ω))
              |                                  g( · , · , ω) = g( · , · , , Y1 (ω), . . . , YNy (ω))               (2.48)
              |                                     B( · , ω) = B( · , Y1 (ω), . . . , YNy (ω)),
blank         | 
text          |                         y N
              | where Ny ∈ N+ and {Yn }n=1 are real-valued, independent random variables. A similar expansion is
              | assumed to hold for the boundary terms.
blank         | 
text          |    Define Ξn := Yn (Ω) as the image of the random variables in Assumption 2.1 and Ξ = Ξ1 ⊗ · · · ⊗
              | ΞNy ⊂ RNy . Let ρn : Ξn → R+ denote the probability density of the random variable Yn and,
              |                                  y    N
              | due to the independence of {Yn }n=1 , the joint density of the random vector Y = (Y1 , . . . , YN ) is
              | ρ : Ξ → R+ where ρ = ρ1 ⊗ · · · ⊗ ρNy . The finite noise assumption allows a change of variables that
              | converts the parametrized stochastic partial differential equation in (2.47) to: find U (µ, y) such
              | that for all y ∈ Ξ
              |                            G(U, ∇U, µ, y) = g(x, µ, y)                  x ∈ B(µ, y)
              |                                                                                                      (2.49)
              |                           H(U, ∇U, µ, y) = h(x, µ, y)                   x ∈ ∂B(µ, y)
blank         | 
text          | for µ ∈ RNµ .
blank         | 
              | 
title         | 2.2.1    Risk Measures of Quantities of Interest
text          | The uncertainty that has been incorporated in the partial differential equation in (2.47) will be
              | propagated to the quantities of interest through the solution U (µ, y) and possibly the domain
              | B(µ, y), boundary ∂B(µ, y), and differential operators. To formulate a well-defined and meaningful
              | optimization problem, we consider an objective and constraints that consist of risk measures of these
              | uncertain quantities of interest. For the remainder of this section, let X be a real-valued random
              | variable, defined as X(y; µ) = F(U (µ, y), µ, y) where F is the quantity of interest in (2.2) without
              | temporal dependence, generalized to the stochastic case, i.e.,
              |                               Z                         Z
              |                F(U, µ, y) =           fB (U, µ, y) dV +                          f∂B (U, µ, y) dA.   (2.50)
              |                                     B(µ, y)                           ∂B(µ, y)
blank         | 
              | 
text          | The dependence of the random variable X on the parameter will be dropped for the remainder of
              | this section as treatment of the stochastic dimension is the focus.
              |    The simplest risk measure is the expected value of the random variable
              |                                             Z
              |                                      E[X] =    ρ(y)X(y) dy.                                          (2.51)
              |                                                        Ξ
blank         | 
text          | The mean of the random variable does not necessarily encode a useful measure of risk, but is a
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              36
blank         | 
              | 
              | 
text          | straightforward generalization of the deterministic quantity of interest to the stochastic case. An
              | obvious deficiency in its use as a risk measure is it does not incorporate the spread of random variable
              | about the mean. The mean plus semideviation, defined as
blank         | 
text          |                                      Rβ [X] = E[X] + βE[(X − E[X])+ ]                            (2.52)
blank         | 
text          | for β ∈ R+ , where (x)+ = max{0, x} overcomes this limitation. In an optimization setting, the value
              | of β must be determined to balance minimization of the (expected) quantity of interest with risk
              | aversion, which may be difficult to do in practice. Another relevant risk measure is the β-quantile
              | of the random variable, also called the value-at-risk, defined as the smallest value such that the
              | probability that the random variable lies below said value is at least β, i.e.,
blank         | 
text          |                                VaRβ [X] = inf{t ∈ R | Pr[X ≤ t] ≥ β},                            (2.53)
blank         | 
text          | where                                              Z
              |                                      Pr[X ≤ t] =                 ρ(y) dy.                        (2.54)
              |                                                     y∈Ξ:X(y)≤t
blank         | 
text          | The main disadvantage of the value-at-risk is that it fail to emphasize rare and low probability
              | events, which tend to be particularly important in engineering settings since they often correspond
              | to failure. The conditional value-at-risk, defined as
blank         | 
text          |                                         CVaRβ [X] = inf Fβ (t, X),                               (2.55)
              |                                                         t∈R
blank         | 
              | 
text          | where
              |                                                         1
              |                                      Fβ (t, X) = t +       E [(X − t)+ ]                         (2.56)
              |                                                        1−β
              | circumvents this limitation. While the conditional value-at-risk is non-smooth (due to the presence of
              | the max operator) and non-trivial to evaluate, it emphasizes rare events for β  0. In the remainder
              | of this thesis, only the expectation risk measure will be considered for simplicity. All developments
              | will extend to any smooth risk measure. For non-smooth risk measures such as the semideviation
              | and conditional value-at-risk, well-defined smoothed approximations can be used [110] in place of
              | the risk measure itself.
blank         | 
              | 
text          | 2.2.2    Examples
              | 1D Steady, Viscous Burgers’ Equation with Uncertain Coefficients
blank         | 
text          | The only stochastic partial differential equation considered in this thesis is the 1D steady, viscous
              | Burgers’ equation with uncertain boundary conditions, source term, and viscosity
blank         | 
text          |                −ν(y)∂xx u(x, y) + u(x, y)∂x u(x, y) = g(x, y)           x ∈ (xl , xr ), y ∈ Ξ
              |                                                                                                  (2.57)
              |                u(xl , y) = d0 (y),     u(xr , y) = d1 (y)               y ∈ Ξ.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                37
blank         | 
              | 
              | 
text          | The risk-neutral measure (expectation) of the tracking-type functional
              |                                       Z xr                                     
              |                                       1
              |                                              (u(x, · ) − ū(x))2 + αg(x, · )2 dx .
blank         |                                                                             
text          |                          T (u) = E
              |                                       2 xl
blank         | 
text          | will be used to define an optimal control problem in Chapter 6.
blank         | 
title         | Static Linear Elasticity with Uncertain Loading
blank         | 
text          | Another stochastic partial differential equation that will be considered in future work is linear elastic-
              | ity with stochasticity in the load conditions. Consider the same physical setup as the deterministic
              | linear elasticity setup—a solid body B ⊂ Rnsd subject to uncertain distributed body forces with
              | boundary ∂B decomposed into two parts: ∂Bu and ∂Bt such that ∂B = ∂Bu ∪ ∂Bt . Displacements
              | are prescribed along ∂Bu and ∂Bt is subject to uncertain, prescribed traction forces. Under the
              | assumption that the resulting deformations are infinitesimal and the pointwise stress and strain
              | are related through a linear relationship, the deformation of the body is governed by the following
              | system of partial differential equations
blank         | 
text          |                              ∇ · σ(x, y) + b(x, y) = 0            x ∈ B,    y∈Ξ
              |                                           u(x, y) = ū(x)         x ∈ ∂Bu                          (2.58)
              |                                        σ(x, y) · n = t̄(x, y)     x ∈ ∂Bt , y ∈ Ξ,
blank         | 
text          | where u is the pointwise deformation and state vector of the PDE, σ is the stress tensor, b is the
              | uncertain body force, ū is the prescribed displacement on ∂Bu , t̄ is the uncertain, prescribed traction
              | on ∂Bt , and n is the pointwise outward normal to the boundary. The system of PDEs is closed with
              | the stress-strain relationship (Hooke’s law)
blank         | 
text          |                                                   σ=C:                                            (2.59)
blank         | 
text          | and the kinematic constraint relates deformation to strain as
blank         | 
text          |                                                  1
              |                                                    ∇u + ∇uT .
blank         |                                                            
text          |                                             =                                                     (2.60)
              |                                                  2
blank         | 
text          | The quantities of interest considered are the volume of the structure—a deterministic quantity since
              | it is a geometrical quantity and all uncertainty is in the loading—and the expectation of the tracking
              | functional
              |                                                       Z                                      
              |                                                       1
              |                    Z
              |              V =        dV       and     T (u) = E        (u(x, · ) − ū)k (u(x, · ) − ū)k dV .   (2.61)
              |                     B                                 2 B
blank         | 
title         | 2.2.3     Finite-Dimensional Approximation
text          | Since analytical techniques cannot, in general, be used to solve parametrized stochastic differential
              | equations in (2.47), discretization techniques are applied to reduce the continuous (differential)
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              38
blank         | 
              | 
              | 
text          | form of the problem to a discrete (algebraic) form. Unlike the continuous formulation, the discrete
              | problem can be solved using computational methods and resources. Two types of discretization
              | must be applied to the SPDE in (2.47) to the yield a (sequence of) algebraic equations that are
              | amenable numerical computation—-spatio-temporal and stochastic discretization.
              |    Each realization of the SPDE in (2.47), i.e., for a given y ∈ Ξ, constitutes a deterministic
              | parametrized partial differential equation and requires spatio-temporal discretization (only spatial
              | discretization for static problems), e.g., such as those in Section 2.1.3, to yield a discrete problem.
              | The specific spatial discretization technique is left unspecified since the appropriate choice depends
              | on the properties of the differential operators G( · , · , µ, y) and H( · , · , µ, y). The semi-discrete
              | form of the SPDE in (2.49) is: find u such that
blank         | 
text          |                                       r(u, µ, y) = 0       ∀y ∈ Ξ.                               (2.62)
blank         | 
text          | A variant of the Implicit Function Theorem (Theorem 2.1) implies the existence of a continuous
              | function u(µ, y), defined implicitly as the solution of r( ·, µ, y) = 0. The corresponding semi-
              | discrete stochastic quantity of interest and its risk measure take the form
blank         | 
text          |                              f (u, µ, y)     and       R[f (u(µ, · ), µ, · )],                   (2.63)
blank         | 
text          | where R is any risk measure introduced in the previous section.
              |    Despite the spatial discretization, the semi-discrete form of the SPDE in (2.62) can still not be
              | treated computationally as the set Ξ contains infinitely many points. There are a few approaches,
              | including stochastic Galerkin methods and stochastic collocation, to discretize the stochastic dimen-
              | sion and yield a fully discrete form of the SPDE that can be solved in a computational setting. This
              | work uses stochastic collocation whereby the equation is (2.62) is enforced only on a finite subset of
              | Ξ, that is, (2.62) is replaced with
blank         | 
text          |                                       r(u, µ, y) = 0       ∀y ∈ Ξh                               (2.64)
blank         | 
text          | where Ξh ⊂ Ξ and card(Ξh ) < ∞. The integrals involved in the computation of the risk measure
              | must then be approximated with a quadrature scheme with nodes Ξh . Section 6.1.3 details an
              | efficient method to construct Ξh using anisotropic sparse grids [67].
blank         | 
              | 
title         | 2.3     PDE-Constrained Optimization
text          | Given the exposition on parametrized partial differential equations in the previous section, atten-
              | tion is turned to the main interest of this document: optimization problems governed by partial
              | differential equations. There are three primary components required to define a PDE-constrained
              | optimization problem:
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                               39
blank         | 
              | 
              | 
text          |    • the governing partial differential equation and corresponding state vector that define the phys-
              |       ical problem of interest,
blank         | 
text          |    • an objective function and constraint functions—the goal of the optimization problem—these
              |       are usually quantities of interest of the partial differential equation that define a performance
              |       measure to be optimized and design requirements, and
blank         | 
text          |    • optimization parameters—usually a control or design—that are used to meet the performance
              |       requirements.
blank         | 
text          | Each of these components were discussed in the previous section, including details pertaining to their
              | formulation and discretization, and concrete examples were provided. The remainder of this section
              | will consider an abstract vector of parameters, µ ∈ RNµ , i.e., the parameter space has already been
              | discretized and the discrete parameters can control any aspect of the PDE (shape/topology of the
              | domain, boundary conditions, coefficient in differential operators). To encompass the wide array of
              | features in the parametrized partial differential equations discussed previously, an abstract PDE of
              | the form
              |                                              D(U, µ) = 0                                          (2.65)
blank         | 
text          | will be considered, where U is the state vector and D is the differential operator. The abstract
              | quantity of interest will be denoted
              |                                                  F(U, µ)                                          (2.66)
blank         | 
text          | and will be the objective function or cost functional in the remainder of this section. This notation
              | will encompass static and time-dependent, deterministic and stochastic PDEs from previous sections.
              | In the static, deterministic case, U is understood to be only a function of space, i.e., U = U (x),
              | and F is likely an integral over a volume or surface. In the time-dependent, deterministic case, U
              | is a function of space and time, i.e., U = U (x, t), and F is a space-time integral. In the stochastic
              | counterparts, U also depends on the realization, i.e., U ( · ) = U ( · , y), and F requires an integral
              | over the stochastic space to compute the risk measure of the quantity of interest. The discretized
              | PDE and QoI will be denoted
blank         | 
text          |                                    r(u, µ) = 0      and      f (u, µ),                            (2.67)
blank         | 
text          | respectively, where u is the discrete state vector. While this abstract framework would lead to a hor-
              | ribly inefficient implementation, it is useful to consider these various cases at once as the same issues
              | and concepts regarding PDE-constrained optimization arise in all cases. Each case must be consid-
              | ered separately to obtain a formulation that will be efficient from an implementation viewpoint. The
              | remainder of this chapter discusses important details in the formulation of PDE-constrained opti-
              | mization problems and introduces concepts and notation that will be used throughout the remainder
              | of this thesis.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             40
blank         | 
              | 
              | 
title         | 2.3.1    Continuous vs. Discrete Formulation
text          | The many steps involved in the discretization of partial differential equations provides a large degree
              | of flexibility in the formulation of the PDE-constrained optimization problem. Namely, it can be
              | formulated at the continuous level or at any stage in the discretization process and these will not,
              | in general, be equivalent for finite values of the discretization parameter since the operations of
              | differentiation and discretization do not commute.
              |    The PDE-constrained optimization problem at the continuous level takes the form
blank         | 
text          |                                        minimize       F(U, µ)
              |                                                U, µ
              |                                                                                                 (2.68)
              |                                        subject to D(U, µ) = 0.
blank         | 
text          | The continuous formulation of the optimization problem, also known as the differentiate-then-
              | discretize approach [78], proceeds by deriving the optimality conditions of (2.68), which leads to
              | a system of partial differential equations that includes the primal and adjoint PDE and optimality
              | condition. These PDEs are discretized using the methods introduced in the previous section and
              | solved using an iterative method. Since this process is heavily dependent on the specific form of the
              | PDE and QoI under consideration, a specific example is provided next.
blank         | 
text          | Example 1 (Optimal control of Poisson’s equation). Consider the optimal control problem that
              | looks to find a distributed control z(x) such that u(x), the solution of the Poisson equation with
              | homogeneous Dirichlet boundary conditions, matches a given target state ū(x) with a penalty on the
              | magnitude of the control. This problem is stated precisely as
blank         | 
text          |                                        1                          α
              |                                            Z                          Z
              |                                                           2
              |                          minimize             (u(x) − ū(x)) dx +          z(x)2 dx
              |                           u(x), z(x)   2    Ω                     2    Ω
blank         | 
text          |                          subject to    − ∆u(x) = z(x)         x∈Ω                               (2.69)
blank         | 
text          |                                            u(x) = 0       x ∈ ∂Ω.
blank         | 
text          | The Lagrangian of this PDE-constrained optimization problem is
blank         | 
text          |                      1                  α
              |                        Z                  Z          Z                   Z
              |        L(u, z, λ) =      (u − ū)2 dx +     z 2 dx −    λ [−∆u − z] dx −     λu dx.             (2.70)
              |                      2 Ω                2 Ω           Ω                   ∂Ω
blank         | 
              | 
text          | Any solution of (2.69) must render the Lagrangian stationary, i.e.,
blank         | 
text          |                                   d
              |                                      L(u + δu, z, λ)|=0 = 0         ∀δu
              |                                   d
              |                                   d
              |                                      L(u, z + δz, λ)|=0 = 0         ∀δz                       (2.71)
              |                                   d
              |                                   d
              |                                      L(u, z, λ + δλ)|=0 = 0         ∀δλ.
              |                                   d
blank         | 
text          | After direct differentiation of the Lagrangian and subsequent integration-by-parts, the first condition
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                 41
blank         | 
              | 
              | 
text          | in (2.71) reduces to
blank         | 
text          |       d
              |                                   Z                                  Z
              |          L(u + δu, z, λ)|=0 =         [−∆λ − (u − ū)] δu dx −           λ [δu + ∇δu · n] dx = 0   (2.72)
              |       d                            Ω                                 ∂Ω
blank         | 
text          | Since this relation holds for all δu, it is equivalent to the following partial differential equation
blank         | 
text          |                                  −∆λ(x) = u(x) − ū(x)               x∈Ω
              |                                                                                                      (2.73)
              |                                       λ(x) = 0                       x ∈ ∂Ω,
blank         | 
text          | which is known as the adjoint PDE. Direct differentiation of the Lagrangian (2.70) reduces the second
              | condition in (2.71) to
blank         | 
text          |                             d
              |                                                            Z
              |                                L(u, z + δz, λ)|=0 =           (αz + λ)δz dx = 0,                   (2.74)
              |                             d                              Ω
blank         | 
text          | which is equivalent to the pointwise relationship
blank         | 
text          |                                               λ(x) = −αz(x).                                         (2.75)
blank         | 
text          | This is known as the optimality condition. Finally, the last condition in (2.71) reduces to
blank         | 
text          |                    d
              |                                                    Z                         Z
              |                       L(u, z, λ + δλ)|=0 =           δλ [−∆u − z] dx −            δλu dx = 0       (2.76)
              |                    d                              Ω                           ∂Ω
blank         | 
text          | and recovers the governing PDE since this holds for all δλ
blank         | 
text          |                                       −∆u(x) = z(x)               x∈Ω
              |                                                                                                      (2.77)
              |                                            u(x) = 0               x ∈ ∂Ω.
blank         | 
text          | The adjoint PDE (2.73), optimality condition (2.75), and primal PDE (2.77) comprise the optimality
              | system at the continuous level, known as the Karush-Kuhn-Tucker (KKT) conditions. Thus, the
              | optimal control problem in (2.71) reduces to: find u(x), z(x), and λ(x) such that
blank         | 
text          |                                  −∆u(x) = z(x)                        x∈Ω
              |                                         u(x) = 0                      x ∈ ∂Ω
              |                                  −∆λ(x) = u(x) − ū(x)                x∈Ω                            (2.78)
              |                                         λ(x) = 0                      x ∈ ∂Ω
              |                                         z(x) = −λ(x)/α                x ∈ Ω.
blank         | 
text          | The derivation of the above KKT system at the continuous level is the first step in the differentiate-
              | then-discretize approach to PDE-constrained optimization. The KKT system is solved by discretizing
              | the parameters, quantities of interest, and primal and adjoint PDEs with the methods outlined in
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                            42
blank         | 
              | 
              | 
text          | Sections 2.1.2–2.1.4. This leads to a coupled system of equations that are solved to yield an approx-
              | imation to (2.69). In general, different discretization methods and levels of refinement can be used
              | for the primal and adjoint equations. This is one of the advantages of this approach compared to the
              | discretize-then-differentiate approach discussed next [78].
blank         | 
text          |    The discrete formulation, also known as the discretize-then-differentiate approach [78], first dis-
              | cretizes the optimization problem in (2.68) to yield
blank         | 
text          |                                        minimize     f (u, µ)
              |                                           u, µ
              |                                                                                                (2.79)
              |                                        subject to r(u, µ) = 0.
blank         | 
text          | Subsequently, the optimality conditions of (2.79) are derived by introducing its Lagrangian
blank         | 
text          |                                  L(u, µ, λ) = f (u, µ) − λT r(u, µ)                            (2.80)
blank         | 
text          | and requiring its stationarity, i.e., (u, µ, λ) such that
blank         | 
text          |                     ∂L                     ∂L                    ∂L
              |                        (u, µ, λ) = 0          (u, µ, λ) = 0         (u, µ, λ) = 0.             (2.81)
              |                     ∂u                     ∂µ                    ∂λ
blank         | 
text          | These are the Karush-Kuhn-Tucker (KKT) conditions [143] and lead to the coupled system of
              | nonlinear algebraic equations
              |                                        ∂r               ∂f
              |                                           (u, µ)T λ =      (u, µ)T
              |                                        ∂u               ∂u
              |                                        ∂r               ∂f                                     (2.82)
              |                                           (u, µ)T λ =      (u, µ)T
              |                                        ∂µ               ∂µ
              |                                            r(u, µ) = 0,
blank         | 
text          | which are solved simultaneously.
              |    The continuous formulation has a significant disadvantage in that it does not possess discrete
              | consistency in the reduced space setting (Section 2.3.2), that is, the computed gradient is not the
              | true gradient of the computed QoI since differentiation and discretization do not commute. This
              | makes it difficult to use blackbox optimizers to solve the optimization problem as convergence may
              | fail or be slowed when supplied with inconsistent gradients. Despite this disadvantage, there a
              | number of advantages to the continuous formulation. Different discretizations can be used for the
              | primal, sensitivity, and adjoint equations (either predefined or adaptively refined grids) depending
              | on required resolution of each. In shape optimization problems, there is no need to account for the
              | grid motion in the sensitivity and adjoint equations since they are posed directly on the new domain.
              | Finally, this approach can naturally be embedded in an optimization framework that leverages and
              | manages inexact gradients since error bounds on the computed sensitivities and adjoints are available
              | [108, 109].
              |    The discrete formulation has a number of advantages as compared to the continuous framework,
              | most notable discrete consistency of computed functionals and gradients. Additionally, the discrete
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                43
blank         | 
              | 
              | 
text          | formulation allows for the use of sophisticated differentiation software, such as automatic [161] and
              | symbolic [126] differentiation, to compute the various quantities that arise in the sensitivity and
              | adjoint equations. The discrete setting also allows for a large degree of flexibility in the quantities of
              | interest and optimization parameters considered, particularly if a well-defined differentiation frame-
              | work is used in the implementation. The continuous approach requires re-deriving the corresponding
              | adjoint equations for each partial differential equation, boundary condition, and quantity of interest.
              | For these reasons, the discrete formulation will be solely considered throughout the remainder of
              | this thesis.
              |    In certain situations, the continuous and discrete formulations of the optimization problem are
              | equivalent. This equivalence holds if the scheme used to discretize the partial differential equation is
              | adjoint consistent—that is, the discrete adjoint equations correspond to a consistent discretization
              | of the continuous adjoint equations [11, 84]. This property is not crucial for this work since the
              | discrete formulation is considered and therefore gradients automatically possess discrete consistency.
              | However, it has been shown that an adjoint consistent discretization of the PDE is necessary for
              | optimal convergence rates in L2 and in quantities of interest [97, 83, 82].
blank         | 
              | 
title         | 2.3.2     Full Space vs. Reduced Space Approach
text          | To this point, the PDE-constrained optimization problem has been posed as an optimization problem
              | over the state and parameter. This is usually called a full space or one-shot formulation as the
              | solution of the PDE and optimization problem are sought simultaneously. In contrast, the reduced
              | space approach explicitly enforces the PDE constraint and considers an optimization problem over
              | the parameters only. In the optimization community, this is commonly referred to as nonlinear
              | elimination of equality constraints [71, 143].
              |    To consider the reduced space approach to PDE-constrained optimization, the following assump-
              | tion on existence and uniqueness of solutions of the PDE is crucial.
blank         | 
text          | Assumption 2.2. For any µ ∈ RNµ , there exists a unique u(µ) such that r(u(µ), µ) = 0.
blank         | 
text          |    From this assumption, it is clear that optimization of pure Neumann problems is not possible in
              | the reduced space setting as the solutions of these problems are only unique up to a constant. For
              | such problems, a full space setting is more appropriate. The implicit function theorem guarantees
              | Assumption 2.2 holds if r is sufficiently regular and its Jacobian is invertible; in fact, it guarantees
              | the existence of a smooth function that maps µ ∈ RNµ to the corresponding solution of the PDE,
              | u(µ).
blank         | 
text          | Theorem 2.1 (Implicit Function Theorem). Let A be an open set in RNu × RNµ and suppose
              | r : A → RNu is a C r function (r ≥ 1). Consider ū ∈ RNu and µ̄ ∈ RNµ such that r(ū, µ̄) = 0 and
              | ∂r
              |     (ū, µ̄) is invertible. Then, there exists a neighborhood B ⊂ RNµ of µ̄ and a unique C r function
              | ∂u
              | u : RNµ → RNu such that ū = u(µ̄) and r(u(µ), µ) = 0 for all µ ∈ B.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             44
blank         | 
              | 
              | 
text          |    This mapping from µ to u(µ) is used to define a quantity of interest that only depends on µ
blank         | 
text          |                                         F (µ) := f (u(µ), µ).                                   (2.83)
blank         | 
text          | Thus, the constrained optimization problem in (2.79) can be reduced to the unconstrained optimiza-
              | tion problem
              |                                            minimize F (µ)                                       (2.84)
              |                                             µ∈RNµ
blank         | 
text          | since the solution of the PDE is fully accounted for in u(µ). In a gradient-based optimization
              | setting, the reduced space approach requires the computation of gradients of quantities of interest
              | that account for the total dependence on µ, that is, the explicit dependence on µ and the implicit
              | dependence through the solution of the PDE itself. This will be the focus of the next two sections
              | that consider two distinct approaches to obtaining such gradients.
              |    There are a number of advantages of the reduced space approach over full space methods, par-
              | ticularly in the context of large-scale, practical problems. First, the optimization problem is smaller
              | and simpler—it is only posed over the parameters since the state variable is taken as an implicit
              | function of these parameters, u(µ), and the nonlinearly constrained optimization problem is reduced
              | to an unconstrained one. The reduced space framework also allows for the use of state-of-the-art
              | PDE solvers and black-box optimizers since it decouples the solution of the PDE and the optimiza-
              | tion problem. This is particularly important in the context of computational fluid dynamics where
              | specialized methods exist for solving the steady-state partial differential equation such as pseudo-
              | transient continuation [104, 105]. For these reasons, the remainder of this thesis will focus solely
              | on the reduced space formulation of PDE-constrained optimization. The close this discussion, it is
              | worthwhile to mention some advantages of the full space approach: (1) it does not require Assump-
              | tion 2.2, thereby enlarging the class of problems to which it can be applied and (2) it is usually
              | more efficient than the reduced space approach since it does not require full resolution of the PDE
              | solution at every iteration.
blank         | 
              | 
title         | 2.3.3    Sensitivity Method for Computing Gradients
text          | Once one commits to using a reduced space approach, the gradient of F (µ) in (2.84) must be
              | computed, if a gradient-based optimization method is to be employed. This gradient must account
              | for the explicit dependence of f on µ as well as its implicit dependence through the solution of
              | the PDE. Throughout the remainder of this chapter, u(µ) will be used to denote the function in
              | Theorem 2.1 that maps µ to the solution of the PDE. Application of the chain rule leads to the
              | expansion
              |                            dF       ∂f             ∂f           ∂u
              |                               (µ) =    (u(µ), µ) +    (u(µ), µ)    (µ).                         (2.85)
              |                            dµ       ∂µ             ∂u           ∂µ
              | Furthermore, since u(µ) is the solution of the PDE for any µ, r(u(µ), µ) = 0 and
blank         | 
text          |                 dr                           ∂r             ∂r           ∂u
              |                    (u(µ), µ) = 0     =⇒         (u(µ), µ) +    (u(µ), µ)    (µ) = 0             (2.86)
              |                 dµ                           ∂µ             ∂u           ∂µ
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                45
blank         | 
              | 
              | 
text          |                                                                    ∂r
              | From the assumptions in Theorem 2.1, the Jacobian matrix              (u(µ), µ) is invertible, which leads
              |                                                                    ∂u
              | to the following expression for the sensitivity
blank         | 
text          |                               ∂u         ∂r             ∂r
              |                                  (µ) = −    (u(µ), µ)−1    (u(µ), µ).                               (2.87)
              |                               ∂µ         ∂u             ∂µ
blank         | 
text          | Combining this equation for the sensitivity with the expression for the gradient of F leads to
blank         | 
text          |                 dF       ∂f             ∂f           ∂r             ∂r
              |                    (µ) =    (u(µ), µ) −    (u(µ), µ)    (u(µ), µ)−1    (u(µ), µ).                   (2.88)
              |                 dµ       ∂µ             ∂u           ∂u             ∂µ
blank         | 
text          | This method of computing the gradient of F is known as the sensitivity or direct method. An
              |                                                                      ∂u
              | important observation is that each column of the sensitivity matrix,    , requires the solution of
              |                                                                      ∂µ
              | a linear system of equation with the Jacobian matrix—a total of Nµ linear systems with the same
              | matrix and different right-hand sides. In large-scale applications, particularly for time-dependent
              | problems, this will be a very expensive endeavor—see Appendix D for details regarding the sensitivity
              | method for time-dependent problems. An advantage of the sensitivity method is that once the
              |              ∂u
              | sensitivity,     is computed, the gradient of any number of functionals can be computed essentially
              |              ∂µ
              | for free. This is useful if the problem has a large number of side constraints—see Section 2.3.5.
              |    Before closing this discussion on sensitivity analysis, define the sensitivity residual as
blank         | 
text          |                                                      ∂r          ∂r
              |                                r ∂ (u, v, µ) :=         (u, µ) +    (u, µ)v,                        (2.89)
              |                                                      ∂µ          ∂u
blank         | 
text          | which is motivated from the sensitivity equations in (2.86). Clearly, we have
blank         |                                                                
text          |                                          ∂             ∂u
              |                                      r           u(µ),    (µ), µ = 0.
              |                                                        ∂µ
blank         | 
text          | The sensitivity residual will be used as an error indicator for any approximation u, w of the true
              |                                        ∂u
              | primal solution u(µ) and sensitivity      (µ), as well as an error bound on the corresponding ap-
              |                                        ∂µ
              | proximation of ∇F (µ)—see Appendix B. In a similar manner, the gradient computation in (2.88)
              | is generalized to consider non-equilibrium solutions u and sensitivities w
blank         | 
text          |                                                      ∂f          ∂f
              |                                g ∂ (u, w, µ) =          (u, µ) +    (u, µ)w                         (2.90)
              |                                                      ∂µ          ∂u
blank         | 
text          | as this will play a role in the residual-based error bounds on QoIs (Appendix B). The next section
              | introduces a method to compute ∇F (µ)—the adjoint method—that circumvents the large cost of
              | the sensitivity approach when Nµ  1.
blank         | 
              | 
title         | 2.3.4    Adjoint Method for Computing Gradients
text          | The adjoint method is an alternative approach to compute the gradient of F that circumvents the
              | sensitivity computation in (2.87) and only requires a single linear system solve with the transpose
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              46
blank         | 
              | 
              | 
text          | of the Jacobian matrix to compute the entire gradient. In this section, three different derivations of
              | the adjoint method will be provided, leading to various interpretations of the adjoint variable.
              |      The first and simplest derivation of the adjoint method applies a simple algebraic trick to the
              | gradient expression in (2.88) to yield
              |                                                                   !T
              |                 dF   ∂f   ∂f ∂r −1 ∂r   ∂f           ∂r −T ∂f T        ∂r   ∂f      ∂r
              |                    =    −             =    −                              =    − λT              (2.91)
              |                 dµ   ∂µ ∂u ∂u ∂µ        ∂µ           ∂u ∂u             ∂µ   ∂µ      ∂µ
blank         | 
text          | where the arguments u(µ) and µ have been dropped for brevity and λ(µ) is defined as the solution
              | of
              |                                  ∂r                   ∂f
              |                                     (u(µ), µ)T λ(µ) =    (u(µ), µ)T .                            (2.92)
              |                                  ∂u                   ∂u
              | The linearized equations in (2.92) are known as the adjoint equations and λ is the adjoint or dual
              | variable. From (2.91) and (2.92) it is clear the gradient of F can be computed from one linear system
              | solve, regardless of Nµ .
              |      The second derivation proceeds by introducing λ as an arbitrary test function, multiplying it
              | by the sensitivity equations in (2.86), and adding the resulting expression to the equation for the
              | gradient of F in (2.85)                                        
              |                               dF   ∂f   ∂f ∂u        ∂r   ∂r ∂u
              |                                  =    +       − λT      +         .                              (2.93)
              |                               dµ   ∂µ ∂u ∂µ          ∂µ ∂u ∂µ
              | This equation is valid since the term in brackets on the right side is identically zero from (2.86) and
              | the fact that all terms are evaluated at the primal and sensitivity solutions. Recall the goal is to get
              |                     dF                                        ∂u
              | an expression for      that is independent of the sensitivity    . To this end, the terms in (2.93) are
              |                     dµ                                        ∂µ
              | re-arranged such that the sensitivity is isolated
blank         |                                                             
text          |                                dF   ∂f      ∂r     ∂f      ∂r ∂u
              |                                   =    − λT    +      − λT       .                               (2.94)
              |                                dµ   ∂µ      ∂µ     ∂u      ∂u ∂µ
blank         | 
text          | Define λ, which has remained arbitrary to this point, as the solution of the adjoint equation
blank         | 
text          |                                  ∂r                   ∂f
              |                                     (u(µ), µ)T λ(µ) =    (u(µ), µ)T                              (2.95)
              |                                  ∂u                   ∂u
blank         | 
text          |                                                                             dF
              | and the expression in the brackets vanishes, leading to an expression for      that is independent of
              |                                                                             dµ
              | the sensitivities
              |                              dF       ∂f                   ∂r
              |                                 (µ) =    (u(µ), µ) − λ(µ)T    (u(µ), µ)                          (2.96)
              |                              dµ       ∂µ                   ∂µ
              | and agrees with (2.92).
              |      The final derivation will introduce the adjoint variable as the Lagrange multipliers corresponding
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                            47
blank         | 
              | 
              | 
text          | to the PDE constraint of the auxiliary PDE-constrained optimization problem
blank         | 
text          |                                       minimize        f (u, µ̂)
              |                                            u
              |                                                                                                (2.97)
              |                                       subject to r(u, µ̂) = 0,
blank         | 
text          | where µ̂ is fixed, i.e., not an optimization variable. Assumption 2.2 implies that the optimization
              | problem is equivalent to the nonlinear system of equation
blank         | 
text          |                                                 r(u, µ̂) = 0.
blank         | 
text          | This follows directly from µ̂ being fixed and uniqueness of the solution of r(·, µ) = 0, i.e., the
              | feasible set of the optimization problem in (2.97) is {u(µ̂)} and therefore u(µ̂) must be the solution
              | of (2.97), regardless of the objective function. The Lagrangian of the optimization problem in (2.97)
              | is
              |                                   L(u, λ) = f (u, µ̂) − λT r(u, µ̂)                            (2.98)
blank         | 
text          | and the KKT system is
              |                                  ∂L   ∂f              ∂r
              |                                     =    (u, µ̂) − λT    (u, µ̂) = 0
              |                                  ∂u   ∂u              ∂u                                       (2.99)
              |                                  ∂L
              |                                     = −r(u, µ̂)                  =0
              |                                  ∂λ
              | The first condition is exactly the adjoint equations in (2.92) and the second condition is the PDE
              |                                                                         dF
              | constraint. Substitution into (2.94) yields the familiar expression for
              |                                                                         dµ
blank         | 
text          |                            dF       ∂f                   ∂r
              |                               (µ) =    (u(µ), µ) − λ(µ)T    (u(µ), µ).                        (2.100)
              |                            dµ       ∂µ                   ∂µ
blank         | 
text          | Thus, the adjoint variable has been introduced as an algebraic trick to re-arrange the operations
              | in (2.88), a test function multiplying the sensitivity equations, and the Lagrange multipliers of an
              | auxiliary PDE-constrained optimization problem. Appendix D details the derivation of the adjoint
              | equations—using the test function and Lagrange multiplier approach—for a time-dependent PDE
              | posed on a deforming domain and discretized with high-order spatial and temporal schemes. Similar
              | to the previous section, the adjoint equations in (2.92) are used to motivate the definition of the
              | adjoint residual
              |                                                 ∂f           ∂r
              |                              r λ (u, v, µ) :=      (u, µ)T −    (u, µ)T v,                    (2.101)
              |                                                 ∂u           ∂u
              | which will be used as an error measure when inexact primal and adjoint solution are used to compute
              | ∇F (µ)—see Appendix B. In a similar manner, the gradient computation in (2.91) is generalized to
              | consider non-equilibrium solutions u and adjoints z
blank         | 
text          |                                                  ∂f              ∂r
              |                               g λ (u, z, µ) =       (u, µ) − z T    (u, µ)                    (2.102)
              |                                                  ∂µ              ∂µ
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             48
blank         | 
              | 
              | 
text          | as this will play a role in the residual-based error bounds on QoI gradients (Appendix B).
blank         | 
              | 
title         | 2.3.5    Optimization Problems with Side Constraints
text          | To this point, unconstrained PDE-constrained optimization problems, i.e, optimization problems
              | where the PDE is the only constraint, have been solely considered due to simplicity in the exposition.
              | Nearly all practical problems, particularly in a design setting, will have additional performance
              | constraints, usually referred to as side constraints. In this case, the fully discrete optimization
              | problem in the full space takes the form
blank         | 
text          |                                        minimize      f (u, µ)
              |                                            u, µ
blank         | 
text          |                                        subject to r(u, µ) = 0
              |                                                                                                (2.103)
              |                                                      c(u, µ) = 0
              |                                                      d(u, µ) ≤ 0,
blank         | 
text          | where c and d are equality and inequality side constraints, respectively. In a gradient-based opti-
              | mization framework, the terms
              |                                            ∂c ∂c ∂d ∂d
              |                                              ,  ,  ,
              |                                            ∂u ∂µ ∂u ∂µ
              | are required in addition to
              |                                            ∂f ∂f ∂r ∂r
              |                                              ,  ,  ,   ,
              |                                            ∂u ∂µ ∂u ∂µ
              | the terms required in the case without side constraints. In the reduced space setting, the optimization
              | problem becomes
              |                                        minimize       F (µ)
              |                                              µ
blank         | 
text          |                                        subject to C(µ) = 0                                     (2.104)
blank         | 
text          |                                                       D(µ) ≤ 0,
blank         | 
text          | where F (µ) is defined in (2.83) and
blank         | 
text          |                        C(µ) := c(u(µ), µ)          and      D(µ) := d(u(µ), µ).                (2.105)
blank         | 
text          | This is a nonlinearly constrained optimization problem over µ and a gradient-based optimization
              | setting will require the Jacobians of the constraints
blank         | 
text          |                                                   dC dD
              |                                                     ,   ,
              |                                                   dµ dµ
blank         | 
text          | which can be computed using the sensitivity or adjoint approach discussed previously. In the case
              | where one of the constraints does not depend on the state vector u, the sensitivity/adjoint method
              | are not needed as the gradient will be equivalent to the partial derivative with respect to µ. If
              |                                                   ∂u
              | the sensitivity approach is used, the sensitivity    (µ) is computed once-and-for-all and used to
              |                                                   ∂µ
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              49
blank         | 
              | 
              | 
text          | reconstruct the required gradients as
blank         | 
text          |                             dF          ∂f             ∂f           ∂u
              |                                (µ) =       (u(µ), µ) +    (u(µ), µ)    (µ)
              |                             dµ          ∂µ             ∂u           ∂µ
              |                             dC          ∂c             ∂c           ∂u
              |                                (µ) =       (u(µ), µ) +    (u(µ), µ)    (µ)                      (2.106)
              |                             dµ          ∂µ             ∂u           ∂µ
              |                             dD          ∂d             ∂d           ∂u
              |                                (µ) =       (u(µ), µ) +    (u(µ), µ)    (µ)
              |                             dµ          ∂µ             ∂u           ∂µ
blank         | 
text          | Thus, even though the sensitivity computation requires a linear solve for each entry in µ, it is used to
              | construct the gradient of any number of functionals and is efficient when the number of constraints
              | is large compared to Nµ .
              |    Conversely, the adjoint equation is tied to a specific functional and each separate constraint
              | requires the solution of a different adjoint equation
blank         | 
text          |                                ∂r                       ∂f
              |                                   (u(µ), µ)T λf (µ) =      (u(µ), µ)T
              |                                ∂u                       ∂u
              |                                ∂r                       ∂c
              |                                   (u(µ), µ)T λc (µ) =      (u(µ), µ)T                           (2.107)
              |                                ∂u                       ∂u
              |                                ∂r                       ∂d
              |                                   (u(µ), µ)T λd (µ) =      (u(µ), µ)T .
              |                                ∂u                       ∂u
blank         | 
text          | Once the dual variable for each functional has been computed, the required derivatives are recon-
              | structed as
              |                             dF         ∂f                     ∂r
              |                                (µ) =      (u(µ), µ) − λf (µ)T    (u(µ), µ)
              |                             dµ         ∂µ                     ∂µ
              |                             dC         ∂c                     ∂r
              |                                (µ) =      (u(µ), µ) − λc (µ)T    (u(µ), µ)                      (2.108)
              |                             dµ         ∂µ                     ∂µ
              |                             dD         ∂d                     ∂r
              |                                (µ) =      (u(µ), µ) − λd (µ)T    (u(µ), µ).
              |                             dµ         ∂µ                     ∂µ
              |    Although it is not common, certain cases arise where it is possible to use nonlinear elimination
              | to remove side constraints, identical to elimination of the PDE constraint in the reduced space
              | approach. In these cases, for each µ, there must exist a u(µ) that satisfies the PDE and side
              | constraint. In general, this mapping will be different from the one defined in Theorem 2.1 and will
              | modify the sensitivity and adjoint equations. Appendix D provides a concrete example of a time-
              | dependent PDE-constrained optimization problem with two side constraints—the first is a lower
              | bound on a QoI (not amenable to elimination) and the second requires time-periodicity of the PDE
              | solution (amenable to elimination). Nonlinear elimination is applied to the periodicity constraint
              | and the adjoint equations are modified accordingly.
title         | Chapter 3
blank         | 
title         | Generalized Multifidelity Trust
              | Region Method
blank         | 
text          | Given the broad discussion on partial differential equations and PDE-constrained optimization in
              | Chapter 2, the scope will be narrowed to consider only the reduced-space framework for the remain-
              | der of the document. In this setting, nonlinear elimination is used to explicitly enforce the PDE
              | constraint and eliminate the state variables from the optimization problem. This leads to an uncon-
              | strained or constrained optimization problem, depending on the presence of side constraints, over
              | only the parameters, µ. Each query to the objective or constraint requires a primal PDE solve and
              | each query to the corresponding gradient requires (possibly many) sensitivity or adjoint PDE solves.
              | For PDEs with uncertain coefficients, an ensemble of primal and dual PDE solves are required to
              | evaluate these optimization functionals and gradients (in order to evaluate risk-averse measures of
              | quantities of interest). For large-scale problems that commonly arise in engineering and scientific
              | practice, this will be an expensive endeavor. To mitigate this computational burden, a globally
              | convergent optimization method is developed that enables the use of inexpensive, locally accurate
              | approximation models. This chapter will develop the multifidelity optimization method with an
              | abstract approximation model for the sake of generality, i.e., any approximation model that satisfies
              | the assumptions to be laid out. Chapters 5–6 will detail the use of projection-based reduced-order
              | models as the approximation model.
              |    This chapter begins with necessary background regarding unconstrained optimization theory.
              | Subsequently, an error-aware multifidelity trust region method—one of the auxiliary contributions
              | of this thesis—is developed. Finally, the special case of an unconstrained problem, i.e., no side
              | constraints, is generalized to handle nonlinear equality constraints.
blank         | 
              | 
              | 
              | 
meta          |                                                  50
              | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                               51
blank         | 
              | 
              | 
title         | 3.1         Unconstrained Optimization
text          | Consider the unconstrained optimization of a twice-continuously differentiable function F : RNµ → R
              | that is bounded below, i.e., F ∈ {g ∈ C 2 (RNµ ) | inf g(µ) > −∞}, stated as
blank         | 
text          |                                             minimize F (µ).                                         (3.1)
              |                                              µ∈RNµ
blank         | 
              | 
text          | This is the exact form of the reduced-space PDE-constrained optimization problem in (2.84). In
              | general, it is desirable to find the global minimum of (3.1), i.e., the point µ∗ such that F (µ∗ ) ≤ F (µ)
              | for all µ ∈ RNµ ; however, it is impossible to construct an efficient and reliable global optimization
              | algorithm for an arbitrary nonlinear function and we settle for local minima, as defined in Defini-
              | tion 3.1.
blank         | 
text          | Definition 3.1 (Unconstrained local minima). A point µ∗ ∈ RNµ is a local minima of F if there
              | is a neighborhood N of µ∗ such that F (µ∗ ) ≤ F (µ) for all µ ∈ N .
blank         | 
text          |    From Theorem 3.1, if a point µ∗ ∈ RN is a local minima of (3.1), it must be a stationary
              | point (Definition 3.2) of the function F . This is known as a first-order condition since it places a
              | requirement on the gradient of F .
blank         | 
text          | Theorem 3.1 (First-order unconstrained optimality conditions). If µ∗ is a local minimizer of F (µ)
              | and F is continuously differentiable in a neighborhood of µ∗ , then
blank         | 
text          |                                               ∇F (µ∗ ) = 0.                                         (3.2)
blank         | 
text          | Proof. See [143]
blank         | 
text          | Definition 3.2 (Unconstrained stationary point). Any point µ that satisfies ∇F (µ) = 0 is called a
              | stationary point.
blank         | 
text          |    There are also second-order necessary and sufficient conditions for µ∗ to be a local minima of
              | (3.1) that involve (semi-)positive definiteness of the Hessian of F [143]. This work will primarily be
              | concerned with first-order optimality conditions.
blank         | 
              | 
title         | 3.1.1       Error-Aware Multifidelity Trust Region Method
text          | In this section, we consider optimization problems of the form (3.1) where the evaluation of F and its
              | gradient are expensive and look to develop an optimization algorithm that leverages an inexpensive
              | approximation model, mk (µ), at iteration k. It is assumed that evaluation of mk (µ) and its gradient
              | are substantially less expensive than the corresponding operation with F (µ). The approximation
              | model mk (µ) is required to be locally accurate around the kth iterate, µk , but may be inaccurate
              | away from this point. An inexpensive optimization procedure involving the approximation model
              | is intended to improve the current iterate and make progress toward the optimal solution. Due to
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             52
blank         | 
              | 
              | 
text          | the inherent locality of the approximation model, it will not suffice to consider the unconstrained
              | optimization problem
              |                                            minimize mk (µ)                                         (3.3)
              |                                              µ∈RNµ
blank         | 
text          | as the new iterate µk+1 may fail to make progress toward the local minima of (3.1). For this reason,
              | the optimization problem is only posed within a trust region, defined as the sublevel sets of a function
              | ϑk : RNµ → R+ , i.e.,
              |                                        minimize      mk (µ)
              |                                          µ∈RNµ
              |                                                                                                    (3.4)
              |                                        subject to ϑk (µ) ≤ ∆k .
blank         | 
text          | In traditional trust region methods [48], the model is taken as the quadratic approximation of F at
              | µk and the trust region constraint is the Euclidean distance from µk
blank         | 
text          |                                                       1
              |                 mk (µ) = F (µk ) + ∇F (µk )(µ − µk ) + (µ − µk )T ∇2 F (µk )(µ − µk )
              |                                                       2
              |                 ϑk (µ) = kµ − µk k2 ,
blank         | 
text          | A plethora of variants have been proposed that leverage inexact gradients and Hessians in the
              | definition of mk (µ) [189, 35, 48, 108] and non-quadratic model objectives [4, 48, 10, 108]. In this
              | work, the trust region constraint itself is generalized such that error bounds between the objective
              | function and approximation model can be directly leveraged. This will, in a sense, define an error-
              | aware trust region.
              |    Before proceeding to the statement of the complete generalized trust region algorithm, an inter-
              | pretation of an error-aware trust region is provided for a special case. Suppose the scalar-valued
              | constraint function ϑk : RNµ → R+ is defined as the Euclidean norm of a linear vector-valued error
              | indicator ϑk : RNµ → Rm , i.e.,
              |                                           ϑk (µ) = kϑk (µ)k2 .
blank         | 
text          | Additionally, suppose the approximation model is exact at trust region centers and this is reflected
              | in the vector-valued error indicator (ϑk (µk ) = 0), i.e.,
blank         | 
text          |                                          ϑk (µ) = Ak (µ − µk ),
blank         | 
text          | where Ak ∈ Rm×Nµ is a fixed matrix. Then the constraint function can be expanded as
blank         | 
text          |                                   ϑ(µ) = kϑk (µ)k2 = kµ − µk kAT Ak ,                              (3.5)
              |                                                                   k
blank         | 
              | 
              | 
text          | which is precisely a traditional trust region constraint in the ATk Ak -norm. Consider the eigenvalue
              | decomposition of the symmetric positive (semi)-definite matrix ATk Ak
blank         | 
text          |                                           ATk Ak = Qk Λk QTk ,                                     (3.6)
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             53
blank         | 
              | 
              | 
              | 
text          |                                           ∆k
              |                                           λ2 q2        ∆k
              |                                                        λ 1 q1
              |                                                   µk
blank         | 
              | 
              | 
              | 
text          | Figure 3.1: Geometry of trust region constraint in special case where ϑk = kAk (µ − µk )k2 =
              | kµ − µk kAT Ak . The eigenvalue decomposition of ATk Ak is ATk Ak = Qk Λk QTk with eigenvectors
              |            k
              | qi = Qk ei and eigenvalues λi = eTi Λk ei .
blank         | 
              | 
text          | where Qk is an orthogonal matrix of eigenvectors of ATk Ak and Λk is the diagonal matrix of non-
              | negative eigenvalues. The trust region constraint ϑk (µ) ≤ ∆k with ϑk (µ) defined in (3.5) is an ellipse
              |                                                            ∆k
              | with principal axis directions qi = Qk ei and lengths T         for i = 1, . . . , m, where ei ∈ RNµ is
              |                                                          ei Λei
              | the ith canonical vector; see Figure 3.1. Thus the ellipse is stretched (compressed) in directions
              | corresponding small (large) eigenvalues. The matrix ATk Ak represents the sensitivity of the error
              | indicator with respect to the components of µ, which provides intuition to the ellipse interpretation:
              | directions where the error indicator is highly sensitive to perturbations (large eigenvalues) correspond
              | to small principal axes and vice versa.
              |    For the sake of both generality and efficiency, the proposed generalized trust region method
              | will allow the model gradient to be inexact at trust region centers, µk . Aside from the standard
              | assumption imposed on the model function, mk , such as twice-continuous differentiability and uni-
              | formly bounded Hessians, the proposed method requires the existence of functions ϑk : RNµ → R+ ,
              | ϕk : RNµ → R+ and arbitrary constants ζ, ξ > 0 such that
blank         | 
text          |                      |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)     ∀µ ∈ Rk
              |                                                                                                    (3.7)
              |                                   k∇F (µk ) − ∇mk (µk )k ≤ ξϕk (µk )
blank         | 
text          | where Rk := {µ ∈ RNµ | ϑk (µ) ≤ ∆k }. The first bound in (3.7) requires the variation of mk
              | from µk to µ to be related to the variation of F from µk to µ. The does not necessarily place a
              | requirement on the pointwise accuracy of mk with respect to F , even at the trust region center µk .
              | However, a requirement on pointwise accuracy is sufficient to lead to the bound in (3.7) as follows.
              | Suppose there exists a function χk : RNµ → R+ and arbitrary constant κ > 0 such that
blank         | 
text          |                                      |F (µ) − mk (µ)| ≤ κχk (µ).                                   (3.8)
blank         | 
text          | A simple application of the triangle inequality lead to the first bound in (3.7) with ζ = κ and
              | ϑk (µ) = χk (µk ) + χk (µ). The second bound in (3.7) is a requirement on the gradient accuracy
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                              54
blank         | 
              | 
              | 
text          | at the trust region center. The existence of the arbitrary constants implies ϑk (µ) and ϕk (µ) are
              | asymptotic error bounds, which will provide considerable flexibility in deriving explicit expressions
              | for them, even for general PDE-constrained optimization problems, in Chapters 5–6 when specific
              | approximation models are considered.
              |    Each iteration k of the proposed trust region method will rely on four main steps: (1) definition
              | of the trust region model, mk (µ), and constraint, ϑk (µ), (2) computation of a candidate point, µ̂k ,
              | for the next iterate as the solution of the optimization problem (3.4), (3) computation of the ratio
              | of the actual reduction realized by µ̂k to that predicted by the model
blank         | 
text          |                                                F (µk ) − F (µ̂k )
              |                                        ρk =                        ,                                (3.9)
              |                                               mk (µk ) − mk (µ̂k )
blank         | 
text          | and (4) using the value of ρk , decide whether to accept or reject the candidate, µ̂k , and how to
              | modify the trust region radius, ∆k . Each of these steps will be detailed in the sections to follow.
              | The generalized trust region algorithm that incorporates these steps is summarized in Algorithm 1.
              | A proof of global convergence, that is, convergence to a local minima from any starting point µ0 , is
              | provided in Appendix A. The computation of the actual-to-predicted reduction ratio (ρk ) is a severe
              | bottleneck of Algorithm 1 since it requires an evaluation of F . Another approximation model will be
              | introduced to enable an approximation of ρk to be used in place of the true value without destroying
              | global convergence. Therefore, the modified trust region method, summarized in Algorithm 2,
              | circumvents the primary bottleneck of Algorithm 1.
blank         | 
title         | Step 1: Model and constraint update
blank         | 
text          | The first and most important step in an iteration of the generalized trust region method is the
              | definition of the model function, mk (µ), and constraint, ϑk (µ). To guarantee global convergence,
              | the model must be equipped with error bounds of the form (3.7) and conditions must be placed on
              | the value of the error indicators, ϑk (µ) and ϕk (µ), at trust region centers to control the quality of
              | the approximation. The requirement on ϑk (µk ) is simply
blank         | 
text          |                                             ϑk (µk ) ≤ κϑ ∆k                                      (3.10)
blank         | 
text          | where 0 < κϑ < 1 is an algorithmic constant, which ensures the feasible set of (3.4) is not empty
              | and the trust region center (µk ) is in the feasible set. In the special case where ϑk (µ) is a pointwise
              | error indicator of the form χk (µk ) + χk (µ), it places a requirement on the accuracy of the model
              | at the trust region center. In the next section and Lemma A.1, this condition will also be used to
              | circumscribe a traditional trust region feasible set (with modified radius) inside the feasible set of
              | (3.4), which enables standard results from trust region theory to be recycled.
              |    The requirement on ϕk (µk ) is recycled from [93, 108, 109]
blank         | 
text          |                                  ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }                             (3.11)
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                       55
blank         | 
              | 
              | 
              | 
text          | Algorithm 1 Error-aware multifidelity trust region method with exact objective evaluations
              |  1:   Initialization: Given
              |                        µ0 , ∆0 , 0 < γ < 1, ∆max > 0, 0 < η1 < η2 < 1, 0 < κϑ < 1, 0 < κϕ
              |  2:   Model and constraint update: Choose a model, mk (µ), constraint, ϑk (µ), and gradient
              |       error bound, ϕk (µ), such that
blank         | 
text          |                            kF (µk ) − F (µ) + mk (µ) − mk (µk )k ≤ ζϑk (µ)            µ ∈ Rk
              |                                           k∇F (µk ) − ∇mk (µk )k ≤ ξϕk (µk )
              |                                                            ϑk (µk ) ≤ κϑ ∆k
              |                                                            ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }
blank         | 
text          |     where ζ, ξ > 0 are arbitrary constants and Rk = {µ ∈ RNµ | ϑk (µ) ≤ ∆k }
              |  3: Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                          min mk (µ)       subject to      ϑk (µ) ≤ ∆k
              |                                       µ∈RNµ
blank         | 
text          |       for a candidate, µ̂k , that satisfies ϑk (µ̂k ) ≤ ∆k and
blank         | 
text          |                                                                                              k∇mk (µk )k
blank         |                                                                                                           
text          |                   mk (µk ) − mk (µ̂k ) ≥ κs k∇mk (µk )k min (1 −              κϑ )κ−1
              |                                                                                    ∇ϑ ∆k ,
              |                                                                                                 βk
blank         | 
text          |       where κs ∈ (0, 1), k∇ϑk (µ)k ≤ κ∇ϑ for all µ ∈ Rk , and βk := 1 + sup                    ∇2 mk (µ) .
              |                                                                                       µ∈Rk
              |  4:   Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio approximation
              |       according to
              |                                            F (µk ) − F (µ̂k )
              |                                      ρk =
              |                                           mk (µk ) − mk (µ̂k )
              |  5:   Step acceptance:
blank         | 
text          |                  if         ρk ≥ η1       then     µk+1 = µ̂k          else       µk+1 = µk         end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                       if      ρk ≤ η 1             then       ∆k+1 ∈ (0, γϑk (µ̂k )]              end if
              |                       if      ρk ∈ (η1 , η2 )      then       ∆k+1 ∈ [γϑk (µ̂k ), ∆k ]            end if
              |                       if      ρk ≥ η 2             then       ∆k+1 ∈ [∆k , ∆max ]                 end if
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           56
blank         | 
              | 
              | 
text          | where κϕ > 0 is an algorithmic constant. The main purpose of the gradient condition is to ensure
              | sufficient accuracy in the model gradient is obtained near convergence (k∇mk (µk )k small) or after
              | failed steps (∆k small). When combined with the error bound in (3.7), it also guarantees a local
              | minima of F is approached as k∇mk (µk )k → 0. The error bounds and requirements on the error
              | indicators are summarized in (3.12)-(3.15) below
blank         | 
text          |                  |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)     µ ∈ Rk                    (3.12)
              |                               k∇F (µk ) − ∇mk (µk )k ≤ ξϕk (µk )                               (3.13)
              |                                               ϑk (µk ) ≤ κϑ ∆k                                 (3.14)
              |                                               ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }.            (3.15)
blank         | 
text          | where ξ, ζ > 0 are arbitrary constants.
              |    Traditional trust region methods (ϑk (µ) = kµ − µk k) that allow for inexact objective and gra-
              | dient evaluations [93, 108, 109] naturally fit requirements (3.12)-(3.15) as follows. Consider an
              | arbitrary model, mk (µ), and gradient error bound, ϕk (µk ), such that (3.13) and (3.15) are satis-
              | fied. Then, ϑk (µ) = kµ − µk k automatically satisfies (3.12) and (3.14). Condition (3.14) is trivial
              | to verify since ϑk (µk ) = 0. Condition (3.12) is verified, following [108], by considering the Taylor
              | expansion of F and mk about µk
blank         | 
text          |                                                      1
              |                F (µ) = F (µk ) + ∇F (µk )(µ − µk ) + (µ − µk )T ∇2 F (y)(µ − µk )
              |                                                      2
              |                                                        1
              |               mk (µ) = mk (µk ) + ∇mk (µk )(µ − µk ) + (µ − µk )T ∇2 mk (z)(µ − µk )
              |                                                        2
blank         | 
text          | where y, z ∈ RNµ are arbitrary points that lie on the line between µ and µk . Subtracting these
              | equations, subsequent rearrangement, and application of the triangle inequality leads to
blank         | 
text          |                                                                         1                             2
              | |F (µk )−F (µ)+mk (µ)−mk (µk )| ≤ k∇F (µk ) − ∇mk (µk )k kµ − µk k+       ∇2 F (y) − ∇mk (z) kµ − µk k .
              |                                                                         2
blank         | 
text          | The gradient condition (3.15) and the fact that µ ∈ Rk = {y ∈ RN | ky − µk k ≤ ∆k } are used to
              | reduce the above inequality to
blank         | 
text          |                                                              1
              |                |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ (ξκϕ + αk )∆k kµ − µk k
              |                                                              2
blank         | 
text          |                   ∇2 F (µ) + ∇2 mk (µ) is well-defined if the objective and model Hessians
blank         |                                            
text          | where αk = sup
              |             µ∈Rk
              | are uniformly bounded on Rk . Furthermore, assume these Hessians are uniformly bounded on all
              | of RNµ , i.e., there exists α > 0 such that αk ≤ α. This assumption and the introduction of an
              | algorithmic parameter ∆max such that ∆k ≤ ∆max (see discussion of step 4) leads to the desired
              | result
              |                            |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)
blank         | 
text          | where ζ = (ξκϕ + α/2)∆max is a constant.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            57
blank         | 
              | 
              | 
text          | Remark. A similar generalization of trust region methods was introduced in [208] that used reduced-
              | order models for the approximation model of the linear PDE. However, the method in that work
              | requires the pointwise error bound (3.8) on the objective accuracy. The bound on the objective
              | variation in (3.7) was shown to be a weaker condition than the pointwise bound since (3.8) is a
              | special case of (3.7). Additionally, the objective variation bound provides considerable flexibility
              | compared to the pointwise bound. For example, the bound in (3.7) encompasses the traditional trust
              | region constraint ϑk (µ) = kµ − µk k without requiring zeroth-order consistency mk (µk ) = F (µk ),
              | whereas this would would be required by the pointwise bound since ϑk (µk ) = 0. Therefore the bound in
              | (3.7) enables the generalized trust region method to reduce to a traditional trust region method, even
              | when the model objective is inexact at trust region centers. This will be exploited in Chapters 5–6.
blank         | 
text          | Step 2: Step candidate as solution of trust region subproblem
blank         | 
text          | The model and trust region constraint functions are used to form the trust region subproblem in
              | (3.4) whose minimizer is used as the candidate for µk+1 . In traditional trust region methods, it is
              | well-known that the trust region subproblem does not need to be solved exactly. In fact, it may be
              | as difficult to solve the trust region subproblem as the original unconstrained optimization problem
              | (3.1). Define the Cauchy point (Definition 3.3) as the minimizer of the trust region subproblem
              | restricted to the steepest decent direction. It turns out that an essential component in the global
              | convergence theory of trust region methods is the decrease in the model realized by the Cauchy
              | point (Theorem 3.2) [133].
blank         | 
text          | Definition 3.3 (Cauchy point). The Cauchy point of the trust region subproblem
blank         | 
text          |                                       minimize     mk (µ)
              |                                        µ∈RNµ
              |                                                                                                 (3.16)
              |                                       subject to   kµ − µk k ≤ ∆k
blank         | 
text          |               ∗                  ∗
              | is µC
              |     k = µk − s ∇mk (µk ), where s is the solution of the univariate optimization problem
blank         | 
              | 
text          |                                     minimize    mk (µk − s∇mk (µk ))
              |                                       s≥0
              |                                                                                                 (3.17)
              |                                     subject to s k∇mk (µk )k ≤ ∆k .
blank         | 
text          | Theorem 3.2 (Cauchy decrease). The decrease in the model from µk to the Cauchy point µC
              |                                                                                       k is at
              | least
              |                                                                    kmk (µk )k
blank         |                                                                                  
text          |                                             1
              |                     mk (µk ) −   mk (µC
              |                                       k)   ≥ k∇mk (µk )k min                  , ∆k ,            (3.18)
              |                                             2                         βk
blank         | 
text          | where βk := 1 + sup    ∇2 mk (µ) .
              |                 µ∈Rk
blank         | 
text          | Proof. See Theorem 6.3.1 of [48].
blank         | 
text          |    Therefore, instead of requiring the candidate for µk+1 be the exact minimizer of (3.16), it suffices
              | to use any point that achieves a fraction of the Cauchy decrease [133, 48]. This not only provides
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           58
blank         | 
              | 
              | 
text          | an opportunity for efficiency, but also a convenient framework for analyzing global convergence
              | properties.
              |    Since these results pertaining to the Cauchy point and its connection to global convergence
              | theory are specific to the quadratic constraint constraint in (3.16), this section aims to generalize
              | these concepts for trust region subproblems of the form (3.4). This can easily be done if the gradient
              | of the constraint is bounded within the trust region, i.e., k∇ϑk (µ)k ≤ κ∇ϑ for all µ ∈ Rk . In this
              | case, Lemma A.1 guarantees {µ ∈ RNµ | kµ − µk k ≤ (1 − κϕ )κ−1
              |                                                             ∇ϑ ∆k } ⊂ {µ ∈ R
              |                                                                              Nµ
              |                                                                                 | ϑk (µ) ≤ ∆k }.
              | Thus, from Theorem 3.2, there exists a point in the trust region µ ∈ Rk such that
blank         | 
text          |                                                              kmk (µk )k
blank         |                                                                                            
text          |                mk (µk ) − mk (µ) ≥ κs k∇mk (µk )k min                   , (1 − κϕ )κ−1
              |                                                                                     ∇ϑ ∆k       (3.19)
              |                                                                 βk
blank         | 
text          | where κs ∈ (0, 1). Appendix A will show that this condition that resembles the fraction of Cauchy
              | decrease leads to global convergence of the proposed trust region method.
              |    In this work, a substantial cost difference is assumed to separate evaluations of F (µ) and ∇F (µ)
              | from evaluations mk (µ) and ∇mk (µ) so the trust region subproblem is solved exactly with little
              | penalty. Section 3.1.2 details an interior-point method to solve the trust region subproblem (3.4).
blank         | 
title         | Step 3: Actual-to-predicted decrease ratio
blank         | 
text          | After the candidate µ̂k has been computed, the ratio between the reduction in F and mk that would
              | be realized by taking this step is computed according (3.9). This will be used in the next section to
              | determine if the step should be accepted and to modify the trust region radius. The computation
              | of ρk according to (3.9) requires queries to the expensive function F (µ) and therefore constitutes
              | a major bottleneck in the trust region algorithm. Following the work in [109], this bottleneck is
              | mitigated through the introduction of another approximation, ψk (µ) that will be used solely in the
              | computation of ρk , i.e.,
              |                                              ψk (µk ) − ψk (µ̂k )
              |                                       ρk =                        .                             (3.20)
              |                                              mk (µk ) − mk (µ̂k )
              | Define ψk : RNµ → R as an approximation of the F (µ), equipped with a familiar asymptotic error
              | bound: there exists a constant σ > 0 such that
blank         | 
text          |                             |F (µk ) − F (µ) + ψk (µ) − ψk (µk )| ≤ σθk (µ),                    (3.21)
blank         | 
text          | where θk : RNµ → R+ is an error indicator. To ensure global convergence (see Appendix A for
              | proof) in the presence of this additional approximation, θk (µ̂k ) must satisfy
blank         | 
text          |                               θkω (µ̂k ) ≤ η min{mk (µk ) − mk (µ̂k ), rk },                    (3.22)
blank         | 
text          | where ω ∈ (0, 1), η < min{η1 , 1 − η2 }, and {rk }∞
              |                                                   k=1 is a sequence such that rk → 0. The algorithmic
              | parameters η1 and η2 are related to the specifics of the step assessment and radius modification
              | detailed in the next section. The forcing sequence rk is required to ensure ρk in (3.20) approaches
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             59
blank         | 
              | 
              | 
text          | the true ratio between the actual and predicted reduction and is taken in this work as rk = 1/(k +1).
              |    The flexibility afforded by the use of an approximation model reveals an immediate and obvious
              | improvement to the generalized trust region algorithm. The error bound required between ψk (µ) and
              | θk (µ) in (3.21) is identical to the relationship between mk (µ) and ϑk (µ) in (3.7), which immediately
              | suggests the choice ψk (µ) = mk (µ) and θk (µ) = ϑk (µ). From the discussion above, this choice will
              | lead to a globally convergent algorithm provided
blank         | 
text          |                                ϑk (µ̂k )ω ≤ η min{mk (µk ) − mk (µ̂k ), rk }.                    (3.23)
blank         | 
text          | This condition is inexpensive to check since it only involves queries to the approximation model
              | and error indicator and does not require evaluations of the expensive objective function F (µ).
              | Additionally, the choice ψk (µ) = mk (µ) guarantees the approximation of the actual-to-predicted
              | reduction ratio is always unity, i.e.,
blank         | 
text          |                                  ψk (µk ) − ψk (µ̂k )   mk (µk ) − mk (µ̂k )
              |                           ρk =                        =                      = 1.                (3.24)
              |                                  mk (µk ) − mk (µ̂k )   mk (µk ) − mk (µ̂k )
blank         | 
text          | Thus, for a given iteration k, if the approximation model mk (µ) and error indicator ϑk (µ) chosen
              | in the first step of the generalized trust region algorithm satisfy (3.23), ρk can be taken as unity
              | without any additional work. The next section will classify such a step as very successful and the
              | step will be accepted µk+1 = µ̂k and the trust region radius increased. In the event that (3.23)
              | is not satisfied, the choice ψk (µ) = mk (µ) and θk (µ) = ϑk (µ) is not sufficient to guarantee global
              | convergence and ψk (µ) and θk (µ) must be constructed to satisfy (3.21)-(3.22).
              |    Algorithm 2 states the optimized trust region method that incorporates this additional level
              | of approximation. Appendix A details the global convergence proof for this algorithm. Global
              | convergence of Algorithm 1, i.e., without the ψk approximation model, follows trivially by taking
              | ψk (µ) = F (µ) and θk (µ) = 0.
blank         | 
title         | Step 4: Step assessment and radius update
blank         | 
text          | Once ρk is computed according to either (3.9) or (3.20), the quality of the step is assessed by
              | comparing ρk to unity, i.e., the actual-to-predicted ratio if the model was perfect, mk (µ) = F (µ).
              | If the ρk is close to unity, the step is accepted by setting µk+1 = µ̂k and the trust region radius is
              | increased. In the case where ρk  1, especially if it is negative (the true objective fails to decrease:
              | F (µ̂k ) > F (µk )), the step is rejected, µk+1 = µk , and trust region radius is decreased.
              |    For a practical algorithm, define algorithmic constants 0 < η1 < η2 < 1 that will indicate
              | values of ρk that govern step acceptance and the radius update. If ρk ≤ η1 , the model did not
              | substantially reduce the true objective so the step is rejected and the trust region radius decreased
              | such that ∆k+1 ≤ γϑk (µ̂k ) where 0 < γ < 1 is a constant. This will be called an unsuccessful
              | step. Modification of the radius in this manner ensures that if the optimization problem in (3.4)
              | terminates at a point µ̂k strictly interior to the feasible set Rk = {µ ∈ RNµ | ϑk (µ) ≤ ∆k }, the
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                      60
blank         | 
              | 
              | 
              | 
text          | Algorithm 2 Error-aware multifidelity trust region method with inexact objective evaluations
              |  1:   Initialization: Given
blank         | 
text          |                       µ0 , ∆0 , 0 < γ < 1, ∆max > 0, 0 < η1 < η2 < 1, 0 < κϑ < 1, 0 < κϕ ,
              |                                   ω ∈ (0, 1), {rk }∞
              |                                                    k=1 ⊂ [0, ∞) such that rk → 0
blank         | 
text          |  2:   Model and constraint update: Choose a model, mk (µ), constraint, ϑk (µ), and gradient
              |       error bound, ϕk (µ), such that
blank         | 
text          |                           |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)            µ ∈ Rk
              |                                          k∇F (µk ) − ∇mk (µk )k ≤ ξϕk (µk )
              |                                                          ϑk (µk ) ≤ κϑ ∆k
              |                                                          ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }
blank         | 
text          |     where ζ, ξ > 0 are arbitrary constants and Rk = {µ ∈ RNµ | ϑk (µ) ≤ ∆k }
              |  3: Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                         min mk (µ)       subject to          ϑk (µ) ≤ ∆k
              |                                     µ∈RNµ
blank         | 
text          |       for a candidate, µ̂k , that satisfies ϑk (µ̂k ) ≤ ∆k and
blank         | 
text          |                                                                           k∇mk (µk )k
blank         |                                                                                      
text          |                mk (µk ) − mk (µ̂k ) ≥ κs k∇mk (µk )k min (1 − κϑ )κ−1 ∆
              |                                                                    ∇ϑ k ,                                  (3.25)
              |                                                                              βk
blank         | 
text          |       where κs ∈ (0, 1), k∇ϑk (µ)k ≤ κ∇ϑ for all µ ∈ Rk , and βk := 1 + sup                   ∇2 mk (µ)
              |                                                                                       µ∈Rk
              |  4:   Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio approximation
              |       according to
              |                       
              |                       
              |                                 1           if ϑk (µ̂k )ω ≤ η min{mk (µk ) − mk (µ̂k ), rk }
              |                  ρk =
              |                        ψk (µk ) − ψk (µ̂k ) otherwise
              |                       
              |                         mk (µk ) − mk (µ̂k )
blank         | 
text          |       where ψk (µ) and θk (µ) satisfy
blank         | 
text          |                      kψk (µk ) − ψk (µ) + mk (µ) − mk (µk )k ≤ σθk (µ)              µ ∈ Rk
              |                                                                                                            (3.26)
              |                                                          θkω (µ̂k )   ≤ η min{mk (µk ) − mk (µ̂k ), rk }
blank         | 
text          |     where η < min{η1 , 1 − η2 } and σ > 0 is an arbitrary constant
              |  5: Step acceptance:
blank         | 
text          |                 if        ρk ≥ η1        then      µk+1 = µ̂k            else     µk+1 = µk       end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                      if      ρk ≤ η 1             then         ∆k+1 ∈ (0, γϑk (µ̂k )]            end if
              |                      if      ρk ∈ (η1 , η2 )      then         ∆k+1 ∈ [γϑk (µ̂k ), ∆k ]          end if
              |                      if      ρk ≥ η 2             then         ∆k+1 ∈ [∆k , ∆max ]               end if
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                               61
blank         | 
              | 
              | 
text          | feasible set at the next iteration, Rk+1 , will not include µ̂k . If ρk ∈ (η1 , η2 ), the step is accepted
              | and the trust region radius is not modified, ∆k+1 = ∆k . This is called a successful step. Finally,
              | if ρk ≥ η2 , the step is accepted since the model predicted the decrease in the objective to high
              | accuracy. In this type of very successful step, the trust region constraint may be too restrictive
              | so the radius is increased, usually according to ∆k+1 = min{(1/γ)∆k , ∆max }, where ∆max is an
              | algorithmic parameter that specifies the maximum trust region radius.
blank         | 
title         | Summary
blank         | 
text          | Two variants of a generalized, multifidelity trust region method were introduced in this section and
              | global convergence was established for both methods in Appendix A. The first version, presented
              | in Algorithm 1, requires the computation of the exact ratio of actual-to-predicted reduction. This
              | method is completely prescribed once the approximation function mk (µ) and error indicators ϑk (µ)
              | and ϕk (µ) that satisfy (3.12)-(3.15) have been defined. Unlike traditional trust region methods,
              | the trust region subproblem of the proposed method is a difficult optimization problem that cannot
              | leverage the many highly efficient trust region solvers—see [48] for a review—and calls for an exact
              | nonlinear optimization solver. Section 3.1.2 presents a simple primal interior point method based
              | on quasi-Newton search directions and a backtracking linesearch to solve the optimization problem
              | in (3.4) exactly. This guarantees the candidate point µ̂k will satisfying the fraction of Cauchy
              | decrease condition (Theorem 3.2), an important component of the global convergence theory. The
              | variant of the error-aware multifidelity trust region method presented in Algorithm 2 leverages
              | an approximation of the actual-to-predicted reduction ratio. In additional to mk (µ) and the error
              | indicators ϑk (µ), ϕk (µ), this method requires the construction of an additional approximation model
              | ψk (µ) and error indicator θk (µ) that satisfy (3.21)-(3.22). This flexibility was leveraged to define
              | condition (3.23) that ensures the choice ψk (µ) = mk (µ) and θk (µ) = ϑk (µ) will preserve global
              | convergence and guarantees step k is very successful without requiring queries to the expensive
              | objective F (µ) or construction of a new approximation model.
              |    As written, Algorithms 1 and 2 are skeletons since details pertaining to the construction of
              | the approximation models mk (µ), ψk (µ) and error indicators ϑk (µ), ϕk (µ), θk (µ) have been ab-
              | stracted away. This will serve as the point of departure for Chapters 5–6, which will construct
              | these approximation models and error indicators for the specific class of problems under consider-
              | ation. In particular, Chapter 5 will use projection-based reduced-order/hyperreduced models and
              | residual-based error bounds to define these trust region functions in the context of deterministic
              | PDE-constrained optimization. Chapter 6 will combine projection-based reduced-order models and
              | sparse grids to define the approximation model to efficiently solve stochastic PDE-constrained opti-
              | mization problems. The error indicators will use residual-based error bounds to account for pointwise
              | error and dimension-adaptive sparse grids [67] to account for truncation error. The proposed meth-
              | ods will implicitly assume there are relatively few parameters compared to the size of the state vector
              | Nµ  Nu (since reduction will solely be applied to the state space). Appendix C will discuss the
              | use of linesearch and subspace methods to extend the proposed methods to efficiently handle many
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             62
blank         | 
              | 
              | 
text          |                                                 φγk (x)
blank         | 
              | 
              | 
              | 
text          |                                                                            x
blank         | 
text          | Figure 3.2: Logarithmic barrier function (3.27) corresponding to mk (x) = x4 − x3 (     ), ϑk (x) = x2 ,
              | ∆k = 1 with γ = 0.1 (    ) and γ = 0.0001 (     ).
blank         | 
              | 
text          | optimization variables, i.e., Nµ = O(Nu ).
blank         | 
              | 
title         | 3.1.2     Interior-Point Method for Trust Region Subproblem
text          | The trust region subproblem employed in the proposed generalized trust region method is a general
              | nonlinear program and cannot be (approximately) solved with the plethora of highly efficient and
              | specialized trust region subproblem solvers that have been developed [133, 48]. In this work, the
              | trust region subproblem is solved exactly (up to a tolerance on the first-order optimality conditions),
              | which is in opposition to most trust region methods that only seek a point that achieves a fraction
              | of the Cauchy decrease. Due to the assumed substantial cost separation between the evaluation
              | of F and mk , an exact trust region solver comes at a relatively small penalty in cost. In fact, it
              | may even be substantially more efficient than finding an approximate minimizer if it can result in
              | even one fewer query to F . While any nonlinear optimization solver can be employed to solve the
              | trust region subproblem (3.4), an interior point method [143] is used since the trust region center is
              | strictly interior to the feasible set from condition (3.14).
              |    Consider the logarithmic barrier function associated with the optimization problem (3.4)
blank         | 
text          |                                 φγk (µ) = mk (µ) − γ log [∆k − ϑk (µ)] .                         (3.27)
blank         | 
text          | This function, shown in Figure 3.2 for a specific choice of mk and ϑk , tends to +∞ as µ approaches
              | the boundary of the feasible set. This ensures that an unconstrained optimization problem of the
              | form
              |                                             minimize φγk (µ)                                     (3.28)
              |                                              µ∈RNµ
blank         | 
text          | will remain interior to the feasible set (provided the initial guess is a feasible point). The uncon-
              | strained optimization problem in (3.28) approaches the constrained optimization problem in (3.41)
              | as the barrier parameter goes to zero, i.e., γ → 0. Thus, the constrained optimization problem in
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            63
blank         | 
              | 
              | 
text          | (3.4) has been reduced to a sequence of unconstrained optimization problems (3.28) correspond-
              | ing to a sequence γp → 0. A more robust variant of the primal interior-point method discussed
              | is a primal-dual approach that avoids the difficulty of solving (3.28) when γ approaches 0; how-
              | ever, only the primal approach will be considered for simplicity. Each unconstrained optimization
              | problem is solved using a quasi-Newton method with Broyden-Fletcher-Goldfarb-Shanno (BFGS)
              | Hessian updates and a backtracking linesearch to satisfy the Armijo sufficient decrease condition.
              | Quasi-Newton methods look to improve an iterate µjk (the subscript k denotes the trust region, or
              | major, iteration and the superscript j denotes the subproblem, or minor, iteration) by search along
              | a direction, pjk , defined as the solution of
blank         | 
text          |                                               Bkj pjk = −∇φγk (µjk ),                           (3.29)
blank         | 
text          | where Bkj is a symmetric positive-definite approximation of the Hessian ∇2 φγk (µjk ). The BFGS
              | Hessian update defines Bkj from Bkj−1 according to
blank         | 
text          |                                                                   T                T
              |                                                         ykj ykj           Bkj sjk sjk Bkj
              |                                   Bkj+1   =   Bkj   +      T
              |                                                                       −       T
              |                                                                                                 (3.30)
              |                                                         ykj sjk            sjk Bkj sjk
blank         | 
text          | where
              |                            sjk = µj+1
              |                                   k   − µjk             ykj = ∇φγk (µj+1     γ   j
              |                                                                      k ) − ∇φk (µk )
blank         | 
text          | and the Hessian approximation is initialized as the identity Bk0 = I (implying the first search
              | direction p0k is the steepest descent direction). With the search direction computed according to
              | (3.29), the new subproblem iterate is computed as
blank         | 
text          |                                                µj+1
              |                                                 k   = µjk + αpjk ,                              (3.31)
blank         | 
text          | where α > 0 is selected such that the Armijo condition
blank         | 
text          |                                                                                T
              |                                φγk (µjk + αpjk ) ≤ φγk (µjk ) + αcpjk ∇φγk (µjk )               (3.32)
blank         | 
text          | is satisfied, where c > 0 is a constant usually taken as c = 10−4 . The step length α is determined
              | via a backtracking algorithm, i.e., α = τ n , where τ ∈ (0, 1) is the backtrack factor and n ≥ 0 is the
              | smallest integer such that (3.32) is satisfied. The complete algorithm is summarized in Algorithm 3.
blank         | 
              | 
title         | 3.1.3     Numerical Experiment: Contrived
text          | The generality and performance of the multifidelity trust region method proposed in this chapter is
              | demonstrated on the canonical Rosenbrock problem
blank         | 
text          |                             minimize
              |                                  2
              |                                      F (µ) := 100(µ2 − µ21 )2 + (1 − µ1 )2 .                    (3.33)
              |                                µ∈R
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                        64
blank         | 
              | 
              | 
              | 
text          | Algorithm 3 Interior Point BFGS Method with Backtracking Linesearch
              |  1:   Initialization: Given
blank         | 
text          |                                  µ0k = µk , γ, B0 = I, 0 < c < 1, 0 < τ < 1
blank         | 
text          |  2:   Search direction computation: Define step direction, pjk , as the solution of
blank         | 
text          |                                                 Bkj pjk = −∇φγk (µjk )
blank         | 
text          |  3:   Linesearch: Define the step length as α = τ n where n is the smallest integer such that
              |                                                                                     T
              |                                 φγk (µjk + τ n pjk ) ≤ φγk (µjk ) + τ n cpjk ∇φγk (µjk )
blank         | 
text          |  4:   Update iterate: Given search direction pjk and step length α, update current iterate
blank         | 
text          |                                                  µj+1
              |                                                   k   = µjk + αpjk
blank         | 
text          |  5:   BFGS update: Define sjk and ykj as
blank         | 
text          |                               sjk = µj+1
              |                                      k   − µjk            ykj = ∇φγk (µj+1     γ   j
              |                                                                        k ) − ∇φk (µk )
blank         | 
text          |       and Hessian approximation update as
              |                                                                     T                   T
              |                                                           ykj ykj           Bkj sjk sjk Bkj
              |                                     Bkj+1   =   Bkj   +      T
              |                                                                         −       T
              |                                                           ykj sjk            sjk Bkj sjk
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                               65
blank         | 
              | 
              | 
text          | The approximation model and error indicators that will be used are not less expensive to evaluate
              | than the objective function F (µ), which is an underlying assumption of the proposed methods. The
              | purpose of this section is to study the behavior of the generalized trust region algorithm on a simple
              | problem; Chapters 5 and 6 will consider more interesting applications where the approximation
              | model and error indicators are substantially less expensive to evaluate than F (µ).
              |    The model function will be taken as a quadratic approximation of F (µ) with controllable errors
              | introduced into the value and gradient at the expansion point. For this purpose define
blank         | 
text          |                                                                1
              |       G(µ; µ̄, , δ) := F (µ̄) +  + (∇F (µ̄) + δ1)T (µ − µ̄) + (µ − µ̄)T ∇2 F (µ̄)(µ − µ̄),       (3.34)
              |                                                                2
blank         | 
text          | where the gradient and Hessian of F (µ) are
              |                                           "                                #
              |                                            −400µ1 (µ2 − µ21 ) − 2(1 − µ1 )
              |                                ∇F (µ) =
              |                                                     200(µ2 − µ21 )
              |                                           "                           #                            (3.35)
              |                                 2          400(3µ21 − µ2 ) + 2 −400µ1
              |                               ∇ F (µ) =                                       .
              |                                                    −400µ1             200
blank         | 
text          | The gradient of G(µ; µ̄, , δ) is
blank         | 
text          |                           ∇G(µ; µ̄, , δ) = ∇F (µ̄) + δ1 + ∇2 F (µ̄)(µ − µ̄).                      (3.36)
blank         | 
text          | From the definition of G and its gradient, it is clear that , δ are the errors in the value and gradient,
              | respectively, at the expansion point µ̄ since the evaluation of G and ∇G at µ̄ gives
blank         | 
text          |                     G(µ̄; µ̄, , δ) = F (µ̄) +       ∇G(µ̄; µ̄, , δ) = ∇F (µ̄) + δ1.             (3.37)
blank         | 
text          | The approximation model mk (µ) is taken as the (inexact) quadratic approximation of F (µ) at the
              | trust region center, the trust region constraint is taken based on the exact pointwise objective error,
              | and the gradient error indicator ϕk (µ) is taken to be the exact gradient error, i.e.,
blank         | 
text          |                  mk (µ) := G(µ, µk , k , δk )
              |                   ϑk (µ) := |F (µ) − G(µ; µk , k , δk )| + |F (µk ) − G(µk ; µk , k , δk )|      (3.38)
              |                   ϕk (µ) := k∇F (µ) − ∇G(µ; µk , k , δk )k
blank         | 
text          | where the error terms k , δk must be chosen based on the requirements of the global convergence
              | theory. With these choices, the error bounds in (3.12)-(3.13) hold with ζ = ξ = 1. The value of
              | k and δk will be chosen to ensure the error conditions (3.14) and (3.15) hold. At the trust region
              | centers, the error indicators reduce to the following simple expressions
blank         | 
text          |                                               ϑk (µk ) = 2k
              |                                                          √                                         (3.39)
              |                                               ϕk (µk ) = 2δk
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            66
blank         | 
              | 
              | 
              | 
text          |                                1.0
blank         | 
              | 
              | 
              | 
text          |                                0.5
blank         | 
              | 
              | 
              | 
text          |                          µ2
              |                                0.0
blank         | 
              | 
              | 
              | 
text          |                               −0.5
blank         | 
              | 
              | 
              | 
text          |                               −1.0
              |                                 −0.2   0.0    0.2   0.4        0.6   0.8   1.0
              |                                                           µ1
blank         | 
              | 
              | 
text          | Figure 3.3: Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33). The contours
              | represent the true function F (µ), the red dots indicate trust region centers, and the blue line is the
              | trajectory of the trust region subproblem.
blank         | 
              | 
text          | and the error conditions in (3.14)-(3.15) become
blank         | 
text          |                                             κϑ
              |                                        k ≤     ∆k
              |                                              2
              |                                             κϕ                                                   (3.40)
              |                                        δk ≤ √ min{k∇mk (µk )k , ∆k }
              |                                                2
blank         | 
text          | Since the right-hand side of the inequality for k is independent of k , admissible values are easily
              | determined and k = κϑ ∆k /2 will be used throughout. The right-hand side of the inequality for δk
              | depends on δk itself (through mk (µk )) and, in general, an iterative algorithm must be used. A simple
              | backtracking algorithm is used where any initial value of δk is chosen and reduced by a predefined
              | factor until (3.40) is satisfied.
              |    With the proposed definitions of mk (µ), ϑk (µ), and ϕk (µ) all ingredients necessary for the com-
              | plete description of Algorithm 1 have been prescribed. The trust region subproblem (3.4) is solved
              | using the BFGS interior-point method described in Section 3.1.2; the non-quadratic trust region
              | constraint eliminates the possibility of using standard trust region solvers such as Steihaug-Toint
              | CG. The trajectory of the optimization iterations—including the progress of the trust region centers
              | and the trajectory of each trust region subproblem—is shown in Figure 3.3. Figure 3.4 provides
              | additional insight to the Algorithm 1 by showing individual iterations, including the trust region
              | center, candidate step, and feasible region for the trust region subproblem. Notice the substantial
              | difference between the shape of the trust regions in Figure 3.4 and traditional trust regions that are
              | spheres or ellipsoids. These error-aware trust region allows progress to be made toward the optimal
              | solution by searching regions of the parameter space where the model is sufficiently accurate.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                                                                                     67
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  µ2
blank         | 
              | 
              | 
              | 
text          |                                                                µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       µ2
              |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       −0.5                                                          −0.5                                                   −0.5
blank         | 
text          |                                             Optimal solution
              |                                             TR center
              |                                             Candidate
              |       −1.0                                                          −1.0                                                   −1.0
              |         −0.2   0.0   0.2   0.4        0.6     0.8      1.0            −0.2   0.0   0.2   0.4        0.6   0.8   1.0          −0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  µ1                                                            µ1                                                     µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  µ2
blank         | 
              | 
              | 
              | 
text          |                                                                µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       µ2
              |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       −0.5                                                          −0.5                                                   −0.5
blank         | 
              | 
              | 
              | 
text          |       −1.0                                                          −1.0                                                   −1.0
              |         −0.2   0.0   0.2   0.4        0.6     0.8      1.0            −0.2   0.0   0.2   0.4        0.6   0.8   1.0          −0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  µ1                                                            µ1                                                     µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  µ2
blank         | 
              | 
              | 
              | 
text          |                                                                µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       µ2
              |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       −0.5                                                          −0.5                                                   −0.5
blank         | 
              | 
              | 
              | 
text          |       −1.0                                                          −1.0                                                   −1.0
              |         −0.2   0.0   0.2   0.4        0.6     0.8      1.0            −0.2   0.0   0.2   0.4        0.6   0.8   1.0          −0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  µ1                                                            µ1                                                     µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  µ2
blank         | 
              | 
              | 
              | 
text          |                                                                µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       µ2
blank         | 
              | 
              | 
              | 
text          |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       −0.5                                                          −0.5                                                   −0.5
blank         | 
              | 
              | 
              | 
text          |       −1.0                                                          −1.0                                                   −1.0
              |         −0.2   0.0   0.2   0.4        0.6     0.8      1.0            −0.2   0.0   0.2   0.4        0.6   0.8   1.0          −0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  µ1                                                            µ1                                                     µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  µ2
blank         | 
              | 
              | 
              | 
text          |                                                                µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       µ2
blank         | 
              | 
              | 
              | 
text          |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       −0.5                                                          −0.5                                                   −0.5
blank         | 
              | 
              | 
              | 
text          |       −1.0                                                          −1.0                                                   −1.0
              |         −0.2   0.0   0.2   0.4        0.6     0.8      1.0            −0.2   0.0   0.2   0.4        0.6   0.8   1.0          −0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  µ1                                                            µ1                                                     µ1
blank         | 
              | 
              | 
              | 
text          | Figure 3.4: Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33); iterations
              | proceed from left to right then top to bottom. The contours represent the true function F (µ), the
              | red dots indicate trust region centers µk , the blue dots are the candidate for the next trust region
              | center µ̂k , and the green region indicates the feasible set for the trust region subproblem.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            68
blank         | 
              | 
              | 
              | 
text          |                   101
blank         | 
text          |                  10−2
blank         | 
text          |                  10−5
blank         | 
text          |                  10−8
              |                         0         5           10               15     20          25
              |                                                    iteration
blank         | 
text          | Figure 3.5: Convergence history of the objective quantities using Algorithm 1: F (µk ) (  ), F (µ̂k )
              | (    ), mk (µk ) (  ), mk (µ̂k ) (  ). Steady progress is made toward the optimal solution, despite
              | the objective and model only agreeing at iteration 0.
blank         | 
              | 
text          |                   102
blank         | 
              | 
text          |                   100
blank         | 
              | 
text          |                  10−2
blank         | 
              | 
text          |                  10−4
              |                         0         5           10               15     20          25
              |                                                    iteration
blank         | 
text          | Figure 3.6: Convergence history of gradient quantities using Algorithm 1: k∇F (µk )k (         ),
              | k∇F (µ̂k )k ( ), k∇mk (µk )k (  ). The gradient of the true objective function decreases 6 orders
              | of magnitude.
blank         | 
              | 
text          |    Figures 3.5 and 3.6 show the convergence history of the objective and gradient quantities, re-
              | spectively. From Figure 3.5 it can be seen that the objective function F (µ) continually decreases as
              | the algorithm iterates, despite the fact that the model values mk (µ) do not agree well with F (µ) at
              | either the trust region centers µk or candidates µ̂k . Figure 3.6 shows that the first-order optimality
              | condition decreases 6 orders of magnitude throughout the iterations. A more detailed report of the
              | convergence history is provided in Table 3.1.
              |              Table 3.1: Convergence history of Algorithm 1 applied to the Rosenbrock problem.
blank         | 
text          |   F (µk )       mk (µk )      F (µ̂k )     mk (µ̂k )   k∇F (µk )k        ρk            ∆k        Success?
              | 1.0100e+02    1.0150e+02    1.1002e+00   1.0017e-01    2.0001e+02      9.8521e-01   2.0000e+00    True
              | 1.1002e+00    2.1002e+00    9.1177e-01   2.0676e+00    2.1176e+00     5.7778e+00    4.0000e+00    True
              | 9.1177e-01    2.9118e+00    1.5584e+00   2.6152e+00    6.2403e+00    -2.1805e+00    2.6419e-01    False
              | 9.1177e-01    1.0439e+00    6.5172e-01   8.7534e-01    6.2403e+00     1.5431e+00    5.2838e-01    True
              | 6.5172e-01    9.1592e-01    4.8297e-01   8.4624e-01    2.5230e+00     2.4220e+00    1.0568e+00    True
              | 4.8297e-01    1.0114e+00    4.8755e-01   9.8450e-01    4.1851e+00     -1.7086e-01   1.2424e-01    False
              | 4.8297e-01    5.4508e-01    3.2345e-01   4.4402e-01    4.1851e+00     1.5783e+00    2.4847e-01    True
              | 3.2345e-01    4.4769e-01    2.2451e-01   4.0474e-01    2.9992e+00     2.3038e+00    4.9694e-01    True
              | 2.2451e-01    4.7298e-01    2.3107e-01   4.6842e-01    1.9897e+00    -1.4389e+00    5.9337e-02    False
              | 2.2451e-01    2.5418e-01    1.4907e-01   1.8817e-01    1.9897e+00     1.1428e+00    1.1867e-01    True
              | 1.4907e-01    2.0840e-01    8.2419e-02   1.5822e-01    6.8219e+00     1.3281e+00    2.3735e-01    True
              | 8.2419e-02    2.0109e-01    8.6494e-02   2.0100e-01    3.4986e-01    -4.4516e+01    2.8627e-02    False
              | 8.2419e-02    9.6733e-02    7.9495e-02   5.0868e-02    3.4986e-01     6.3763e-02    7.1567e-03    False
              | 8.2419e-02    8.5998e-02    4.3691e-02   3.6534e-02    3.4986e-01     7.8297e-01    1.4313e-02    True
              | 4.3691e-02    5.0847e-02    1.3241e-02   2.4755e-02    5.6732e+00     1.1670e+00    2.8627e-02    True
              | 1.3241e-02    2.7555e-02    4.3950e-03   2.3160e-02    2.5786e-01     2.0128e+00    5.7254e-02    True
              | 4.3950e-03    3.3022e-02    5.5308e-03   3.1229e-02    1.4895e+00     -6.3368e-01   6.4246e-03    False
              | 4.3950e-03    7.6073e-03    9.3057e-04   5.1524e-03    1.4895e+00     1.4112e+00    1.2849e-02    True
              | 9.3057e-04    7.3552e-03    5.4597e-04   7.2723e-03    2.4271e-01     4.6412e+00    2.5699e-02    True
              | 5.4597e-04    1.3395e-02    2.0254e-03   1.2934e-02    2.3541e-02    -3.2065e+00    2.7271e-03    False
              | 5.4597e-04    1.9095e-03    4.7041e-05   1.6024e-03    2.3541e-02     1.6243e+00    5.4543e-03    True
              | 4.7041e-05    2.7742e-03    1.2499e-04   2.7398e-03    1.3032e-01    -2.2700e+00    6.5371e-04    False
              | 4.7041e-05    3.7390e-04    1.2059e-06   3.3688e-04    1.3032e-01     1.2384e+00    1.3074e-03    True
              |                                                                                                             CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD
blank         | 
              | 
              | 
              | 
text          | 1.2059e-06    6.5492e-04    2.1025e-06   6.5473e-04    1.0560e-02    -4.8064e+00    1.6316e-04    False
              | 1.2059e-06    8.2784e-05    4.2021e-08   8.1934e-05    1.0560e-02     1.3691e+00    3.2631e-04    True
meta          |                                                                                                             69
              | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                              70
blank         | 
              | 
              | 
title         | 3.2       Nonlinearly Constrained Optimization
text          | This section extends the unconstrained generalized trust region method introduced in Section 3.1.1
              | to handle nonlinear equality constraints. Consider the nonlinear equality-constrained optimization
              | problem
              |                                          minimize     F (µ)
              |                                           µ∈RNµ
              |                                                                                                   (3.41)
              |                                          subject to c(µ) = 0.
blank         | 
text          | This is the exact form of the reduced-space PDE-constrained optimization problem in (2.104) without
              | inequality constraints. The feasible set (Definition 3.4) is an important concept in constrained
              | optimization theory as it defines the set of all points that satisfy the constraints of (3.41).
blank         | 
text          | Definition 3.4 (Feasible set). The set of points that satisfy the constraints of the optimization
              | problem in (3.41)
              |                                       Ω := {µ ∈ RNµ | c(µ) = 0}                                   (3.42)
blank         | 
text          | is called the feasible set.
blank         | 
text          |    As in the unconstrained case, it is desirable to find the global minimum of (3.41), i.e., the point µ∗
              | such that F (µ∗ ) ≤ F (µ) for all µ ∈ Ω; however, due to the inherent difficulty of global optimization
              | we settle for local minima, as defined in Definition 3.5.
blank         | 
text          | Definition 3.5 (Constrained local minima). A point µ∗ is a local minima of (3.41) if µ∗ ∈ Ω and
              | there is a neighborhood N of µ∗ such that F (µ∗ ) ≤ F (µ) for all µ ∈ N ∩ Ω.
blank         | 
text          |    Before stating the first-order necessary optimality condition, two concepts must be introduced.
              | The first is the concept of constraint qualifications that provide conditions that must be satisfied for
              | the linearized feasible set to resemble the tangent cone [143]. In this work, we solely consider the
              | Linear Independence Constraint Qualifications (Definition 3.6) that requires linear independence of
              | the gradient of the constraints at a particular point.
blank         | 
text          | Definition 3.6 (Linear Independence Constraint Qualification (LICQ)). The LICQ holds at a point
              |                                                 ∂c
              | µ ∈ RNµ if the rows of the constraint Jacobian,    (µ) are linearly independent.
              |                                                 ∂µ
              |    The second concept is the Lagrangian (Definition 3.7) that combines the objective and constraints
              | into a single function by introducing auxiliary variables known as Lagrange multipliers.
blank         | 
text          | Definition 3.7 (Lagrangian). The Lagrangian corresponding to the optimization problem in (3.41)
              | is defined as
              |                                       L(µ, τ ) = F (µ) − τ T c(µ),                                (3.43)
blank         | 
text          | where τ ∈ RNc is a vector of Lagrange multipliers.
blank         | 
text          |    Equipped with these concepts, the first-order necessary optimality conditions for µ∗ to be a local
              | minima (in the sense of Definition 3.5) of (3.41) are stated in Theorem 3.3.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           71
blank         | 
              | 
              | 
text          | Theorem 3.3 (First-order constrained optimality condition). Suppose µ∗ is a local minima of
              | (3.41), that F and ci are continuously differentiable, and LICQ holds at µ∗. Then there is a
              | Lagrange multiplier vector λ∗ such that
blank         | 
text          |                                 ∇µ L(µ∗ , λ∗ ) = 0
              |                                         c(µ∗ ) = 0                                             (3.44)
              |                                     λ∗i ci (µ∗ ) = 0   for i ∈ {1, . . . , Nc }.
blank         | 
text          |    The first condition requires stationarity of the Lagrangian at a local minima and the second
              | requires the feasibility. The last condition is usually referred to as complementarity. There are also
              | second-order necessary and sufficient conditions for µ∗ to be a local minima of (3.41) that involve
              | the Hessian of the Lagrangian [143]. This will not be considered further as this work will primarily
              | be concerned with first-order optimality conditions.
blank         | 
              | 
title         | 3.2.1     Error-Aware Augmented Lagrangian Multifidelity Trust Region Method
text          | This section extends the multifidelity trust region framework introduced in Section 3.1.1 to handle
              | equality-constrained problems in (3.41). The proposed approach converts the constrained optimiza-
              | tion problem in (3.41) to a sequence of unconstrained problems using the the concept of the aug-
              | mented Lagrangian. The unconstrained multifidelity trust region method proposed in Section 3.1.1
              | is used to solve each unconstrained problem in the sequence for an efficient algorithm that leverages
              | inexpensive approximation models. The augmented Lagrangian corresponding to the optimization
              | problem in (3.41) is
blank         | 
text          |                 Lτ (µ, λ) := L(µ, λ) + τ c(µ)T c(µ) = F (µ) − λT c(µ) + τ c(µ)T c(µ),          (3.45)
blank         | 
text          | where τ > 0 is the penalty parameter. A standard result in constrained optimization theory states
              | that, under certain assumptions (Theorem 3.4), there exists a constant τ̄ such that a local minima
              | of (3.41) is a local minima of Lτ (µ, λ∗ ) for τ ≥ τ̄ , where λ∗ are the Lagrange multipliers at the
              | local minima.
blank         | 
text          | Theorem 3.4. Let µ∗ be a local minima of (3.41) with Lagrange multipliers λ∗ . If the LICQ holds
              | at µ∗ and the second-order sufficient-conditions hold at (µ∗ , λ∗ ) (Theorem 12.6 of [143]), there
              | exists τ̄ such that µ∗ is a local minima of Lτ (µ, λ∗ ) for all τ ≥ τ̄ .
blank         | 
text          | Proof. See Theorem 17.5 of [143].
blank         | 
text          |    Therefore, the optimization problem in (3.41) reduces to a sequence of unconstrained optimization
              | problems of the form
              |                                           minimize Lτp (µ, λ̂p )                               (3.46)
              |                                            µ∈RNµ
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           72
blank         | 
              | 
              | 
text          | for a sequence τp → τ > τ̄ , where λ̂p are Lagrange multiplier estimates, usually taken as
blank         | 
text          |                                        λ̂p = λ̂p−1 − τp−1 c(µ∗p−1 ),                            (3.47)
blank         | 
text          | where µ∗p−1 is the solution of (3.46) at iteration p − 1. The augmented Lagrangian is employed
              | instead of, e.g., a quadratic penalty function, as equivalence between (3.41) and (3.46) is guaranteed
              | for a finite value of the penalty parameter.
              |    The generalized multifidelity trust region method of Section 3.1.1 applies, without modification,
              | to each unconstrained optimization problem in (3.46), i.e., for a fixed τp . In this case, the approx-
              | imation model, mk (µ), and constraint functions, ϑk (µ) and ϕk (µ), must be constructed such that
              | the conditions in (3.12)-(3.15), applied to the augmented Lagrangian in (3.45) hold, that is,
blank         | 
text          |          |Lτp (µk , λ̂p ) − Lτp (µ, λ̂p ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)   µ ∈ Rk
blank         | 
text          |                             ∇µ Lτp (µk , λ̂p ) − ∇mk (µk ) ≤ ξϕk (µk )
              |                                                                                                 (3.48)
              |                                                      ϑk (µk ) ≤ κϑ ∆k
              |                                                     ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }.
blank         | 
text          | Furthermore, the model and constraint functions must satisfy assumptions (AM1)–(AM4). The trust
              | region subproblem in (3.4), based on this model and constraint, is solved using the interior-point
              | method outlined in Section 3.1.2.
              |    Global convergence of this equality-constrained variant of the trust region method of Section 3.1.1
              | follows trivially from the global convergence of the generalized trust region method (Appendix A) and
              | Theorem 3.4 provided assumptions (AF1)–(AF2) hold for the augmented Lagrangian. Assumption
              | (AF1) holds since the objective F and constraint c are assumed twice-continuously differentiable on
              | RNµ and assumption (AF2) holds since (3.45) is bounded below provided F is bounded below.
blank         | 
              | 
title         | 3.2.2    Numerical Experiment: Contrived
text          | This section closes with the application of Algorithm 1, embedded in the augmented Lagrangian
              | framework of Section 3.2.1, to solve the following optimization problem with a single nonlinear
              | equality constraint
              |                                       minimize
              |                                            2
              |                                                    µ1 + µ2
              |                                         µ∈R
              |                                                                                                 (3.49)
              |                                       subject to µ21 + µ22 − 2 = 0.
blank         | 
text          | The augmented Lagrangian corresponding to this problem, for a fixed penalty τ and Lagrange
              | multiplier estimate λ, is
blank         | 
text          |                       Lτ (µ, λ) = µ1 + µ2 + (µ21 + µ22 − 2) τ (µ21 + µ22 − 2) − λ
blank         |                                                                                  
text          |                                                                                                 (3.50)
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            73
blank         | 
              | 
              | 
              | 
text          |                                   1.5
blank         | 
              | 
              | 
text          |                                   1.0
blank         | 
              | 
              | 
text          |                                   0.5
blank         | 
              | 
              | 
text          |                                   0.0
blank         | 
              | 
              | 
              | 
text          |                             µ2
              |                                  −0.5
blank         | 
              | 
              | 
text          |                                  −1.0
blank         | 
              | 
              | 
text          |                                  −1.5
              |                                    −1.5   −1.0    −0.5   0.0   0.5   1.0   1.5
              |                                                          µ1
blank         | 
              | 
              | 
text          | Figure 3.7: Trajectory of Algorithm 1 as applied to the constrained problem (3.49). The contours
              | represent the true function F (µ), the red dots indicate trust region centers, and the blue line is the
              | trajectory of the trust region subproblem.
blank         | 
              | 
text          | and the resulting sequence of unconstrained optimization problems are
blank         | 
text          |                                             minimize
              |                                                  2
              |                                                      Lτj (µ, λj ),                              (3.51)
              |                                                  µ∈R
blank         | 
              | 
text          | which will be solved using Algorithm 1. The model mk (µ), objective decrease error indicator ϑk (µ),
              | and gradient error indicator ϕk (µ) are identical to those in Section 3.1.3 with F (µ) replaced with
              | Lτj (µ, λj ). The objective and gradient errors k and δk are similarly chosen using (3.40).
              |    With these definitions of mk (µ), ϑk (µ), and ϕk (µ) all ingredients necessary for the complete
              | description of Algorithm 1 are set. The trust region subproblem, for fixed τj and λj , is solved
              | using the BFGS interior-point method described in Section 3.1.2. The trajectory of the optimiza-
              | tion iterations—including the progress of the trust region centers and the trajectory of each trust
              | region subproblem—are shown in Figure 3.7. These iterations are aggregated over three augmented
              | Lagrangian iterations corresponding to τ0 = 10−4 , τ1 = 10−5 , and τ2 = 10−6 , with λj updated
              | according to (3.47) and initialized with λ0 = 0. Figure 3.8 provides additional insight to the Al-
              | gorithm 1 by showing (selected) individual iterations, including the trust region center, candidate
              | step, and feasible region for the trust region subproblem. Again, notice the substantial difference
              | between the shape of the trust regions in Figure 3.8 with traditional trust regions that are spheres
              | or ellipsoids. From both of these figures, it is clear the iterations converge to a feasible point, as
              | expected from the augmented Lagrangian framework.
              |    Figures 3.9 and 3.10 show the convergence history of the objective and gradient quantities,
              | respectively, aggregated over all three augmented Lagrangian iterations. A dashed vertical line
              | separates augmented Lagrangian iterations. From Figure 3.9 it can be seen that the objective
              | function Lτj (µ, λj ) continually decreases within an augmented Lagrangian iteration, despite the
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                                                                        74
blank         | 
              | 
              | 
text          |           1.5                                                 1.5                                                 1.5
blank         | 
              | 
              | 
text          |           1.0                                                 1.0                                                 1.0
blank         | 
              | 
              | 
text          |     µ2    0.5                                                 0.5                                                 0.5
blank         | 
              | 
              | 
text          |           0.0
blank         | 
              | 
              | 
              | 
text          |                                                         µ2
              |                                                               0.0
blank         | 
              | 
              | 
              | 
text          |                                                                                                             µ2
              |                                                                                                                   0.0
blank         | 
              | 
              | 
text          |          −0.5                                                −0.5                                                −0.5
blank         | 
              | 
              | 
text          |          −1.0                                                −1.0                                                −1.0
blank         | 
              | 
              | 
text          |          −1.5                                                −1.5                                                −1.5
              |            −1.5   −1.0   −0.5   0.0   0.5   1.0   1.5          −1.5   −1.0   −0.5   0.0   0.5   1.0   1.5          −1.5   −1.0   −0.5   0.0   0.5   1.0   1.5
              |                                 µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          |           1.5                                                 1.5                                                 1.5
blank         | 
              | 
              | 
text          |           1.0                                                 1.0                                                 1.0
blank         | 
              | 
              | 
text          |           0.5                                                 0.5                                                 0.5
              |     µ2
blank         | 
              | 
              | 
              | 
text          |           0.0
blank         | 
              | 
              | 
              | 
text          |                                                         µ2
              |                                                               0.0
blank         | 
              | 
              | 
              | 
text          |                                                                                                             µ2
              |                                                                                                                   0.0
blank         | 
              | 
              | 
text          |          −0.5                                                −0.5                                                −0.5
blank         | 
              | 
              | 
text          |          −1.0                                                −1.0                                                −1.0
blank         | 
              | 
              | 
text          |          −1.5                                                −1.5                                                −1.5
              |            −1.5   −1.0   −0.5   0.0   0.5   1.0   1.5          −1.5   −1.0   −0.5   0.0   0.5   1.0   1.5          −1.5   −1.0   −0.5   0.0   0.5   1.0   1.5
              |                                 µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          | Figure 3.8: Trajectory of Algorithm 1 as applied to the constrained problem (3.49) embedded in
              | the augmented Lagrangian framework. The contours represent the true function F (µ), the red dots
              | indicate trust region centers µk , the blue dots are the candidate for the next trust region center µ̂k ,
              | and the green region indicates the feasible set for the trust region subproblem.
blank         | 
              | 
text          | fact that the model values mk (µ) do not agree with Lτj (µ, λj ) at either the trust region centers
              | µk or candidates µ̂k when the algorithm is far from convergence. Figure 3.10 shows that the first-
              | order optimality condition decreases 3 − 4 orders of magnitude throughout these iterations. A more
              | detailed report of the convergence history is provided in Table 3.2.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                        75
blank         | 
              | 
              | 
              | 
text          |                  101
blank         | 
              | 
              | 
              | 
text          |                  100
blank         | 
text          |                        0      5        10        15        20       25        30
              |                                              iterations
blank         | 
text          | Figure 3.9: Convergence history of the augmented Lagrangian objective quantities using Algorithm 1:
              | Lτj (µk ) (    ), Lτj (µ̂k ) ( ), mk (µk ) (    ), mk (µ̂k ) (  ). The three augmented Lagrangian
              | iterations are separated by a vertical dashed line with the following penalty parameters: τ0 = 10−4
              | (iterations 0 − 9), τ1 = 10−5 (iterations 10 − 19), τ2 = 10−6 (iterations 20 − 29).
blank         | 
              | 
              | 
              | 
text          |                   102
blank         | 
text          |                   100
blank         | 
text          |                 10−2
blank         | 
text          |                 10−4
              |                         0      5        10        15       20        25        30
              |                                               iterations
blank         | 
text          | Figure 3.10: Convergence history of the augmented Lagrangian gradient quantities using Algo-
              | rithm 1: k∇Lτj (µk )k (     ), k∇Lτj (µ̂k )k (   ), k∇mk (µk )k (    ). The three augmented La-
              | grangian iterations are separated by a vertical dashed line with the following penalty parameters:
              | τ0 = 10−4 (iterations 0 − 9), τ1 = 10−5 (iterations 10 − 19), τ2 = 10−6 (iterations 20 − 29). For
              | each augmented Lagrangian iteration, the gradient of the true augmented Lagrangian (for fixed τj )
              | decreases 3 − 4 orders of magnitude.
              | Table 3.2: Convergence history of Algorithm 1 applied to the constrained problem (3.49). Iterations 0 − 9: τ0 = 10−4 , iterations 10 − 19:
              | τ1 = 10−5 , iterations 20 − 29: τ2 = 10−6 . The norm of the gradient of Lτj (µ), for fixed τj , decreases 3 − 4 orders of magnitude throughout
              | the iterations despite the values of Lτj (µk ) and mk (µk ) or Lτj (µ̂k ) and mk (µ̂k ) not being close until near convergence.
blank         | 
text          |                  Lτj (µk )      mk (µk )      Lτj (µ̂k )    mk (µ̂k )    k∇Lτj (µk )k        ρk            ∆k        Success?
              |                2.5312e+00     3.0312e+00    3.0781e+00     2.0781e+00    2.5125e+00      -5.7368e-01   2.5000e-01      False
              |                2.5312e+00     2.6562e+00    1.9946e+00     1.9971e+00    2.5125e+00      8.1405e-01    5.0000e-01      True
              |                1.9946e+00     2.2446e+00    1.6205e+00     2.0460e+00    2.5410e+00      1.8838e+00    1.0000e+00      True
              |                1.6205e+00     2.1205e+00    1.7166e+00     2.1042e+00    1.0140e+00     -5.9067e+00    9.6906e-02      False
              |                1.6205e+00     1.6689e+00    1.5149e+00     1.5999e+00    1.0140e+00      1.5304e+00    1.9381e-01      True
              |                1.5149e+00     1.6118e+00    1.5188e+00     1.6115e+00    3.1026e-01     -1.4365e+01    2.3193e-02      False
              |                1.5149e+00     1.5265e+00    1.5008e+00     1.5163e+00    3.1026e-01     1.3860e+00     4.6386e-02      True
              |                1.5008e+00     1.5240e+00    1.5013e+00     1.5239e+00    6.7233e-02     -6.9139e+00    5.6381e-03      False
              |                1.5008e+00     1.5036e+00    1.5000e+00     1.5031e+00    6.7233e-02     1.4075e+00     1.1276e-02      True
              |                1.5000e+00     1.5057e+00    1.5001e+00     1.5056e+00    1.2228e-02     -3.5785e+00    1.3905e-03      False
              |                5.1554e+01     5.2054e+01    2.5730e+01     2.4730e+01    1.9955e+02      9.4510e-01    2.0000e+00      True
              |                2.5730e+01     2.6730e+01    3.0107e+00     1.0107e+00    1.5760e+02      8.8336e-01    4.0000e+00      True
              |                3.0107e+00     5.0107e+00    3.4134e+00     -5.8657e-01   3.6765e+01      -7.1943e-02   1.0000e+00      False
              |                3.0107e+00     3.5107e+00    2.1365e+00     1.1365e+00    3.6765e+01      3.6822e-01    5.0000e-01      True
              |                2.1365e+00     2.3865e+00    2.0013e+00     2.2475e+00    1.4813e+01      9.7295e-01    1.0000e+00      True
              |                2.0013e+00     2.5013e+00    2.7229e+00     2.3393e+00    1.4232e+00     -4.4558e+00    9.5895e-02      False
              |                2.0013e+00     2.0492e+00    2.0018e+00     2.0454e+00    1.4232e+00      -1.4312e-01   1.0887e-02      False
              |                2.0013e+00     2.0067e+00    2.0000e+00     2.0054e+00    1.4232e+00      9.5556e-01    2.1775e-02      True
              |                2.0000e+00     2.0109e+00    2.0002e+00     2.0108e+00    3.4607e-02     -1.6948e+00    2.6365e-03      False
              |                2.0000e+00     2.0013e+00    2.0000e+00     2.0013e+00    3.4607e-02     1.6271e+00     5.2730e-03      True
              |                2.0000e+00     2.5000e+00    3.3861e+00     2.3862e+00     8.0931e-01    -1.2175e+01    2.5000e-01      False
              |                2.0000e+00     2.1250e+00    2.3588e+00     2.1088e+00     8.0931e-01    -2.2161e+01    6.2500e-02      False
              |                2.0000e+00     2.0313e+00    2.0165e+00     2.0300e+00     8.0931e-01    -1.3440e+01    3.3820e-03      False
              |                2.0000e+00     2.0017e+00    2.0000e+00     2.0017e+00     8.0931e-01     -3.8261e-02   4.2134e-04      False
              |                                                                                                                                                  CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD
blank         | 
              | 
              | 
              | 
text          |                2.0000e+00     2.0002e+00    2.0000e+00     2.0002e+00     8.0931e-01    1.0374e+00     8.4269e-04      True
              |                2.0000e+00     2.0004e+00    2.0000e+00     2.0004e+00     2.8971e-02     -8.1375e-01   1.0521e-04      False
              |                2.0000e+00     2.0001e+00    2.0000e+00     2.0001e+00     2.8971e-02      2.2565e-01   1.3148e-05      False
              |                2.0000e+00     2.0000e+00    2.0000e+00     2.0000e+00     2.8971e-02      9.5811e-01   2.6296e-05      True
              |                2.0000e+00     2.0000e+00    2.0000e+00     2.0000e+00     5.5167e-05      3.0124e-01   1.3148e-05      True
              |                2.0000e+00     2.0000e+00    2.0000e+00     2.0000e+00     8.5278e-05    6.9467e+00     2.6296e-05      True
meta          |                                                                                                                                                  76
title         | Chapter 4
blank         | 
title         | Projection-Based Model Reduction
blank         | 
text          | The trust region method introduced in the previous chapter is general in that any approximation
              | model equipped with error bounds (3.12) and (3.13) can be employed. Projection-based model
              | reduction has been shown to be a promising method to dramatically reduce the cost—in terms of
              | computational time and resources—of PDE simulations, while retaining a high degree of fidelity
              | [31, 198]. In this approach, the solution of the partial differential equation is sought in a well-
              | chosen, low-dimensional (possibly affine) trial subspace by solving a reduced representation of the
              | governing equations, usually a projection onto a test subspace. It has been shown to yield Reduced-
              | Order Models (ROMs) that are O(105 ) smaller (in terms of number of degrees of freedom) and faster
              | to solve than the original discretized PDE [31, 198], which will be called the High-Dimensional Model
              | (HDM). This makes reduced-order models a promising candidate for the trust region approximation
              | model from the previous chapter.
              |    This chapter provides background necessary to use projection-based reduced-order models as
              | the approximation model in the error-aware trust region method of Chapter 3. The discussion will
              | include the derivation of the primal, sensitivity, and adjoint reduced-order model and computable
              | error bounds. While some of this discussion is a review, there are novel contributions regarding
              | the formulation of a minimum-residual reduced-order model for sensitivity and adjoint equations
              | that guarantee the reduced quantities optimally approximate the HDM counterparts. When the
              | governing equations are nonlinear, a critical bottleneck exists in the evaluation of the ROM that will
              | destroy nearly all resource reduction potential. To eliminate this bottleneck, hyperreduction methods
              | [17, 175, 115, 41, 31, 59] have been developed that introduce an additional level of approximation.
              | Another contribution of this chapter is the extension of the minimum-residual formulation of the
              | primal, sensitivity, and adjoint reduced-order model to a specific hyperreduction method known as
              | collocation.
blank         | 
              | 
              | 
              | 
meta          |                                                  77
              | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          78
blank         | 
              | 
              | 
title         | 4.1      Global Reduced-Order Models
text          | For simplicity, this section will consider a static, deterministic partial differential equation at the
              | discrete level
              |                                              r(u, µ) = 0,                                          (4.1)
blank         | 
text          | where u ∈ RNu is the state vector, µ ∈ RNµ is the parameter vector, and r : RNu × RNµ → RNu
              | is the discrete PDE. Most of the developments will extend to the time-dependent case where r is
              | the governing equation and u is the state vector at a single time step. The fundamental ansatz of
              | (global) projection-based model reduction is that the state vector u can be well-approximated in a
              | single low-dimensional subspace
              |                                               u = Φur ,                                            (4.2)
blank         | 
text          | where Φ ∈ RNu ×ku is the reduced-order basis (basis for the trial subspace), ur ∈ Rku are the
              | reduced coordinates of u in the basis Φ, and ku  Nu . It is also common to consider an affine
              | expansion in (4.2); however, this generalization will not significantly contribute to the following
              | developments and is omitted for clarity. Subsequent sections will use this ansatz to arrive at the
              | primal, sensitivity, and adjoint form of the reduced-order model. A central focus will be the concept
              | of minimum-residual reduced-order models—defined such that its solution coincides with the first-
              | order optimality condition of residual minimization over the trial subspace in some norm—as they
              | possess desirable properties such as monotonicity and interpolation.
blank         | 
              | 
title         | 4.1.1     Primal Formulation
text          | The general form of the projection-based reduced-order model is obtained by substituting the ansatz
              | (4.2) into the governing equation (4.1) and projecting the resulting overdetermined nonlinear system
              | of equations onto a test subspace spanned by the columns of the basis Ψ ∈ RNu ×ku
blank         | 
text          |                                    rr (ur , µ) := ΨT r(Φur , µ) = 0,                               (4.3)
blank         | 
text          | where rr : Rku × RNµ → Rku is a nonlinear system of equations with ku equations and unknowns.
              | Define ur (µ; Φ, Ψ) implicitly as the solution of rr ( · , µ) = 0—the Implicit Function Theorem
              | (Theorem 2.1) guarantees the existence of such a function and its smoothness with respect to µ. In
              | the remainder, the notation ur (µ; Φ, Ψ) will be simplified to ur (µ) when there is no risk of confusion
              | regarding the choice of test and trial basis. The reduced coordinates must be reconstructed in the
              | full space according to Φur prior to the evaluation of a quantity of interest, which leads to the
              | definition of the reduced quantity of interest fr : Rku × RNµ → R as
blank         | 
text          |                                      fr (ur , µ; Φ) := f (Φur , µ).                                (4.4)
blank         | 
text          | The reduced quantity of interest becomes purely a function of µ when the implicit definition
              | ur (µ; Φ, Ψ) is used in the above equation, i.e., when the reduced QoI is only evaluated at solutions
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                             79
blank         | 
              | 
              | 
text          | of the reduced-order model (4.3),
blank         | 
text          |                                  Fr (µ; Φ, Ψ) := f (Φur (µ; Φ, Ψ), µ).                               (4.5)
blank         | 
text          | The implicit function Fr ( · ; Φ, Ψ) : RNµ → R is the reduced-order model approximation of the
              | quantity of interest F : RNµ → R, defined as F (µ) := f (u(µ), µ), where u(µ) is the solution of
              | r( · , µ) = 0. The notation for the reduced quantity of interest in (4.4) and (4.5) will be simplified
              | to fr (ur , µ) and Fr (µ), respectively, when there is no risk confusion.
              |    In the general case, the test basis may be non-constant, i.e., Ψ = Ψ(u, µ). Two common choices
              | for the test basis are
              |                                                            ∂r
              |                                Ψ=Φ            and     Ψ=      (Φur , µ)Φ,                            (4.6)
              |                                                            ∂u
              | which correspond to a Galerkin and Least-Squares Petrov-Galerkin [28, 31] projection, respectively.
              | At this point, there have been no restrictions placed on either the test or trial bases—aside from the
              | implicit requirement that they are valid bases, i.e., their columns are linearly independent—nor has
              | any relationship between these bases been specified. Next, the concept of minimum-residual reduced-
              | order models will be introduced that equips the reduced-order models with desirable properties:
              | (1) monotonicity—the quality of the solution can only improve (in some well-defined metric) as the
              | trial space is hierarchically refined and (2) interpolation—the reduced-order model will recover the
              | HDM solution if it lies in the trial space.
blank         | 
text          | Definition 4.1 (Minimum-Residual Property). A reduced-order model possesses the minimum-
              | residual property if the solution satisfies the first-order optimality conditions of the following residual
              | minimization problem
              |                                                     1             2
              |                                       minimize        kr(Φur , µ)kΘ                                  (4.7)
              |                                        ur ∈Rku      2
              | for some symmetric positive-definite Θ ∈ RNu ×Nu .
blank         | 
text          | Proposition 4.1. Let (Φ, Ψ, Θ) define a minimum-residual reduced-order model whose solution
              | coincides with the global minimum of (4.7). Then, the following properties hold for any µ ∈ RNµ :
blank         | 
text          |    • (Optimality) For any u ∈ col(Φ),
blank         | 
text          |                                    kr(Φur (µ; Φ, Ψ), µ)kΘ ≤ kr(u, µ)kΘ                               (4.8)
blank         | 
              | 
text          |    • (Monotonicity) Let (Φ0 , Ψ0 ) define a projection-based reduced-order model such that col(Φ0 ) ⊆
              |       col(Φ), then
              |                            kr(Φur (µ; Φ, Ψ), µ)kΘ ≤ r(Φ0 ur (µ; Φ0 , Ψ0 ), µ)       Θ
              |                                                                                                      (4.9)
blank         | 
text          |    • (Interpolation) If u(µ) ∈ col(Φ), then
blank         | 
text          |                          r(Φur (µ; Φ, Ψ), µ) = 0        and     u(µ) = Φur (µ; Φ, Ψ)                (4.10)
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                             80
blank         | 
              | 
              | 
text          | Proof. Optimality follows trivially from the fact that ur (µ; Φ, Ψ) is the global minima of the opti-
              | mization problem in (4.7). A simple application of the optimality property to u = Φ0 ur (µ; Φ0 , Ψ0 ) ∈
              | col(Φ0 ) ⊆ col(Φ) leads to monotonicity. Finally, if the exact solution of r( · , µ) = 0 is contained in
              | the columnspace of Φ, i.e., u(µ) ∈ col(Φ), the optimality property implies
blank         | 
text          |                             kr(Φur (µ; Φ, Ψ), µ)kΘ ≤ kr(u(µ), µ)kΘ = 0.                             (4.11)
blank         | 
text          | This result, along with the assumed uniqueness of solutions of r( · , µ) = 0 (Assumption 2.2), leads
              | to the interpolation property in (4.10).
blank         | 
text          | Remark. The results in Proposition 4.1 only hold if the solution of the minimum-residual reduced-
              | order model coincides with the global solution of the optimization problem in (4.7). In general this
              | is only guaranteed if the optimization problem is convex, which will be the case if the governing
              | equation is affine in its first argument, i.e., r(u, µ) = A(µ)u + b(µ), where A(µ) ∈ RNu ×Nu and
              | b(µ) ∈ RNu . When the optimization problem in (4.7) is non-convex, the stationary point that will be
              | found by the minimum-residual reduced-order model will not necessarily be the global minima of (4.7).
              | A heuristic that, in practice, is usually sufficient to lead to the results in Proposition 4.1 (optimality,
              | monotonicity, and interpolation) is to initialize the reduced-order model solver with a quality starting
              | point. Due to the required training phase in model reduction (Section 4.3), a reasonable starting point
              | can typically be obtained via interpolation of the training data [198].
blank         | 
text          |    From Appendix B, the approximation of the reduced quantity of interest is equipped with a
              | residual-based error bound (Lemma B.4) that takes the form
blank         | 
text          |                       |F (µ) − Fr (µ)| ≤ κ kr(Φur (µ), µ)k ≤ κ0 kr(Φur (µ), µ)kΘ                    (4.12)
blank         | 
text          | for some constants κ, κ0 > 0, where the second inequality follows from norm equivalence in finite di-
              | mensions. The residual-based error bound illuminates one motivation behind the minimum-residual
              | formulation: it minimizes the error bound over the columnspace of Φ.
              |    A general relationship between projection-based reduced-order models and minimum-residual
              | reduced-order models (Definition 4.1) is established by matching terms in the first-order optimality
              | condition of (4.7), i.e.,
              |                                                      T
              |                                       ∂r
              |                                          (Φur , µ)Φ        Θr(Φur , µ) = 0                          (4.13)
              |                                       ∂u
              | with the form of the projection-based reduced-order model in (4.3). From these two equations, the
              | relationship
              |                                                            ∂r
              |                                          Ψ(u, µ) = Θ          (u, µ)Φ                               (4.14)
              |                                                            ∂u
              | is sufficient for a general projection-based reduced-order model in (4.3) to possess the minimum-
              | residual property.
              |                                                              ∂r
              |    The LSPG reduced-order model in (4.6), i.e., Ψ(u, µ) =        (u, µ)Φ, satisfies the condition in
              |                                                              ∂u
              | (4.14) with Θ = I, where I is the Nu identity matrix, and therefore possesses the minimum-residual
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                         81
blank         | 
              | 
              | 
text          | property. For problems with symmetric positive-definite Jacobian matrices
blank         | 
text          |                                  ∂r
              |                                     (u, µ)  0     ∀u ∈ RNu , µ ∈ RNµ
              |                                  ∂u
blank         | 
text          | the Galerkin reduced-order model in (4.6), i.e., Ψ = Φ, possesses the minimum-residual property
              | in the metric defined by the Jacobian inverse transpose evaluated at the (reconstructed) solution of
              | the Galerkin reduced-order model (Φur (µ; Φ, Φ)), i.e.,
blank         | 
text          |                                           ∂r
              |                                     Θ=       (Φur (µ; Φ, Φ), µ)−T .                             (4.15)
              |                                           ∂u
blank         | 
text          | This is a valid metric since: (1) the Jacobian is evaluated at a specific state and (2) the Jacobian
              | is symmetric positive-definite at any state (by assumption) and therefore its inverse transpose is
              | symmetric positive-definite. This metric reduces the first-order optimality conditions (4.13) of the
              | residual minimization problem in (4.7) to: find y ∈ Rku such that
              |                                     T
              |                           ∂r            ∂r
              |                              (Φy, µ)Φ      (Φur (µ; Φ, Φ), µ)−T r(Φy, µ) = 0
              |                           ∂u            ∂u
blank         | 
text          | for a fixed µ ∈ RNµ . It is easily verified that the solution of the Galerkin reduced-order model
              | satisfies the above equation, i.e., with y = ur (µ; Φ, Φ). Therefore Galerkin reduced-order models
              | possess the minimum-residual property in the metric in (4.15) for problems with symmetric positive-
              | definite Jacobians.
blank         | 
text          | Remark. To this point, minimum-residual reduced-order models have been interpreted as a specific
              | projection of the governing equations
              |                                               r(u, µ) = 0.
blank         | 
text          | Given the existence and uniqueness assumption (Assumption 2.2) regarding solutions of the above
              | equation, it can equivalently be formulated as the solution of the minimum-residual optimization
              | problem
              |                                                    1          2
              |                                         minimize     kr(u, µ)kΘ ,                               (4.16)
              |                                          u∈RNu     2
              | where Θ is a symmetric, positive-definite matrix, with first-order optimality condition
blank         | 
text          |                                         ∂r
              |                                            (u, µ)T Θr(u, µ) = 0.                                (4.17)
              |                                         ∂u
blank         | 
text          | From this equation and (4.13), it is clear that minimum-residual reduced-order models are equiva-
              | lently derived as a Galerkin projection, with Φ as the test and trial basis, of the governing equations
              | in minimum-residual form (4.17).
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                      82
blank         | 
              | 
              | 
title         | 4.1.2        Exact and Minimum-Residual Sensitivity Formulation
text          | The intention of this work is to use the reduced-order models of the previous section in a gradient-
              | based, reduced-space PDE-constrained optimization setting, in an attempt to reduce the overall
              | computational cost. This requires a discussion regarding gradients of the reduced quantities of
              | interest. Both the sensitivity and adjoint approaches will be detailed for this purpose. Following
              | the procedure outlined in Section 2.3.3, the total derivative of (4.5) is expanded as
blank         | 
text          |                             ∂f                      ∂f                     ∂ur
              |       ∇Fr (µ; Φ, Ψ) =          (Φur (µ; Φ, Ψ), µ) +    (Φur (µ; Φ, Ψ), µ)Φ     (µ; Φ, Ψ).                    (4.18)
              |                             ∂µ                      ∂u                     ∂µ
blank         | 
text          |                          ∂ur
              | The reduced sensitivities    (µ; Φ, Ψ)1 are derived by considering the total variation of the
              |                           ∂µ
              | reduced-order model in (4.3) with respect to perturbations in µ. In the general case where Ψ
              | is state- and parameter-dependent, the reduced sensitivities are defined as the solution of the linear
              | equations
              |                                                                                
              |                    Nu     ∂ ΨT ej          ∂r    ∂u
              |                                                           Nu     ∂   Ψ T
              |                                                                          ej        ∂r
              |                                                    r
              |                   X                                      X
              |                       rj           Φ + ΨT    Φ     = −     rj              + ΨT                          (4.19)
              |                    j=1
              |                              ∂u            ∂u    ∂µ       j=1
              |                                                                      ∂µ            ∂µ
blank         | 
              | 
text          | where all terms are evaluated at the reconstructed primal solution, Φur (µ; Φ, Ψ) of the reduced-
              | order model (4.3). In the special case where the primal reduced-order model is exact, i.e.,
              | r(Φur (µ; Φ, Ψ), µ) = 0, or the test basis is constant, the expression in (4.19) reduces to
blank         |                                                  
text          |                                               ∂r    ∂ur       ∂r
              |                                            ΨT    Φ      = −ΨT    .                                           (4.20)
              |                                               ∂u    ∂µ        ∂µ
blank         | 
text          | A Galerkin projection employs a constant test basis Ψ = Φ and the equation for the reduced
              | sensitivity in (4.19) or (4.20) reduces to
blank         |                                                   
text          |                                              T ∂r    ∂ur       ∂r
              |                                             Φ     Φ      = −ΦT    .                                          (4.21)
              |                                                ∂u    ∂µ        ∂µ
blank         | 
text          |                                                            ∂r
              | A LSPG projection employs a non-constant test basis Ψ =       Φ and the derivatives of the test basis
              |                                                            ∂u
              | in (4.19) cannot be ignored in the general case where the primal solution is not exact. In this case,
              | the reduced sensitivities are the solution of the following equation
              |                                                                                  
              |          Nu          2              T                 Nu          2            T
              |         X          ∂   rj        ∂r   ∂r    ∂u r
              |                                                      X          ∂   rj      ∂r   ∂r
              |             rj ΦT        Φ + ΦT         Φ      = −     rj ΦT        + ΦT         .                       (4.22)
              |          j=1
              |                    ∂u∂u          ∂u   ∂u    ∂µ        j=1
              |                                                                 ∂u∂µ        ∂u   ∂µ
blank         | 
text          |    1 This                                                   ∂ur                          ∂ur
              |             notation is simplified from ∇Fr (µ; Φ, Ψ) and       (µ; Φ, Ψ) to ∇Fr (µ) and     (µ), respectively, when
              |                                                             ∂µ                           ∂µ
              | there is no risk of confusion.
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          83
blank         | 
              | 
              | 
text          | Despite the many advantages of minimum-residual reduced-order models (Proposition 4.1), a major
              | disadvantage is that they complicate sensitivity analysis due to the required partial derivatives of the
              | test basis in (4.19). This is particularly true in the case of LSPG where the first-order sensitivities,
              | ∂ur
              |      , require second-order information about the partial differential equation—information that
              |  ∂µ
              | is rarely available is large-scale PDE implementations. For this reason, the theory of minimum-
              | residual primal reduced-order models is extended to the sensitivity equations in an attempt to avoid
              | terms involving derivatives of the test basis, while generating reduced sensitivities that optimally
              | reconstruct the HDM sensitivities. The main drawback of this approach is the computed sensitivities
              | will not be consistent with the reduced-order model to which they correspond since they will not
              | coincide with the solution of (4.19).
              |    Before embarking on the discussion of minimum-residual sensitivity analysis, recall the definition
              | of the sensitivity residual
blank         | 
text          |                                                  ∂r          ∂r
              |                               r ∂ (u, w, µ) =       (u, µ) +    (u, µ)w,                         (4.23)
              |                                                  ∂µ          ∂u
blank         | 
text          | and the generalization of the gradient of the quantity of interest
blank         | 
text          |                                                  ∂f          ∂f
              |                                g ∂ (u, w, µ) =      (u, µ) +    (u, µ)w                          (4.24)
              |                                                  ∂µ          ∂u
blank         | 
text          | introduced in Section 2.3.3. With this notation, the gradient of the reduced quantity of interest
              | takes the form                                                  
              |                                                        ∂ur
              |                               ∇Fr (µ) = g ∂ Φur (µ), Φ     (µ), µ .                              (4.25)
              |                                                        ∂µ
              |                                                                      ∂ur
              | Instead of considering the reconstructed reduced sensitivity, Φ          , as an approximation for the
              |                                                                      ∂µ
              |                   ∂u
              | HDM sensitivity      , an approximation of the form
              |                   ∂µ
blank         | 
text          |                                             ∂u      ∂u
              |                                                     dr
              |                                                = Φ∂                                              (4.26)
              |                                             ∂µ      ∂µ
blank         | 
text          | will be considered where Φ∂ ∈ RNu ×ku is a reduced-order basis (linearly independent columns) for
              |                       ∂u
              |                       dr
              | the sensitivities and    ∈ Rku ×Nµ are the reduced coordinates. The reduced coordinates will be
              |                       ∂µ
              | defined as the argument that minimizes the sensitivity residual
blank         | 
text          |                      ∂u
              |                      dr                               1 ∂                      2
              |                         (µ; Φ∂ , Θ∂ , u) = arg min      r (u, Φ∂ wr , µ)            ,            (4.27)
              |                      ∂µ                   wr ∈Rku ×Nµ 2                        Θ∂
blank         | 
              | 
text          | where u ∈ RNu is any linearization point, usually the reconstructed primal solution, i.e., u =
              | Φur (µ; Φ, Ψ) and Θ∂  0 is the metric defining the norm. The first-order optimality condition of
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                                  84
blank         | 
              | 
              | 
text          | the linear least-squares problem in (4.27) leads to the normal equations
              |                          T                                    d                   T
              |            ∂r                                 ∂r                  ∂ur      ∂r                ∂r
              |               (u, µ)Φ∂           Θ   ∂
              |                                                  (u, µ)Φ∂             =−      (u, µ)Φ∂
              |                                                                                           Θ∂    (u, µ).                   (4.28)
              |            ∂u                                 ∂u                  ∂µ       ∂u                ∂µ
blank         | 
text          | The following proposition parallels Proposition 4.1 for the minimum-residual sensitivity approxima-
              |                                                                           ∂u
              |                                                                           dr
              | tion and provides conditions that result in the reduced sensitivities, Φ∂    , exactly reconstructing
              |                                                                           ∂µ
              |                        ∂u
              | the HDM sensitivities,    .
              |                        ∂µ
              | Proposition 4.2. Let (Φ∂ , Θ∂ ) define a minimum-residual sensitivity reduced-order model. Then,
              | the following properties hold for any µ ∈ RNµ :
blank         | 
text          |    • (Optimality) For any u ∈ RNu and w ∈ col(Φ∂ )
              |                                                              !
              |                                       ∂u
              |                                       dr
              |                                               ∂∂  ∂
              |                            rk∂   u, Φ    (µ; Φ , Θ , u)ek , µ                               ≤ rk∂ (u, w, µ)   Θ∂
              |                                                                                                                           (4.29)
              |                                       ∂µ
              |                                                                                        Θ∂
blank         | 
              | 
text          |      for k = 1, . . . , Nµ , where rk∂ (u, w · ek , µ) := r ∂ (u, weTk , µ)ek and ek is the kth canonical
              |      unit vector.
              |                                           0         0
              |    • (Monotonicity) Let (Φ∂ , Θ∂ ) define a minimum-residual sensitivity reduced-order model such
              |                   0
              |      that col(Φ∂ ) ⊆ col(Φ∂ ), then
blank         | 
text          |                                                    rk∂ (u, w, µ)   Θ∂
              |                                                                         ≤ rk∂ (u, w0 , µ)       Θ∂
              |                                                                                                      ,                    (4.30)
blank         | 
text          |                           ∂u
              |                           dr                                0 ∂u
              |                                                               dr       0    0
              |      where w = Φ∂            (µ; Φ∂ , Θ∂ , u)ek and w0 = Φ∂      (µ; Φ∂ , Θ∂ , u)ek , for any u ∈ RNu .
              |                           ∂µ                                  ∂µ
              |                              ∂u
              |    • (Interpolation) If          (µ) ∈ col(Φ∂ ) for k ∈ {1, . . . , Nµ }, then
              |                              ∂µk
              |                                                                                                   !
              |                                                            ∂u
              |                                                            dr
              |                                                             ∂
              |                                          rk∂       u(µ), Φ    (µ; Φ∂ , Θ∂ , u(µ))ek , µ                  =0
              |                                                            ∂µ
              |                                                                                                                           (4.31)
              |                                                         ∂u           ∂u
              |                                                                      dr
              |                                                             (µ) = Φ∂    (µ; Φ∂ , Θ∂ , u(µ))ek .
              |                                                         ∂µk          ∂µ
blank         | 
              | 
text          |                                                        ∂u
              |                                                        dr
              | Proof. Optimality follows trivially from the fact that Φ∂  (µ; Φ∂ , Θ∂ , u) is the (unique) minima
              |                                                         ∂µ
              | of the optimization problem in (4.27). Monotonicity follows directly from the optimality property
meta          |                       0
text          | since w0 ∈ col(Φ∂ ) ⊆ col(Φ∂ ). Finally, if the exact solution of rk∂ (u(µ), · , µ) = 0 is contained in
              |                               ∂u
              | the columnspace of Φ∂ , i.e.,     (µ) ∈ col(Φ∂ ), the optimality property implies
              |                               ∂µk
              |                                                                    !                                   
              |                    ∂u
              |                    dr
              |                     ∂                                                                         ∂u
              |     rk∂    u(µ), Φ    (µ; Φ∂ , Θ∂ , u(µ))ek , µ                              ≤   rk∂    u(µ),     (µ), µ           = 0.   (4.32)
              |                    ∂µ                                                                         ∂µk             Θ∂
              |                                                                         Θ∂
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                        85
blank         | 
              | 
              | 
text          | The interpolation property in (4.31) follows from this and the fact that for a given u ∈ RNu and
              | µ ∈ RNµ , the solution of rk∂ (u, · , µ) = 0 is unique (due to invertibility of the Jacobian and linear
              | independence of the columns of Φ∂ ).
blank         | 
text          |    The proposed minimum-residual sensitivity approximation is used to reconstruct an approxima-
              | tion of the gradient of the quantity of interest as
              |                                                                                          !
              |                                                ∂
              |                         dr (µ; Φ, Ψ, Φ , Θ ) := g      ∂    ∂        ∂u
              |                                                                      dr
              |                                                                        ∂     ∂   ∂
              |                ∇F (µ) ≈ ∇F                                      u, Φ    (µ; Φ , Θ , u), µ ,                     (4.33)
              |                                                                      ∂µ
blank         | 
text          | where u = Φur (µ; Φ, Ψ) is the reconstructed primal solution2 . From Appendix B, the approxima-
              | tion of the gradient of the reduced QoI is equipped with a residual-based error bound (Lemma B.4)
              | that takes the form
              |                                                                                                         !
              |           dr (µ; Φ, Ψ, Φ∂ , Θ∂ ) ≤ κ kr(u, µ)k + τ r ∂                           ∂u
              |                                                                                  dr                
              |  ∇F (µ) − ∇F                                                               u, Φ∂      µ; Φ∂ , Θ∂ , u , µ
              |                                                                                  ∂µ
              |                                                                                                           !
              |                                                    0               0       ∂       ∂∂u
              |                                                                                     dr 
              |                                                                                              ∂   ∂
blank         |                                                                                                      
text          |                                             ≤ κ kr(u, µ)kΘ + τ         r       u, Φ      µ; Φ , Θ , u , µ
              |                                                                                     ∂µ
              |                                                                                                              Θ∂
              |                                                                                                         (4.34)
              | for some constants κ, κ0 , τ, τ 0 > 0, where u = Φur (µ; Φ, Ψ) and the second inequality follows from
              | norm equivalence in finite dimensions. The residual-based error bound illuminates one motivation
              | behind the minimum-residual primal and sensitivity formulations: the minimum-residual primal
              | reduced-order model minimizes the first term in (4.34) over the columnspace of Φ and the minimum-
              | residual sensitivity reduced-order model minimizes the second term over the columnspace of Φ∂ .
              |                                                                                               ∂u
              |     To this point, two different approximations of the high-dimensional model sensitivities      (µ)
              |                                                                                               ∂µ
              |                           ∂ur          ∂u
              |                                        dr
              | have been introduced: Φ        and Φ∂     . Each leads to a different approximation of the gradient
              |                            ∂µ          ∂µ
              | of the quantity of interest: ∇Fr (µ) and ∇F
              |                                           dr (µ). Proposition 4.3 states sufficient conditions under
              | which these two approximations are equal. Specifically, it requires the test basis for the primal ROM
              | (Ψ), the sensitivity optimality metric (Θ∂ ), and sensitivity basis (Φ∂ ) be related according to (4.35).
              |                                                                      ∂ur      ∂u
              |                                                                                dr
              | Furthermore, these conditions also imply the reduced coordinates          and      themselves are equal.
              |                                                                      ∂µ        ∂µ
              | Proposition 4.3 is significant since it provides conditions under which the easily computed minimum-
              | residual sensitivities (since they do not require second derivatives of r) reduce to the desired reduced-
              | order model sensitivities (since they guarantee consistency of the gradients of reduced quantities of
              | interest).
blank         | 
text          | Proposition 4.3. Consider a primal reduced-order model defined by trial and test bases Φ and Ψ,
              | respectively, and a minimum-residual sensitivity reduced-order model defined by basis Φ∂ and metric
              | Θ∂ . Suppose that either: (1) the primal solution of the reduced-order model exactly reconstructs the
              |   2 The   notation ∇F                              dr (µ; Φ, Ψ, Φ∂ , Θ∂ ) when there is no risk of confusion.
              |                    dr (µ) will be used in place of ∇F
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        86
blank         | 
              | 
              | 
text          | HDM solution, i.e.,
              |                                            u(µ) = Φur (µ; Φ, Ψ)
blank         | 
text          | or (2) the test basis Ψ is constant. Then, for any u ∈ RNu , the relationships
blank         | 
text          |                                               Φ∂ = Φ
              |                                                         ∂r                                     (4.35)
              |                                          Ψ(u, µ) = Θ∂      (u, µ)Φ∂
              |                                                         ∂u
blank         | 
text          | guarantee the sensitivity of the primal reduced-order model (Φ, Ψ) coincides with the solution of the
              | minimum-residual sensitivity reduced-order model (Φ∂ , Θ∂ ) and the corresponding gradient approx-
              | imations match
              |                          ∂ur             ∂u
              |                                          dr
              |                              (µ; Φ, Ψ) =    (µ; Φ∂ , Θ∂ , Φur (µ; Φ, Ψ))
              |                          ∂µ              ∂µ                                                    (4.36)
              |                                          dr (µ; Φ, Ψ, Φ∂ , Θ∂ ).
              |                          ∇Fr (µ; Φ, Ψ) = ∇F
blank         | 
text          | Proof. Let Φur = Φur (µ; Φ, Ψ) denote the reconstructed primal solution of the projection-based
              | reduced-order model. If either the primal solution is exact or the test basis is constant, the general
              | form of the reduced-order model sensitivity equations in (4.19) reduce to the equations in (4.20),
              | where all terms are evaluated at the primal solution, i.e.,
blank         |                                           
text          |                               ∂r             ∂ur                ∂r
              |                   Ψ(Φur , µ)T    (Φur , µ)Φ      = −Ψ(Φur , µ)T    (Φur , µ).                  (4.37)
              |                               ∂u             ∂µ                 ∂µ
blank         | 
text          | Conversely, the normal form of the minimum-residual sensitivity reduced-order model in (4.28)
              | reduces to                                 d
              |                               ∂r             ∂ur                ∂r
              |                   Ψ(Φur , µ)T    (Φur , µ)Φ      = −Ψ(Φur , µ)T    (Φur , µ),                  (4.38)
              |                               ∂u             ∂µ                 ∂µ
              | when the relationships in (4.35) are enforced. Thus, under conditions (4.35), the governing equations
              |     ∂ur       ∂u
              |                dr
              | for      and       are identical and the (unique) solutions must be equal, which establishes the first
              |     ∂µ         ∂µ
              | result in (4.36). The second result in (4.36) follows from the simple relation
blank         |                                                                     
text          |                            ∂                        ∂ur
              |        ∇Fr (µ; Φ, Ψ) = g           Φur (µ; Φ, Ψ), Φ     (µ; Φ, Ψ), µ
              |                                                     ∂µ
              |                                                                                          !
              |                                                      ∂u
              |                                                      dr
              |                                                                                                (4.39)
              |                        = g∂        Φur (µ; Φ, Ψ), Φ∂     (µ; Φ∂ , Θ∂ , Φur (µ; Φ, Ψ)), µ
              |                                                      ∂µ
              |                          dr (µ; Φ, Ψ, Φ∂ , Θ∂ ),
              |                        = ∇F
blank         | 
text          | where the first and last equality use the definition of g ∂ and the second equality uses the identity
              | between the true and minimum-residual reduced sensitivities established in the first part.
blank         | 
text          |    To close this section, the specific form of the minimum-residual sensitivity equations in (4.19)
              | are discussed for the special cases of Galerkin and LSPG projections (4.6). For problems with
              | symmetric positive-definite Jacobians, the Galerkin sensitivity equations in (4.21) exactly match the
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           87
blank         | 
              | 
              | 
text          | minimum-residual sensitivity equations with Ψ = Φ = Φ∂ and sensitivity metric
blank         | 
text          |                                            ∂r
              |                                     Θ∂ =      (Φur (µ; Φ, Φ))−T .                                 (4.40)
              |                                            ∂u
blank         | 
text          | Additionally, these choices satisfy (4.35), which further supports the claim. Thus, for such problems,
              | the true Galerkin sensitivities possess the minimum residual property (and therefore optimality,
              | monotonicity, and interpolation as defined in Proposition 4.2) and are easy to compute since they
              |                                                                                      ∂u
              | do not rely on second derivatives of r. For the case of a LSPG projection (Ψ(u, µ) =    (u, µ)Φ),
              |                                                                                      ∂µ
              | the choices Φ∂ = Φ and Θ∂ = I reduce the minimum-residual sensitivity equations to
blank         | 
text          |                                         ∂r T ∂r ∂u
              |                                                  dr      ∂r T ∂r
              |                                    ΦT          Φ    = ΦT                                          (4.41)
              |                                         ∂u ∂u ∂µ         ∂u ∂µ
blank         | 
text          | where all nonlinear terms are evaluated at the reconstructed primal solution Φur (µ; Φ, Ψ). The
              | above equation is identical to the LSPG sensitivity equations when the primal solution is exact and
              | thus the true and minimum-residual sensitivities agree. This is reaffirmed since the choices satisfy
              | (4.35).
blank         | 
              | 
title         | 4.1.3     Exact and Minimum-Residual Adjoint Formulation
text          | For optimization problems that involve more optimization variables than constraints, it is desirable
              | to employ the adjoint method to compute gradients of quantities of interest. The derivation of
              | the adjoint equations for the reduced-order model can apply any of the three procedures outline in
              | Section 2.3.4 to the governing equation in (4.3), i.e., rr (ur , µ) = 0, and reduced quantity of interest
              | in (4.4), i.e., fr (Φur , µ). For brevity, only the optimization approach is detailed. Consider the
              | auxiliary optimization problem
blank         | 
text          |                                     minimize     f (Φur , µ̂)
              |                                      ur ∈Rku
              |                                                                                                   (4.42)
              |                                     subject to ΨT r(Φur , µ̂) = 0
blank         | 
text          | for a fixed µ̂ and the corresponding Lagrangian
blank         | 
text          |                             Lr (ur , λr ) = f (Φur , µ̂) − λTr ΨT r(Φur , µ̂).                    (4.43)
blank         | 
text          | By comparing this expression for the Lagrangian with that in (2.98), it is clear that the HDM
              | Lagrange multipliers are reconstructed from the reduced Lagrange multipliers as
blank         | 
text          |                                                λ = Ψλr .                                          (4.44)
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                             88
blank         | 
              | 
              | 
text          | The stationarity of the Lagrangian with respect to ur leads to the reduced adjoint equations
blank         | 
text          |                                                      T
              |                            XNu     ∂ ΨT ej          ∂r           ∂f T
              |                                rj           Φ + ΨT    Φ λ r = ΦT                                (4.45)
              |                             j=1
              |                                       ∂u            ∂u            ∂u
blank         | 
              | 
text          | where all terms are evaluated at the reconstructed primal solution, Φur (µ; Φ, Ψ).
              |    For any µ ∈ RNµ and bases Φ, Ψ, the solution of the above equation is denoted λr (µ; Φ, Ψ).
              | The gradient of the quantity of interest is then reconstructed as
              |                                                                          
              |            ∂f                                     XNu     ∂ ΨT ej        ∂r
              |  ∇Fr (µ) =    (Φur (µ; Φ, Ψ), µ) − λr (µ; Φ, Ψ)T      rj           + ΨT                               .
              |            ∂µ                                      j=1
              |                                                              ∂µ          ∂µ
              |                                                                                    (Φur (µ; Φ, Ψ), µ)
              |                                                                                                  (4.46)
              | In the special case where the primal reduced-order model is exact, i.e., r(Φur (µ; Φ, Ψ), µ) = 0, or
              | the test basis is constant, the expression in (4.45) reduces to
              |                                               T
              |                                                         ∂f T
blank         |                                      
text          |                                         T ∂r
              |                                       Ψ      Φ λ r = ΦT      ,                                   (4.47)
              |                                           ∂u            ∂u
blank         | 
text          | and the gradient of the QoI becomes
blank         | 
text          |                      ∂f                                       ∂r
              |          ∇Fr (µ) =      (Φur (µ; Φ, Ψ), µ) − λr (µ; Φ, Ψ)T ΨT    (Φur (µ; Φ, Ψ), µ).             (4.48)
              |                      ∂µ                                       ∂µ
blank         | 
text          | In the special case where the primal reduced-order model employs a Galerkin projection (Ψ = Φ),
              | the test basis is state- and parameter-independent and the adjoint equations in (4.45) or (4.47)
              | become                                       T
              |                                                       ∂f T
blank         |                                      
text          |                                          ∂r
              |                                       ΦT    Φ λr = ΦT                                            (4.49)
              |                                          ∂u           ∂u
              | and the gradient of the QoI is
blank         | 
text          |                      ∂f                                       ∂r
              |          ∇Fr (µ) =      (Φur (µ; Φ, Φ), µ) − λr (µ; Φ, Φ)T ΦT    (Φur (µ; Φ, Φ), µ).             (4.50)
              |                      ∂µ                                       ∂µ
blank         | 
text          | In the special case of a LSPG projection, the adjoint equations in (4.45) become
              |                                                        T
              |                            Nu                      T
              |                           X           2
              |                                      ∂ rj        ∂r ∂r           ∂f T
              |                               rj ΦT      Φ + ΦT       Φ λ r = ΦT                                (4.51)
              |                            j=1
              |                                      ∂u∂u        ∂u ∂u            ∂u
blank         | 
text          | and the QoI gradient is
              |                                                                            
              |                                                 Nu         2           T
              |           ∂f                                   X          ∂ rj      ∂r ∂r 
              | ∇Fr (µ) =    (Φur (µ; Φ, Ψ), µ)−λr (µ; Φ, Ψ)T      rj ΦT      + ΦT                                         .
              |           ∂µ                                    j=1
              |                                                           ∂u∂µ      ∂u   ∂µ
              |                                                                                      (Φur (µ; Φ, Ψ), µ)
              |                                                                                                  (4.52)
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                     89
blank         | 
              | 
              | 
text          |    For cases where the test basis is non-constant, the adjoint equations and QoI gradient are diffi-
              | cult to compute due to the presence of derivatives of the test basis, which usually involves second
              | derivatives of the discrete PDE. These terms are rarely available in large-scale PDE implementations
              | and expensive to compute when available. Furthermore, the adjoint equations for the reduced-order
              | model are not developed such that the HDM adjoint variable will be optimally reconstructed and,
              | therefore, the gradient of the reduced QoI may not be a good approximation of the gradient of
              | the true QoI. For these reasons, minimum-residual adjoint equations will be formulated such that
              | the reconstructed reduced adjoint variable minimizes the HDM adjoint residual in some norm. For
              | generality, approximate the HDM adjoint variable in a reduced-order basis Φλ ∈ RNu ×ku , i.e.,
blank         | 
text          |                                                        λ = Φλ λ̂r .                                          (4.53)
blank         | 
text          | In general, the adjoint basis may depend on the primal solution and parameter, i.e., Φλ (u, µ). The
              | reduced coordinates λ̂r ∈ Rku are defined as the solution of the linear residual minimization problem
              | (linear least-squares)
              |                                                   1 λ                         2
              |                                     minimize        r (Φur , Φλ λ̂r , µ)           .                         (4.54)
              |                                                   2                           Θλ
blank         | 
text          | where u ∈ RNu is any linearization point, usually the primal ROM solution. Expanding (4.54) with
              | the definition of r λ in (2.101) leads to the following definition of λ̂r
blank         | 
text          |                                                                                                     2
              |                                                        1   ∂f           ∂r
              |                λ̂r (µ; Φλ , Θλ , u) = arg min            −    (u, µ)T +    (u, µ)T Φλ zr                 ,   (4.55)
              |                                              zr ∈Rku   2   ∂u           ∂u                          Θλ
blank         | 
              | 
text          | for any u ∈ RNu . The definition of λ̂r in (4.55) is equivalent to the solution of the normal equations
              |                                     !T                     !                      !T
              |                          ∂r T λ              λ   ∂r T λ                ∂r T λ               ∂f T
              |                              Φ           Θ           Φ         λ̂r =       Φ           Θλ                    (4.56)
              |                          ∂u                      ∂u                    ∂u                   ∂u
blank         | 
text          | where the dependence on the linearization point (u) and parameter (µ) have been dropped.
              |    As with the primal and sensitivity minimum-residual reduced-order models, the minimum-
              | residual adjoint reduced-order models are guaranteed to be monotonic and interpolatory as defined
              | in Proposition 4.4. This result is relevant since it provides conditions under which the minimum-
              | residual adjoint reduced-order model solution monotonically approaches the HDM adjoint solution
              | and the requirement for these solutions to exactly match.
blank         | 
text          | Proposition 4.4. Let (Φλ , Θλ ) define a minimum-residual adjoint reduced-order model. Then the
              | following properties hold for any µ ∈ RNµ
blank         | 
text          |    • (Optimality) For any u ∈ RNu and z ∈ col(Φλ )
blank         |                                                          
text          |                          r λ u, Φλ λ̂r (µ; Φλ , Θλ , u), µ                  ≤ r λ (u, z, µ)        Θλ
              |                                                                                                              (4.57)
              |                                                                        Θλ
blank         | 
              | 
text          |                                 0        0
              |    • (Monotonicity) Let (Φλ , Θλ ) define a minimum-residual adjoint reduced-order model such
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                               90
blank         | 
              | 
              | 
text          |                    0
              |         that col(Φλ ) ⊆ col(Φλ ), then
blank         | 
text          |                                     r λ (u, z, µ)   Θλ
              |                                                          ≤ r λ (u, z 0 , µ)       Θλ
              |                                                                                        ,               (4.58)
blank         | 
text          |                                                           0          0        0
              |         where z = Φλ λ̂r (µ; Φλ , Θλ , u) and z 0 = Φλ λ̂r (µ; Φλ , Θλ , u), for any u ∈ RNµ .
blank         | 
text          |       • (Interpolatory) If λ(µ) ∈ col(Φλ ), then
blank         |                                                                        
text          |                                  r λ u(µ), Φλ λ̂r (µ; Φλ , Θλ , u(µ)), µ = 0
              |                                                                                                        (4.59)
              |                                               λ(µ) = Φλ λ̂r (µ; Φλ , Θλ , u(µ)).
blank         | 
              | 
text          | Proof. Optimality follows trivially from the fact that Φλ λ̂(µ; Φλ , Θλ , u) is the (unique) minima
              | of the optimization problem in (4.54). Monotonicity follows directly from the optimality property
              |                        0
              | since z 0 ∈ col(Φλ ) ⊆ col(Φλ ). Finally, if the exact solution of r λ (u(µ), · , µ) = 0 is contained in
              | the columnspace of Φλ , i.e., λ(µ) ∈ col(Φλ ), the optimality property implies
blank         |                                                  
text          |            r λ u(µ), Φλ λ̂r (µ; Φλ , Θλ , u(µ)), µ            ≤ r λ (u(µ), λ(µ), µ)        Θλ
              |                                                                                                 = 0.   (4.60)
              |                                                          Θλ
blank         | 
              | 
text          | The interpolation property in (4.59) follows from this and the fact that for a given u ∈ RNu and
              | µ ∈ RNµ , the solution of r λ (u, · , µ) = 0 is unique (due to invertibility of the Jacobian and linear
              | independence of the columns of Φλ ).
blank         | 
text          |       Since λ̂r is chosen to optimally reconstruct the HDM adjoint variable λ in the sense of the
              |   λ
              | Θ -norm of the adjoint residual, the gradient of the QoI will be computed as
blank         |                                                                           
text          |                 dr (µ; Φ, Ψ, Φλ , Θλ ) := g λ u, Φλ λ̂r (µ; Φλ , Θλ , u), µ
              |        ∇F (µ) ≈ ∇F
              |                                               ∂f                                   T ∂r
              |                                                                                                        (4.61)
              |                                           =      (u, µ) − λ̂r (µ; Φλ , Θλ , u)T Φλ      (u, µ).
              |                                               ∂µ                                     ∂µ
blank         | 
text          | where u = Φur (µ; Φ, Ψ). From Appendix B, the approximation of the gradient of the reduced
              | quantity of interest is equipped with a residual-based error bound (Lemma B.4) that takes the form
blank         | 
              | 
              |                                                                                   
text          |            dr (µ; Φ, Ψ, Φλ , Θλ ) ≤ κ kr(u, µ)k + τ r λ u, Φλ λ̂r µ; Φλ , Θλ , u , µ
              |   ∇F (µ) − ∇F
blank         |                                                                                         
text          |                                   ≤ κ0 kr(u, µ)kΘ + τ 0 r λ u, Φλ λ̂r (µ; Φλ , Θλ , u), µ
              |                                                                                                          Θλ
              |                                                                                                        (4.62)
              | for some constants κ, κ0 , τ, τ 0 > 0, where u = Φur (µ; Φ, Ψ) and the second inequality follows
              | from norm equivalence in finite dimensions. The residual-based error bound illuminates one moti-
              | vation behind the minimum-residual primal and adjoint formulations: the minimum-residual primal
              | reduced-order model minimizes the first term in (4.62) over the columnspace of Φ and the minimum-
              | residual adjoint reduced-order model minimizes the second term over the columnspace of Φλ .
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          91
blank         | 
              | 
              | 
text          |      In an exact parallel with the previous section, Proposition 4.5 provides conditions under which the
              | two aforementioned approximations of the HDM gradient ∇F (µ), i.e., the approximation based on
              | the reduced-order model adjoint ∇Fr (µ) and that based on the minimum-residual adjoint reduced-
              | order model ∇F
              |              dr (µ), exactly match. The main condition is the requirement (4.63) on the relation-
              | ship between the trial basis (Φ), adjoint basis (Φλ ), and adjoint optimality metric (Θλ ). These
              | conditions also ensure the reduced coordinates λr and λ̂r match. These results are relevant as
              | they provide conditions under which the easily computed minimum-residual adjoint solutions (since
              | the computation does not require second-order derivatives of r) reduce to the desired reduced-order
              | model sensitivities (that guarantee consistency of the gradients of the reduced quantities of interest).
blank         | 
text          | Proposition 4.5. Consider a primal reduced-order model defined by trial and test bases Φ and Ψ,
              | respectively, and a minimum-residual adjoint reduced-order model defined by basis Φλ and metric
              | Θλ . Suppose that either: (1) the primal solution of the reduced-order model exactly reconstructs the
              | HDM solution, i.e.,
              |                                         u(µ) = Φur (µ; Φ, Ψ).
blank         | 
text          | or (2) the test basis Ψ is constant. Then, for any u ∈ RNu , the relationships
              |                                                                 −1
              |                                                       ∂r
              |                              Φλ (u, µ) = Ψ(u, µ) = Θλ    (u, µ)T     Φ                           (4.63)
              |                                                       ∂u
blank         | 
text          | guarantee the adjoint solution of the primal reduced-order model (Φ, Ψ) matches the minimum-
              | residual adjoint reduced-order model (Φλ , Θλ )
blank         | 
text          |                              λr (µ; Φ, Ψ) = λ̂r (µ; Φλ , Θλ , Φur (µ; Φ, Ψ))
              |                                                                                                  (4.64)
              |                                            dr (µ; Φ, Ψ, Φλ , Θλ )
              |                            ∇Fr (µ; Φ, Ψ) = ∇F
blank         | 
text          | Proof. Let Φur = Φur (µ; Φ, Ψ) denote the reconstructed primal solution of the projection-based
              | reduced-order model. If either the primal solution is exact or the test basis is constant, the general
              | form of the reduced-order model adjoint equations in (4.45) reduce to the equations in (4.47), where
              | all terms are evaluated at the primal solution, i.e.,
              |                                                  T
              |                                      ∂r                    ∂f
              |                          Ψ(Φur , µ)T    (Φur , µ)Φ λr = ΦT    (Φur , µ)T .                       (4.65)
              |                                      ∂u                    ∂u
blank         | 
text          | Conversely, the normal form of the minimum-residual adjoint reduced-order model in (4.56) reduces
              | to                                               T
              |                                      ∂r                     ∂f
              |                          Ψ(Φur , µ)T    (Φur , µ)Φ λ̂r = ΦT    (Φur , µ)T                        (4.66)
              |                                      ∂u                     ∂u
              | when the relationships in (4.63) are enforced. Thus, under the aforementioned conditions, the
              | governing equations for λr and λ̂r are identical and the (unique) solutions must be equal, which
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                            92
blank         | 
              | 
              | 
text          | establishes the first result in (4.64). The second result in (4.64) follows from the simple relation
blank         | 
text          |         ∇Fr (µ; Φ, Ψ) = g λ (Φur (µ; Φ, Ψ), Ψλr (µ; Φ, Ψ), µ)
blank         |                                                                                 
text          |                       = g λ Φur (µ; Φ, Ψ), Φλ λ̂r (µ; Φλ , Θλ , Φur (µ; Φ, Ψ)), µ                (4.67)
              |                           dr (µ; Φ, Ψ, Φλ , Θλ ),
              |                         = ∇F
blank         | 
text          | where the first and last equality use the definition of g λ and the second equality uses the identity
              | between the true and minimum-residual reduced adjoints established in the first part.
blank         | 
text          |    The section closes with a discussion of the minimum-residual adjoint equations for the special
              | cases of Galerkin and LSPG projections (4.6). For problems with symmetric positive-definite Jaco-
              | bians, the Galerkin adjoint equations in (4.49) exactly match the minimum-residual adjoint equations
              | in (4.55) with Ψ = Φλ = Φ and adjoint metric
blank         | 
text          |                                             ∂r
              |                                     Θλ =       (Φur (µ; Φ, Φ))−T .                               (4.68)
              |                                             ∂u
blank         | 
text          | Additionally, these choices satisfy (4.63), which further supports the claim. Thus, for such problems,
              | the true Galerkin adjoints possess the minimum residual property and are easy to compute since they
              |                                                                                       ∂r
              | do not rely on second derivatives of r. For the case of a LSPG projection (Ψ(u, µ) =     (u, µ)Φ),
              |                                                                                      ∂u
              |               λ
              | the choices Φ = Ψ and                    "         #−1
              |                                      λ     ∂r T ∂r
              |                                    Θ =                                                       (4.69)
              |                                            ∂u ∂u
              |                                                      (Φur (µ; Φ, Ψ), µ)
blank         | 
text          | lead to the minimum-residual adjoint equations
blank         | 
text          |                                            ∂r T ∂r         ∂f
              |                                       ΦT           Φu = ΦT                                       (4.70)
              |                                            ∂u ∂u           ∂u
blank         | 
text          | where all nonlinear terms are evaluated at Φur (µ; Φ, Ψ). The above equation is identical to the
              | LSPG adjoint equations when the primal solution is exact and thus the true and minimum-residual
              | adjoints agree. This is reaffirmed since the above choices satisfy (4.63).
blank         | 
              | 
title         | 4.2     Global Hyperreduced Models
text          | Despite the small size of the nonlinear system defining the reduced-order model
blank         | 
text          |                                            ΨT r(Φur , µ) = 0,
blank         | 
text          | in terms of the number of equations and unknowns (ku ), it may still be expensive to solve. The
              | major expense emanates from the large-scale operations required to evaluate the reduced residual
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                               93
blank         | 
              | 
              | 
text          | and Jacobian (neglecting higher derivatives if Ψ is not constant)
blank         | 
text          |                                                                       ∂r
              |                                   ΨT r(Φur , µ)               ΨT         (Φur , µ)Φ,                                  (4.71)
              |                                                                       ∂u
blank         | 
text          | i.e., the reconstruction of the full state from the reduced coordinates u = Φur and projection of the
              | full residual and Jacobian into the reduced space. Such bottlenecks do not arise in the case where r is
              | polynomial in the state and parameter as terms can be precomputed offline such that no large-scale
              | operations are required online; see Section 4.2.1 for additional details. To overcome such bottlenecks
              | when r is nonlinear in state or parameter, a slew of hyperreduction 3 techniques have been proposed
              | that introduce an additional layer of approximation on top of that in (4.3). Among the most popular
              | hyperreduction techniques are (1) polynomialization methods, such as Trajectory Piece-Wise Linear
              | (TPWL) approximation [165], where the governing nonlinear equations are replaced by a weighted
              | sum of the equations linearized about preselected points in parameter space and (2) gappy methods
              | [56, 17, 115, 41, 31, 59] where only a subset of the large-scale equations and degrees of freedom are
              | used in the computation of (4.71)—in the context of PDEs this amounts to only using a subset of
              | the mesh to assemble the reduced residual and Jacobian. This document will only consider gappy
              | methods since they maintain a strong connection to the underlying physics model and enable the
              | reduced residual and Jacobian to be evaluated without incurring operations that scale with the size
              | of the full mesh.
blank         | 
              | 
title         | 4.2.1       Precomputation for Polynomial Nonlinearities
text          | In the special case where the nonlinearity in the state and parameter is polynomial, the contraction
              | of the each monomial term with the reduced basis can be precomputed. As a result, each query
              | to the reduced residual and Jacobian will not involve operations that scale with Nu . Consider the
              | Taylor expansion of the governing equation of degree m in the state and degree n in the parameter
              |                m Xn
              |               X      1               ∂ j+k r
              | r(u, µ) =                                                             (u − û)p1 · · · (u − û)pj (µ − µ̂)q1 · · · (µ − µ̂)qk
              |               j=0
              |                     j!k! ∂up1 · · · ∂upj ∂µq1 · · · ∂µqk   (û, µ̂)
              |                   k=0
              |                                                                                                                       (4.72)
              |                Nu                Nµ
              | where û ∈ R        and µ̂ ∈ R        are the expansion points. If the governing equation is at most degree
              | m in the state and n in the parameter, this expansion is exact for any expansion points. Otherwise,
              | it is an approximation and its quality will be heavily dependent on û and µ̂. The remainder of this
              | section will primarily be concerned with the case where the governing equation is polynomial and
              | the expansion points will be taken as û = 0Nu and µ̂ = 0Nµ for simplicity. Define the following
              | j + k + 1-order tensor for j = 1, . . . , m and k = 1, . . . , n as the monomials arising in the above
              |   3a   term coined in [175]
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                               94
blank         | 
              | 
              | 
text          | expansion
blank         | 
text          |                                                                      ∂ j+k r
              |                                        D jk (u, µ) =                                              .                    (4.73)
              |                                                                  · · ∂u} ∂µ · · · ∂µ
              |                                                              | ·{z
              |                                                              ∂u
              |                                                                          | {z }
              |                                                                 j terms      k terms     (u, µ)
blank         | 
text          | When arguments are omitted in the above definition, they are assumed to be zero, i.e., D jk =
              | D jk (0Nu , 0Nµ ). With this definition, the expansion of the governing equation becomes
blank         | 
text          |                                              m Xn
              |                                             X      1    jk
              |                         r(u, µ)i =                     Dip                 u · · · upj µq1 · · · µqk .
              |                                                            1 ···pj q1 ···qk p1
              |                                                                                                                        (4.74)
              |                                             j=0
              |                                                   j!k!
              |                                                    k=0
blank         | 
              | 
text          | Introduce the model reduction ansatz for both the state and parameter :
blank         | 
text          |                                                         u = Φy            µ = Υη                                       (4.75)
blank         | 
text          | where Φ ∈ RNu ×ku and Υ ∈ RNµ ×kµ are the reduced bases for the state and parameter spaces,
              | y ∈ Rku and η ∈ Rkµ are the corresponding reduced coordinates, and ku  Nu and kµ  Nµ .
              | Substitution of these ansatz into the polynomial expansion of the governing equations in (4.72) and
              | subsequent projection onto the columnspace of Φ (a Galerkin projection) leads to
blank         | 
text          |                                      m Xn
              |                                     X      1  jk 
              |                      [rr (y, η)]t =            Dr tr ···r s ···s yr1 · · · yrj ηs1 · · · ηsk                           (4.76)
              |                                     j=0
              |                                           j!k!      1    j 1    k
              |                                                   k=0
blank         | 
              | 
text          | where the reduced monomial terms that have been contracted with the reduced bases are
blank         | 
text          |                                                     jk
              |                      Drjk
blank         |                            
text          |                             tr1 ···rj s1 ···sk
              |                                                  = Dip1 ···pj q1 ···qk
              |                                                                        Φit Φp1 r1 · · · Φpj rj Υq1 s1 · · · Υqk sk .   (4.77)
blank         | 
text          |                                                                                                     ∂rr
              | From (4.76), the evaluation of the reduced residual rr (y, η) and Jacobian                              (y, η) are completely
              |                                                                                                     ∂y
              | independent of the potentially large dimensions Nu and Nµ                                 but scale poorly with the reduced
              |                                                                                        m+1 n
              | dimensions ku and kµ . For example, the evaluation of the reduced residual requires O(ku  kµ )
              | operations, which makes this feasible for only small polynomial orders m and n and reduced basis
              | sizes ku and kµ . Two common examples of nonlinear partial differential equations that possess poly-
              | nomial nonlinearities in the state and parameter are: the incompressible Navier-Stokes equations
              | (quadratic in the state) and the geometrically nonlinear structure with a St. Venant-Kirchhoff ma-
              | terial law (cubic in the state and linear in the material parameters). While these types of problems
              | arise in a number of important applications, the problems considered in this work will not possess
              | polynomial nonlinearities. Therefore, these methods will not be considered further and attention is
              | turned to the more general gappy methods.
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           95
blank         | 
              | 
              | 
title         | 4.2.2    Mask and Sample Mesh
text          | The discussion of gappy methods in the context of partial differential equations begins with the
              | critical notion of a mask and sample mesh. Gappy methods are characterized by the distinguishing
              | feature that they only consider a subset of the governing equations, that is, only entries ri are
              | needed, where i ∈ M ⊂ {1, . . . , Nu }. This subset M is called the mask. Let P ∈ RNu ×|M| be the
              | subset of the columns of the identity matrix that includes ei only if i ∈ M. Then, the mask of the
              | governing residual r can be compactly represented as P T r. When r represents a discretized PDE,
              | the evaluation of ri will require the solution uj for all j ∈ Si ⊂ {1, . . . , Nu }, where the Si depends
              | on the discretization scheme and the PDE under consideration. Define the sample mesh as the set
              |                                                     [
              |                                               S=         Si
              |                                                    i∈M
blank         | 
              | 
text          | and let P̄ ∈ RNu ×|S| be the subset of the columns of the identity matrix that include ei only if i ∈ S.
              | Then, the restriction of the state vector u to the sample mesh is compactly written as P̄ T u. All of
              | the gappy-based hyperreduction methods considered in this document rely on the computation of
              | the masked reduced residual and Jacobian
blank         | 
text          |                                                          ∂r
              |                      P T r(P̄ P̄ T Φur , µ)         PT      (P̄ P̄ T Φur , µ)P̄ P̄ T Φ            (4.78)
              |                                                          ∂u
blank         | 
text          | as they are much less expensive to compute than the terms in (4.71) if |M|  Nu . While the
              | notation in (4.78) will prove convenient in later sections, it does not necessarily reveal the efficiency
              |                                                                                     ∂r
              | of gappy methods. An efficient implementation will compute P T r and P T               P̄ directly from
              |                                                                                     ∂u
              | P̄ Φur without reconstructing a Nu -vector padded with zeros as the notation P̄ P̄ T Φur suggests.
              |   T
blank         | 
text          | Furthermore, the restriction of the reduced-order basis to the mask, P T Φ, can be precomputed.
              | These implementation details enable the terms in (4.78) to be computed efficiently online, i.e.,
              | without incurring operations that scale with the full mesh O(Nu ).
              |    For brevity in the developments to follow, the following notation is introduced for the recon-
              | structed state vector restricted to the sample mesh and reduced residual restricted to the mask
blank         | 
              | 
text          |                          uh = P̄ P̄ T Φur          rh (uh , µ) = P T r(uh , µ).                   (4.79)
blank         | 
text          | Then, the masked Jacobians with respect to the state and parameter are
blank         | 
text          |          ∂rh                ∂r                             ∂rh                ∂r
              |              (uh , µ) = P T    (uh , µ)P̄ P̄ T Φ               (uh , µ) = P T    (uh , µ).        (4.80)
              |          ∂uh                ∂u                             ∂µ                 ∂µ
blank         | 
text          | Armed with this notation, the governing equations for gappy-based hyperreduction methods take
              | the general form
              |                                        A(uh , µ)T rh (uh , µ) = 0,                                (4.81)
blank         | 
text          | where it is assumed that A can be computed efficiently online or precomputed offline. Regardless
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                  96
blank         | 
              | 
              | 
text          | of the hyperreduced approach considered, the quantity of interest is defined according to (4.5), i.e.,
blank         | 
text          |                                            Fr (µ) = f (Φur (µ), µ)
blank         | 
text          | where ur (µ) is the solution of the hyperreduced model. In many cases, the entire reconstructed state
              | Φur is not required to evaluate f , which commonly arises when f corresponds to a surface integral,
              | i.e., only entries corresponding to nodes on the surface are required. In these cases, only a subset of
              | the reconstructed vector P̃ T Φur are required and Fr can be computed efficiently. This implemen-
              | tation optimization will not be considered in the remainder as it complicates the exposition. The
              | industry-scale example of the shape optimization of a full aircraft configuration in Section 5.5.4 will
              | leverage this precise optimization since its optimization functionals involve forces integrated along
              | the surface. The next section provides specific examples of gappy-based hyperreduction method that
              | can be written in the general form (4.81).
blank         | 
              | 
title         | 4.2.3      Examples
text          | The simplest approach to gappy-based hyperreduction is to simply ignore information that is not
              | included in the mask. This approach is usually known as collocation and the general form of the
              | projection-based reduced-order model is
blank         | 
text          |                                           (P T Ψ)T rh (uh , µ) = 0,                                      (4.82)
blank         | 
text          | which fits into the general form in (4.81) with A(u, µ) = P T Ψ(u, µ). While this approach is naive
              | in the sense that it makes no attempt to account for missing information, it is simple and robust,
              | provided a sufficiently large sample mesh is used.
              |    In contrast to the naive approach, a number of methods exist that attempt to account for
              | the missing information using ideas setforth in [56], which include (Discrete) Empirical Interpola-
              | tion Method ((D)EIM) [17, 41] and Gauss-Newton with Approximated Tensors (GNAT) [31]. The
              | (D)EIM approach assumes the residual lies in the low-dimensional subspace defined by the span of
              | a separate basis Φr 4 , i.e.,
              |                                          r(Φur , µ) = Φr h(ur , µ),                                      (4.83)
blank         | 
text          | where h : Rku × RNµ → Rku are the reduced coordinates of the residual in the basis Φ. The reduced
              | coordinates are defined such that the representation in (4.83) exactly matches the true residual on
              | the mask
              |                                  P T Φr h(Φur , µ) = P T r(P̄ P̄ T Φur , µ),                             (4.84)
blank         | 
text          | which leads to the following expression for the residual reduced coordinates h
blank         | 
text          |                                     h(Φur , µ) = (P T Φr )−1 rh (ur , µ)                                 (4.85)
              |   4 Usually the residual is separated into its linear and nonlinear components and (D)EIM is only applied to the
blank         | 
text          | nonlinear portion.
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        97
blank         | 
              | 
              | 
text          | where the definition of rh in (4.80) was used. Combining (4.83) and (4.85) into the form of the
              | projection-based reduced-order model in (4.3), the (D)EIM governing equations are
              |                                            T            −1
              |                                     ΦTr Ψ        P T Φr           rh (uh , µ) = 0.             (4.86)
blank         | 
text          | (D)EIM fits into the general form of gappy-based hyperreduced models in (4.81) with A(u, µ) =
              |        T        −1
              |   ΦTr Ψ    P T Φr     .
              |    The GNAT method is a minor generalization of (D)EIM that also approximates the residual in
              | a separate low-dimensional subspace spanned by the basis Φr ∈ RNu ×kr (kr  Nu )
blank         | 
text          |                                        r(Φur , µ) = Φr h(ur , µ),                              (4.87)
blank         | 
text          | and the reduced coordinates h : Rku × RNµ → Rkr are defined such that the representation in (4.87)
              | matches, in a least-squares sense, the true residual on the mask
blank         | 
text          |                            h(ur , µ) = arg min P T Φr z − rh (uh , µ)                2
              |                                                                                          .     (4.88)
              |                                              z∈Rkr
blank         | 
              | 
text          | The GNAT governing equations follow from combining (4.87) and (4.88) into the form of the
              | projection-based reduced-order model in (4.3)
              |                                             T             †
              |                                      ΦTr Ψ          P T Φr        rh (uh , µ) = 0.             (4.89)
blank         | 
text          | The GNAT equations fit into the general form of gappy-based hyperreduction models in (4.81) with
              |          T       †
              | A = ΦTr Ψ     P T Φr .
              |    From the above construction, a number of advantages and disadvantages of each approach emerge.
              | As previously mentioned, the (D)EIM and GNAT methods have the desirable property of attempting
              | to account for information missing from the mask by approximating the nonlinear residual in a low-
              | dimensional subspace, while collocation simply ignores missing information. However, due to the
              | nonlinearity of the residual r(u, µ), it cannot be guaranteed (or even expected) to lie in a low-
              | dimensional subspace, even if the state vector u does. In fact, several works [31, 33] have shown
              | that, even to reproduce training data, the residual approximation in (4.87) requires kr  ku for the
              | resulting hyperreduced model to be sufficiently accurate. In practice, the (D)EIM and GNAT ansatz
              | in (4.83) and (4.87) cause the corresponding methods to exhibit classical over-fitting behavior, i.e.,
              | superb accuracy when reproducing training data and low accuracy at predictive points, particularly
              | when applied to real engineering applications [198]. A notable distinction between these methods is
              | that collocation only requires the trial basis (Φ), test basis (Ψ), and mask (P ), while (D)EIM and
              | GNAT require the construction of a separate residual basis (Φr ) that involves substantial additional
              | offline training—usually requiring the collection and compression of residual snapshots [31]. It has
              | been observed [198] that residual snapshots are not necessarily amenable to compression, which
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           98
blank         | 
              | 
              | 
text          | results large number of basis vectors (kr  1) and ultimately hurts the performance of the reduced-
              | order model.
              |    Given this discussion, only the collocation hyperreduction method will be considered in the
              | remainder. This is predominantly to avoid the overfitting behavior of the other approaches since
              | the hyperreduced model will be heavily used in predictive settings in subsequent chapters. The
              | remainder of this chapter will discuss a formulation of the primal, sensitivity, and adjoint collocation-
              | based hyperreduced model that results in optimal approximations, in the sense of minimizing the
              | residual in some norm on the mask.
blank         | 
              | 
title         | 4.2.4     Minimum-Residual Primal Formulation
text          | The form of the test basis Ψ in the gappy-accelerated projection-based reduced-order models in
              | (4.82), (4.86), (4.89) has remainder arbitrary to this point.        Given the desirable properties of
              | minimum-residual reduced-order models detailed in Sections 4.1.1–4.1.3, the test basis is defined
              | in accordance with a parallel concept in the collocation-based hyperreduction setting—the masked
              | minimum-residual property. This property (Definition 4.2) requires the solution of the hyperreduced
              | model to minimize the residual over the mask in some norm. For the remainder of this document, the
              | solution of the general form of the collocation-based hyperreduced model in (4.81) will be denoted
              | uh (µ; Φ, Ψ, P ), i.e.,
              |                                        T
              |                                  PTΨ        rh (uh (µ; Φ, Ψ, P ), µ) = 0.
blank         | 
text          | Furthermore, the mask-reconstructed state vector uh is identified with its corresponding reduced
              | coordinates ur from (4.79), i.e., uh (µ; Φ, Ψ, P ) = P̄ P̄ T Φur (µ; Φ, Ψ, P ). The sample mesh P̄ is
              | not included in the argument list since it is uniquely determined from the mask P and the structure
              | of the governing equation r. When there is no risk of confusion regarding the choice of mask, test
              | basis, and trial basis, the arguments will be dropped.
blank         | 
text          | Definition 4.2 (Masked Minimum-Residual Property). A hyperreduced model of the form (4.81)
              | possesses the masked minimum-residual property if the solution satisfies the first-order optimality
              | conditions of the following masked residual minimization problem
blank         | 
text          |                                               1                          2
              |                                  minimize       P T r(P̄ P̄ T Φur , µ)   Θ
              |                                                                                                   (4.90)
              |                                   ur ∈Rku     2
blank         | 
text          | for some symmetric positive-definite Θ ∈ R|M|×|M| .
blank         | 
text          |    Masked minimum-residual hyperreduced models possess a similar monotonicity property as that
              | defined in Proposition 4.1 for minimum-residual projection-based reduced-order models. The inter-
              | polation property only holds if the solution of the minimum-residual hyperreduced model is unique
              | (guaranteed if the mask is full, i.e., M = {1, . . . , Nu } by Assumption 2.2). These properties are
              | stated precisely in Proposition 4.6.
blank         | 
text          | Proposition 4.6. Let (Φ, Ψ, Θ, P ) define masked minimum-residual hyperreduced model whose
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                        99
blank         | 
              | 
              | 
text          | solution coincides with the global minimum of (4.90). Then, the following properties hold for any
              | µ ∈ RNµ :
blank         | 
text          |    • (Optimality) For any u ∈ RNu such that P̄ T u ∈ col(P̄ T Φ),
blank         | 
text          |                           P T r(P̄ P̄ T Φur (µ; Φ, Ψ, P ), µ)      Θ
              |                                                                        ≤ P T r(P̄ P̄ T u, µ)    Θ
              |                                                                                                     .           (4.91)
blank         | 
              | 
text          |    • (Monotonicity) Let (Φ0 , Ψ0 , P ) define a hyperreduced model such that col(P̄ T Φ0 ) ⊆ col(P̄ T Φ),
              |       then
blank         | 
text          |              P T r(P̄ P̄ T Φur (µ; Φ, Ψ, P ), µ)   Θ
              |                                                        ≤ P T r(P̄ P̄ T Φ0 ur (µ; Φ0 , Ψ0 , P ), µ)      Θ
              |                                                                                                             .   (4.92)
blank         | 
              | 
text          |    • (Interpolatory) If P̄ T u(µ) ∈ col(P̄ T Φ), then
blank         | 
text          |                                      P T r(P̄ P̄ T Φur (µ; Φ, Ψ, P ), µ) = 0.                                   (4.93)
blank         | 
              | 
text          | Proof. Optimality follows from the fact that ur (µ; Φ, Ψ, P ) is the global minima of the optimization
              | problem in (4.90): for P̄ T u ∈ col(P̄ T Φ), there exists y ∈ Rku such that P̄ T u = P̄ T Φy and since
              | ur is the global minima of (4.90), we have
blank         | 
text          |                       P T r(P̄ P̄ T Φur (µ; Φ, Ψ, P ), µ)     Θ
              |                                                                   ≤ P T r(P̄ P̄ T Φy, µ)    Θ
              |                                                                                                 .               (4.94)
blank         | 
text          | A simple application of the optimality property to P̄ T Φ0 ur (µ; Φ0 , Ψ0 ) ∈ col(P̄ T Φ0 ) ⊆ col(P̄ T Φ)
              | leads to monotonicity. Finally, if the solution of P T r( · , µ) = 0 is contained in the columnspace of
              | P̄ T Φ, i.e., P̄ T u(µ) ∈ col(P̄ T Φ), the optimality property implies
blank         | 
text          |                     P T r(P̄ P̄ T Φur (µ; Φ, Ψ), µ)     Θ
              |                                                             ≤ P T r(P̄ P̄ T u(µ), µ)    Θ
              |                                                                                             = 0,                (4.95)
blank         | 
text          | which is precisely the interpolation property in (4.93).
blank         | 
text          |    The first-order optimality condition of (4.90) is
blank         | 
text          |                                       ∂rh
              |                                           (uh , µ)T Θrh (uh , µ) = 0                                            (4.96)
              |                                       ∂uh
blank         | 
text          | and, therefore, the masked test basis must satisfy
blank         | 
text          |                                                              ∂rh
              |                                       P T Ψ(uh , µ) = Θ          (uh , µ)                                       (4.97)
              |                                                              ∂uh
blank         | 
text          | for the collocation-based projection-based hyperreduced model (4.82) to possess the masked minimum-
              | residual property. The special case of a LSPG projection satisfies (4.97) with
blank         | 
text          |                                                                        ∂rh
              |                               Θ=I              P T Ψ(uh , µ) =             (uh , µ).                            (4.98)
              |                                                                        ∂uh
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                       100
blank         | 
              | 
              | 
text          | where I is the |M| × |M| identity matrix and therefore possess the masked minimum-residual
              | property. The special case of a Galerkin projection (P T Ψ = P T Φ) is more cumbersome to interpret
              | as a special case of the optimality conditions in (4.97) in the hyperreduced setting. Unlike the pure
              |                                                                    ∂rh
              | projection setting of Section 4.1.1, the reduced Jacobian matrix        is not square, in general, and
              |                                                                    ∂uh
              | cannot define a valid norm. From (4.97), for a Galerkin projection to possess the masked minimum-
              | residual property, Θ must be selected such that the following constrained linear system of equations
              | (linear in Θ) is satisfied
blank         | 
text          |                                    ∂rh
              |                                Θ       = PTΦ        subject to      Θ  0.                     (4.99)
              |                                    ∂uh
blank         | 
text          | In general, there is no guarantee this constrained system of equations has a solution. The next
              | section derives the sensitivity equations corresponding to the collocation-based hyperreduced models
              | introduced in this section and develops a minimum-residual variant.
blank         | 
              | 
title         | 4.2.5     Exact and Minimum-Residual Sensitivity Formulation
text          | The sensitivity analysis for the hyperreduced model parallels the exposition for the reduced-order
              | models in Section 4.1.2. The total derivative of the quantity of interest is expanded as
blank         | 
text          |                ∂f                          ∂f                         ∂ur
              |    ∇Fr (µ) =      (Φur (µ; Φ, Ψ, P ), µ) +    (Φur (µ; Φ, Ψ, P ), µ)Φ     (µ; Φ, Ψ, P )       (4.100)
              |                ∂µ                          ∂u                         ∂µ
blank         | 
text          | where ur (µ; Φ, Ψ, P ) are the reduced coordinates corresponding to the solution of the hyperreduced
              |                      ∂ur
              | model in (4.82) and      (µ; Φ, Ψ, P ) is the corresponding sensitivity. The reduced sensitivities are
              |                      ∂µ
              | derived by differentiating the governing hyperreduced model in (4.82). In the general case where
              | P T Ψ is state- and parameter-dependent, the reduced sensitivities are defined as the solution of the
              | linear system of equations
              |                                                                              
              |                        |M|            T   T
blank         |                                                
text          |                                 ∂ (P    Ψ)  e                           ∂r
              |                                              j                             h   ∂ur =
              |                       X
              |                       (P T r)j                  P̄ P̄ T Φ + (P T Ψ)T
              |                        j=1
              |                                       ∂u                                ∂u  h   ∂µ
              |                                                                                             (4.101)
              |                                     |M|                  T   T
blank         |                                                                    
text          |                                    X             ∂   (P    Ψ)  e j               ∂r h
              |                                 −  (P T r)j                         + (P T Ψ)T       
              |                                     j=1
              |                                                          ∂µ                      ∂µ
blank         | 
              | 
text          | where all terms are evaluated at the primal solution uh (µ; Φ, Ψ, P ). In the special case where the
              | masked primal solution is exact on the mask, i.e., rh (uh (µ), µ) = 0, or the masked test basis is
              | constant, the expression in (4.101) reduces to
blank         |                                             
text          |                                           ∂rh ∂ur             ∂rh
              |                                  (P T Ψ)T         = −(P T Ψ)T     .                           (4.102)
              |                                           ∂uh ∂µ              ∂µ
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           101
blank         | 
              | 
              | 
text          | A Galerkin projection uses a constant test basis P T Ψ = P T Φ and the hyperreduced sensitivity
              | equations takes the form                           
              |                                           T   T ∂rh   ∂ur             ∂rh
              |                                         (P Φ)             = −(P T Φ)T     .                        (4.103)
              |                                                 ∂uh ∂µ                ∂µ
              |                                                                          ∂rh
              | A LSPG projection employs the non-constant test basis P T Ψ(u, µ) =          (u, µ) and derivatives
              |                                                                          ∂uh
              | of the test basis cannot be ignored. The resulting hyperreduced sensitivity equations are
              |                                                                                   
              |                        |M|                      2
              |                                                                            T
              |                       X
              |                       (P T r)j (P̄ T Φ)T P̄ T ∂   r j                 ∂r h   ∂rh   ∂ur =
              |                                                        P̄ (P̄ T Φ) +
              |                        j=1
              |                                                ∂u∂u                    ∂u h   ∂u h     ∂µ
              |                                                                                                  (4.104)
              |                                         |M|                         2
              |                                                                                     T
              |                                        X                           ∂   rj      ∂r  h   ∂r h
              |                                    −  (P T r)j (P̄ T Φ)T P̄ T               +              
              |                                         j=1
              |                                                                   ∂u∂µ         ∂u  h    ∂µ
blank         | 
text          |    The difficulty associated with computing derivatives of the test basis, as well as the merits of
              | minimum-residual formulations discussed in Section 4.1, motivate the introduction of a collocation-
              | based equivalent of the minimum-residual sensitivity reduced-order model of Section 4.1.2. For
              | generality, consider the low-dimensional approximation of the high-dimensional model sensitivity
blank         | 
text          |                                                    ∂u      ∂u
              |                                                            dr
              |                                                       = Φ∂    ,                                    (4.105)
              |                                                    ∂µ      ∂µ
blank         | 
text          |                                                                    ∂u
              |                                                                    dr
              | where Φ∂ ∈ RNu ×ku is the reduced-order basis for the sensitivities andare the corresponding
              |                                                                     ∂µ
              | reduced coordinates. The reduced coordinates are defined as the argument that minimizes the
              | sensitivity residual on the mask, i.e.,
blank         | 
text          |                    ∂u
              |                    dr                                    1                                  2
              |                       (µ; Φ∂ , Θ∂ , P , u) = arg min       P T r ∂ (u, P̄ P̄ T Φ∂ wr , µ)          (4.106)
              |                    ∂µ                       wr ∈R ku ×Nµ 2                                  Θ∂
blank         | 
              | 
text          | where u ∈ RNu is any linearization point, usually the reconstructed primal solution, i.e., u =
              | P̄ P̄ T Φur (µ; Φ, Ψ, P ) and Θ∂  0 is the metric defining the norm. The first-order optimality
              | condition of the linear least-squares problem in (4.106) leads to the normal equations
              |                             T                        d                       T
              |              ∂r                          ∂r              ∂ur        ∂r                      ∂r
              |         PT      P̄ P̄ T Φ∂        Θ∂ P T    P̄ P̄ T Φ∂       = − PT    P̄ P̄ T Φ∂    Θ∂ P T    .   (4.107)
              |              ∂u                          ∂u              ∂µ         ∂u                      ∂µ
blank         | 
text          | where all terms are evaluated at the linearization point. A variant of the monotonicity and inter-
              | polation properties of Proposition 4.2 hold for masked minimum-residual sensitivity hyperreduced
              | model. In this case, monotonicity is guaranteed with respect to a fixed metric and mask and in-
              | terpolation requires a sufficiently large mask such that P T r(u, µ) = 0 =⇒ r(u, µ) = 0. These
              | results are stated and proved in Proposition 4.7.
blank         | 
text          | Proposition 4.7. Let (Φ∂ , Θ∂ , P ) define a masked minimum-residual sensitivity reduced-order
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                                       102
blank         | 
              | 
              | 
text          | model. Then the following properties hold for any µ ∈ RNµ
blank         | 
text          |    • (Optimality) For any u ∈ RNu , w ∈ RNu , and P̄ T w ∈ col(P̄ T Φ∂ ), then
              |                                                                                    !
              |                                       ∂u
              |                                       dr
              |                                               ∂
              |                  T
              |                      rk∂             T
              |                                          (µ; Φ∂ , Θ∂ , P , u)ek , µ                             ≤ P T rk∂ u, P̄ P̄ T w, µ
blank         |                                                                                                                          
text          |              P             u, P̄ P̄ Φ                                                                                        Θ∂
              |                                       ∂µ
              |                                                                                         Θ∂
              |                                                                                                                              (4.108)
              |        for k = 1, . . . , Nµ , where                rk∂ (u,                   ∂         T
              |                                                               w · ek , µ) := r (u, we , µ)ek and ek is the kth canonical
              |        unit vector.
              |                                               0       0
              |    • (Monotonicity) Let (Φ∂ , Θ∂ , P ) define a masked minimum-residual sensitivity reduced-order
              |                                           0
              |        model such that col(Φ∂ ) ⊆ col(Φ∂ ), then
blank         | 
text          |                                                   P T rk∂ (u, w, µ)   Θ∂
              |                                                                            ≤ P T rk∂ (u, w0 , µ)      Θ∂
              |                                                                                                            ,                 (4.109)
blank         | 
text          |                           ∂u
              |                           dr                                     0 ∂u
              |                                                                    dr       0    0
              |        where w = Φ∂           (µ; Φ∂ , Θ∂ , P , u)ek and w0 = Φ∂      (µ; Φ∂ , Θ∂ , P , u)ek , for k =
              |                           ∂µ                                       ∂µ
              |        1, . . . , Nµ and any u ∈ RNµ .
              |                                          ∂u
              |    • (Interpolation) If P̄ T                 (µ) ∈ col(P̄ T Φ∂ ), then
              |                                          ∂µk
              |                                                                                                 !
              |                                                                    ∂u
              |                                                                    dr
              |                                    P T rk∂           u, P̄ P̄ T Φ∂    (µ; Φ∂ , Θ∂ , P , u)ek , µ = 0.                        (4.110)
              |                                                                    ∂µ
blank         | 
              | 
text          |                                             ∂u
              |                                             dr
              | Proof. Optimality follows from the fact that     (µ; Φ∂ , Θ∂ , P , u) is the (unique) minima of the
              |                                             ∂µ
              | optimization problem in (4.106). A simple application of the optimality property to
blank         | 
text          |                                      0 ∂u
              |                                        dr                     0   0                         0
              |                            P̄ T Φ∂                 (µ; Φ∂ , Θ∂ , P , u) ∈ col(P̄ T Φ∂ ) ⊆ col(P̄ T Φ∂ )
              |                                          ∂µ
blank         | 
text          | leads to monotonicity. Finally, if a solution of P T r ∂ (P̄ P̄ T u(µ), · , µ) = 0 is contained in the
              |                                     ∂u
              | columnspace of P̄ T Φ∂ , i.e., P̄ T    (µ) ∈ col(P̄ T Φ∂ ), the optimality property implies
              |                                     ∂µ
              |                                                                               !                                   
              |                           ∂u
              |                           dr
              |                                ∂                                                                        ∂ û
              |    T
              |  P r   ∂
              |            û(µ), P̄ P̄ Φ  T
              |                              (µ; Φ∂ , Θ∂ , P , û(µ)), µ                               ≤ P T r ∂ û(µ),      (µ), µ               = 0,
              |                           ∂µ                                                                            ∂µ                   Θ∂
              |                                                                                   Θ∂
              |                                                                                               (4.111)
              |                            T ∂ û             T ∂u
              | where û(µ) = P̄ P̄ u(µ) and      (µ) = P̄ P̄      (µ), which is precisely the interpolation property
              |                              ∂µ                 ∂µ
              | in (4.110).
blank         | 
text          |    In addition to monotonicity and interpolation, conditions exist (Proposition 4.8) that guarantee
              | the two types of sensitivities introduced in this section, i.e., the sensitivities of the hyperreduced
              | model and the masked minimum-residual hyperreduced sensitivities, agree. These conditions also
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                              103
blank         | 
              | 
              | 
text          | guarantee that the reconstruction of these sensitivities in the full space yield the same approximation
              | of the high-dimensional model sensitivities. Among these conditions is a required relationship (4.112)
              | between the trial (Φ) and test basis (Ψ) for the primal hyperreduced model, the sensitivity metric
              | (Θ), and the sensitivity basis (Φ∂ ). The result of Proposition 4.8 is significant since it provides
              | conditions under which the easily computed masked minimum-residual hyperreduced sensitivities
              | (independent of second derivatives of r) match the desired hyperreduction sensitivities (guarantee
              | consistency of gradient computations).
blank         | 
text          | Proposition 4.8. Consider a primal hyperreduced model defined by trial and test bases Φ and Ψ,
              | respectively, and mask P and a minimum-residual sensitivity hyperreduced model defined by basis
              | Φ∂ , mask P , and metric Θ∂ . Suppose that either: (1) the primal solution of the hyperreduced model
              | exactly reconstructs the HDM solution on the sample mesh, i.e.,
blank         | 
text          |                                          P̄ T u(µ) = P̄ T Φur (µ; Φ, Ψ)
blank         | 
text          | or (2) the masked test basis P T Ψ is constant. Then, for any u ∈ RNu , the relationships
blank         | 
text          |                                           P T Φ∂ = P T Φ
              |                                                            ∂r                                        (4.112)
              |                                     P T Ψ(u, µ) = Θ∂ P T      (u, µ)P̄ P̄ T Φ∂
              |                                                            ∂u
blank         | 
text          | guarantee the sensitivity of the primal hyperreduced model (Φ, Ψ, P ) coincides with the solution of
              | the minimum-residual sensitivity hyperreduced model (Φ∂ , Θ∂ , P )
blank         | 
text          |                       ∂ur                 ∂u
              |                                           dr
              |                           (µ; Φ, Ψ, P ) =    (µ; Φ∂ , Θ∂ , P , Φur (µ; Φ, Ψ, P )).                   (4.113)
              |                       ∂µ                  ∂µ
blank         | 
text          | Proof. Let P̄ T Φur = P̄ T Φur (µ; Φ, Ψ, P ) denote the reconstructed primal solution of the projection-
              | based reduced-order model, restricted to the primal mesh. If either the primal solution is exact
              | (P T r(P̄ P̄ T Φur , µ) = 0) or the test basis is constant, the general form of the reduced-order model
              | sensitivity equations in (4.101) reduces to the equation in (4.102), where all terms are evaluated at
              | the primal solution, i.e.,
blank         |                                                     
text          |                              T ∂rh                    ∂ur                           T ∂rh
              |     P T Ψ(P̄ P̄ T Φur , µ)          (P̄ P̄ T Φur , µ)      = − P T Ψ(P̄ P̄ T Φur , µ)       (P̄ P̄ T Φur , µ).
              |                                 ∂uh                    ∂µ                               ∂µ
              |                                                                                                       (4.114)
              | Conversely, the normal form of the minimum-residual sensitivity reduced-order model in (4.107)
              | reduces to
              |                                                   d
              |                           T ∂rh                    ∂ur                           T ∂rh
              |     P T Ψ(P̄ P̄ T Φur , µ)       (P̄ P̄ T Φur , µ)      = − P T Ψ(P̄ P̄ T Φur , µ)       (P̄ P̄ T Φur , µ).
              |                              ∂uh                    ∂µ                               ∂µ
              |                                                                                                    (4.115)
              | when the relationships in (4.112) are enforced. Thus, under the aforementioned conditions, the
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                            104
blank         | 
              | 
              | 
text          |                            ∂ur     ∂u
              |                                    dr
              | governing equations for        and    are identical and the (unique) solutions must be equal, which
              |                            ∂µ      ∂µ
              | establishes (4.113).
blank         | 
              | 
title         | 4.2.6     Adjoint Formulation
text          | The adjoint equations for the collocation-based hyperreduced model are derived using the optimiza-
              | tion procedure, outlined in Section 2.3.4, applied to the governing equation (P T Ψ)T rh (P̄ P̄ T Φur , µ) =
              | 0 and reduced quantity of interest f (Φur , µ). Consider the auxiliary optimization problem
blank         | 
text          |                                     minimize       f (Φur , µ̂)
              |                                       ur ∈Rku
              |                                                                                                     (4.116)
              |                                                       T    T
              |                                     subject to     (P Ψ) rh (uh , µ̂) = 0
blank         | 
text          | for a fixed µ̂ and the corresponding Lagrangian
blank         | 
text          |                         Lr (ur , λr ) = f (Φur , µ̂) − λTr (P T Ψ)T rh (P̄ P̄ T Φur , µ̂).          (4.117)
blank         | 
text          | by comparing this expression for the Lagrangian with that in (2.98), it is clear that the masked
              | HDM Lagrange multipliers are reconstructed from the reduced Lagrange multipliers as
blank         | 
text          |                                                    λ = Ψλr .                                        (4.118)
blank         | 
text          | The stationarity of the Lagrangian with respect to ur leads to the reduced adjoint equations
              |                                                                  
              |                    |M|
              |                                                                              ∂f T
              |                                 T  T
blank         |                                        
text          |                   X         ∂ (P Ψ) ej                        ∂rh 
              |                   (P T r)j              P̄ P̄ T Φ + (P T Ψ)T       λ r = ΦT                        (4.119)
              |                    j=1
              |                                 ∂u                            ∂uh            ∂u
blank         | 
              | 
text          | where all terms are evaluated at the reconstructed primal solution, P̄ P̄ T Φur (µ; Φ, Ψ, P ). For any
              | µ ∈ RNµ , bases Φ, Ψ, and mask P , the solution of the above equation is denoted λr (µ; Φ, Ψ, P ).
              | The gradient of the quantity of interest is reconstructed as
blank         | 
text          |                        ∂f
              |          ∇Fr (µ) =         (Φur (µ; Φ, Ψ, P ), µ)−
              |                        ∂µ
              |                                                                            
              |                                            |M|      ∂ ΨT e j                                        (4.120)
              |                                           X                               ∂rh
              |                        λr (µ; Φ, Ψ, P )T  (P T r)j            + (P T Ψ)T     
              |                                            j=1
              |                                                        ∂µ                 ∂µ
              |                                                                                          (uh , µ)
blank         | 
              | 
text          | where uh = P̄ P̄ T Φur (µ; Φ, Ψ, P ), µ). In the special case where the primal solution is exact on
              | the mask, in the sense that rh (uh , µ) = 0, or the masked test basis is constant, the adjoint equations
              | in (4.119) reduce to
              |                                                   T
              |                                                              ∂f T
blank         |                                       
text          |                                             T ∂rh
              |                                          T
              |                                        (P Ψ)         λr = ΦT                                        (4.121)
              |                                               ∂uh            ∂u
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                              105
blank         | 
              | 
              | 
text          | and the gradient of the QoI becomes
blank         |                                                                             
text          |                   ∂f                                                     ∂rh
              |       ∇Fr (µ) =      (Φur (µ; Φ, Ψ, P ), µ) − λr (µ; Φ, Ψ, P )T (P T Ψ)T                              (4.122)
              |                   ∂µ                                                     ∂µ (uh , µ)
blank         | 
text          |                             ∂f T
              |    In general, the term ΦT        requires O(Nu ) works and memory to evaluate. However, in many
              |                             ∂u
              | applications f corresponds to an integral over a surface or small portion of the domain and therefore
              |                                                             ∂f T                  ∂f T
              | ∂f /∂u is sparse. Then this terms is exactly equal to ΦT         = (P̂ T Φ)T P̂ T      , where P̂ is the
              |                                                             ∂u                    ∂u
              | subset of columns of the identity matrix that exactly restricts ∂f /∂u to its nonzero entries and can
              | be computed without requiring large-scale operations. This is an important implementation detail;
              | however, for simplicity, the additional notation will not be continued.
              |    In the special case where the primal hyperreduced model employs a Galerkin projection (P T Ψ =
              | P T Φ), the test basis is state- and parameter-independent and the adjoint equations in (4.119) or
              | (4.121) become
              |                                                      T
              |                                                                 ∂f T
blank         |                                        
text          |                                                  ∂rh
              |                                         (P T Φ)T        λr = ΦT                                       (4.123)
              |                                                  ∂uh            ∂u
              | and the gradient of the QoI is
blank         |                                                                             
text          |                   ∂f                                                     ∂rh
              |       ∇Fr (µ) =      (Φur (µ; Φ, Φ, P ), µ) − λr (µ; Φ, Φ, P )T (P T Φ)T                              (4.124)
              |                   ∂µ                                                     ∂µ (uh , µ)
blank         | 
text          | In the special case of a LSPG projection, the adjoint equations in (4.119) become
              |                                                                 
              |            |M|                                              T
              |                                      2
              |                                                                             ∂f T
blank         |                                            
text          |           X                        ∂   rj               ∂rh   ∂rh
              |           (P T r)j (P̄ T Φ)T P̄ T        P̄ (P̄ T Φ) +            λ r = ΦT                           (4.125)
              |            j=1
              |                                    ∂u∂u                 ∂uh ∂uh             ∂u
blank         | 
text          | and the gradient of the QoI is
blank         | 
text          |              ∂f
              |   ∇Fr (µ) =      (Φur (µ; Φ, Ψ, P ), µ)−
              |              ∂µ
              |                                                                                 
              |                                  |M|                      2
              |                                                                                                      (4.126)
              |                                 X                        ∂   rj              ∂rh
              |              λr (µ; Φ, Ψ, P )T  (P T r)j (P̄ T Φ)T P̄ T          + (P T Ψ)T                     .
              |                                  j=1
              |                                                          ∂u∂µ                ∂µ
              |                                                                                        (uh , µ)
blank         | 
              | 
text          |    The introduction and derivation of the masked minimum-residual hyperreduced adjoint model
              | will be deferred to future work. There are special considerations that rise when considering the
              | minimization problem
blank         | 
text          |                                                                 ∂f T      ∂r
              |   minimize    P T r λ (u, Φλ zr , µ)        = minimize   −P T        + PT    (u, µ)T Φλ zr            (4.127)
              |    zr ∈Rku                             Θλ      zr ∈Rku          ∂u        ∂u
              |                                                                                              Θλ
blank         | 
text          |                      ∂r
              | since the term P T      (u, µ)T Φλ zr requires a separate restriction matrix P̂ (subset of the columns
              |                      ∂u
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                         106
blank         | 
              | 
              | 
text          | of the identity matrix) for the following identity to hold
blank         | 
text          |                                   ∂r                  ∂r
              |                              PT      (u, µ)T Φλ = P T    (u, µ)T P̂ P̂ T Φλ                     (4.128)
              |                                   ∂u                  ∂u
blank         | 
text          | and lead to operations independent of the large dimension Nu . This is the exact type of imple-
              | mentation optimization discussed in Section 4.2.2 that lead to the efficient computations with the
              | masked reduced Jacobian (no transpose)
blank         | 
text          |                                     ∂r               ∂r
              |                                PT      (u, µ)Φ = P T    (u, µ)P̄ P̄ T Φ.                        (4.129)
              |                                     ∂u               ∂u
blank         | 
              | 
title         | 4.3     Construction of Reduced-Order Basis and Residual Mask
text          | The present exposition on projection-based reduced-order models has focused on the formulation of
              | the governing equations that guarantee desirable properties for a fixed trial basis Φ and mask P ;
              | however, there has been no mention of the origin of these quantities, a process usually called training.
              | The specific training strategy will vary for the various applications encountered in Chapters 5–
              | 6 and an in-depth discussion will be deferred to the appropriate chapter. This section details
              | commonalities between the training methods employed in those chapters to facilitate the discussion.
              | Additionally, a general discussion is provided on training concepts used to enforce conditions required
              | for Propositions 4.1, 4.2, 4.4 to hold.
              |    A ubiquitous theme in all reduced-order model training algorithms considered in this document is
              | the method of snapshots [183]. This is the idea of building the reduced-order basis Φ from solutions,
              | or snapshots, of the high-dimensional model. In addition to building the basis from fully converged
              | solutions (individual time steps for unsteady problems [183] or steady states for steady problems),
              | unconverged nonlinear iterations [198], unconverged linear system iterates [198], sensitivities [87, 86,
              | 32, 85, 52, 210, 198], and adjoint solutions [57, 74] have also been used. These various snapshots
              | are combined into the columns of a snapshot matrix with Ns columns. This approach ensures the
              | reduced-order basis includes relevant, information-rich basis vectors that incorporate physics from
              | the underlying PDE and, in many cases, even a small reduced-order basis can result in an accurate
              | reduced-order model.
              |    When the number of snapshots becomes large, it is desirable to apply a compression method
              | to retain most of the original information contained in the snapshots. Among the most popular
              | methods is the Proper Orthogonal Decomposition (POD), Algorithm 4, also known as the truncated
              | Singular Value Decomposition (SVD), Karhunen-Loève (KL) decomposition, and Principal Compo-
              | nent Analysis (PCA). POD possess the desirable property of ordering the potential basis vectors
              | according to energy, or importance with regard to reconstructing the snapshot matrix. Therefore the
              | reduced-order basis is taken as the first ku vectors, where ku is chosen based on the singular value
              | decay or naively according to a desired basis size. The latter approach is described in Algorithm 4
              | and the operation of applying POD to build a reduced-order basis Φ from the snapshot matrix X
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          107
blank         | 
              | 
              | 
text          | will be denoted Φ = POD(X).
blank         | 
title         | Algorithm 4 Proper Orthogonal Decomposition
blank         | 
text          |                                                Φ = POD(X)
              | Input: Snapshot matrix X ∈ RNu ×ks and reduced-order basis size ku
              | Output: Reduced-order basis Φ                                                        
              |  1: Compute the thin SVD of X: X = U ΣV T , where U = u1 u2 · · ·               uks
blank         |                         
text          |  2: Φ = u1 u2 · · · uku
blank         | 
              | 
text          |    POD is well-known to be susceptible to bias when there is substantial variation in the scale of
              | the columns of the snapshot matrix. This will occur, for example, when a heterogeneous collection
              | of snapshots are used, i.e., states and sensitivities, since the units of the columns will not be consis-
              | tent. The result is sub-optimal compression that favors snapshots with the largest size. Following
              | the work in [210], this is remedied by partitioning the heterogeneous snapshot matrix X into homo-
              | geneous snapshot matrices Y and Z according to X = [Y , Z]. Each homogeneous snapshot matrix
              | is optimally compressed using POD and the results are combined via concatenation to yield the
              | reduced-order basis, i.e., Φ = [POD(Y ), POD(Z)]. This algorithm, denoted Φ = PODH(Y , Z), is sum-
              | marized in Algorithm 5 and includes a final step that employs a QR factorization to orthogonalize the
              | basis. An alternate approach, known as Compact Proper Orthogonal Decomposition [32], to remove
              | the potential bias of POD, specifically when states and sensitivity snapshot are used, weights the
              | sensitivity snapshots according to the magnitude of parameter perturbations. The former approach
              | based on compression of homogeneous submatrices is preferred in this work due to its generality
              | in handling any types of snapshots, flexibility in handling more than two types of snapshots, and
              | optimality in compressing individual snapshot types (since the compression is POD-based).
blank         | 
text          | Algorithm 5 Proper Orthogonal Decomposition for Heterogeneous Data
blank         | 
text          |                                           Φ = PODH(Y , Z)
blank         |                                                   
text          | Input: Heterogeneous snapshot matrix, X = Y Z and truncation sizes ky and kz
              | Output: Reduced-order basis Φ
              |  1: Compute the thin SVD of Y : Y = UY ΣY VYT
              |  2: Compute the thin SVD of Z: Z = UZ ΣZ VZT
              |  3: Form matrix of dominant singular vectors      
              |  4: W = (uY )1 · · · (uY )ky (uZ )1 · · · (uZ )kz
              |  5: Orthogonalize columns of W via QR factorization, ΦR = W
blank         | 
              | 
text          |    Another desirable property that POD does not possess is the exact preservation of a particular
              | subset snapshots in the span of the reduced basis. The interpolation property of minimum-residual
              | reduced-order models motivates such a property. For some µ ∈ RNµ , suppose X = [u(µ), X2 ]
              | where u(µ) is the exact solution of the high-dimensional model and X2 contains other snapshots.
              | If u(µ) ∈ span(Φ), the resulting (minimum-residual) reduced-order model will exactly recover this
              | solution (Proposition 4.1). This property will prove particularly important in Chapters 5–6, where
              | a certain level of accuracy is required at trust region centers. However, if POD is applied to X,
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        108
blank         | 
              | 
              | 
text          | u(µ) 6∈ span(Φ), in general, even if u(µ) ∈ span(X). To enhance POD to exactly preserve a
              | subsets of the snapshots in the reduced subspace, consider the decomposition of the snapshots as
              | X = [X1 , X2 ], where X1 contains the snapshots to be preserved. POD compression is applied
              | only to the snapshot matrix X2 and the reduced-order basis is defined as Φ = [X1 , POD(X2 )]. This
              | algorithm, denoted Φ = PODSP(X1 , X2 ), is summarized in Algorithm 6 and includes a final step
              | that employs a QR factorization to orthogonalize the basis.
blank         | 
text          | Algorithm 6 Proper Orthogonal Decomposition with Span Preservation
blank         | 
text          |                                      Φ = PODSP(X1 , X2 )
              |                                  Nu ×ks
blank         |                                                        
text          | Input: Snapshot matrix X ∈ R        where X = X1 X2 and truncation size kx
              | Output: Reduced-order basis Φ such that span X1 ⊂ span Φ                
              |  1: Compute the thin SVD of X2 : X2 = U ΣV T , where U = u1 u2 · · · uks
blank         |                            
text          |  2: W = X1 u1 · · · ukx
              |  3: Orthogonalize columns of W via QR factorization, ΦR = W
blank         | 
              | 
text          |    In many cases, heterogeneous snapshots are encountered and certain subsets of each homoge-
              | neous snapshot collection must be preserved in the span of the basis. For example, the minimum-
              | residual sensitivity reduced-order models of Section 4.1.2 exactly recover the exact sensitivities at
              |                                                                     ∂u
              | a parameter configuration µ ∈ RNµ if u(µ) ∈ span(Φ) and                 (µ) ∈ span(Φ). In this situa-
              |                                                                     ∂µ
              | tion, it is desirable to utilize both state and sensitivity snapshots and preserve the exact state and
              | sensitivity corresponding to parameter configuration µ. This will have important implications in
              | the context of the trust region method introduced in Chapter 5 that requires a certain level of
              | accuracy, in both the objective and gradient, at trust region centers. In such situations, it is de-
              | sirable to combine the basic enhancements to POD introduced in Algorithms 5 and 6. For this
              | purpose, decompose the heterogeneous snapshot matrix X into homogeneous snapshot matrices
              | Y and Z. Further decompose these snapshot matrices according to the subset that must be pre-
              | served in the reduced subspace, i.e., Y = [Y1 , Y2 ] and Z = [Z1 , Z2 ] where the columns of Y1 and
              | Z1 must be contained in the span of Φ. This yields the decomposition of the original snapshot
              | matrix as X = [Y1 , Y2 , Z1 , Z2 ] and the basis is defined via POD-based compression to Y2 and
              | Z2 only, i.e., Φ = [Y1 , Z1 , POD(Y2 ), POD(Z2 )] = [Y1 , Z1 , PODH(Y2 , Z2 )]. This algorithm, denoted
              | Φ = PODHSP(Y1 , Y2 , Z1 , Z2 ), is summarized in Algorithm 7 and includes a final step that employs
              | a QR factorization to orthogonalize the basis.
              |    At the core of POD, and all the variants introduced in this section, lies a singular value decom-
              | position, which remains among the most expensive matrix factorizations. In many large-scale PDE
              | applications, particularly time-dependent applications, the snapshot matrix that is passed to POD
              | for compression may have O(108 ) rows and O(103 ) columns, which requires a substantial amount of
              | computational resources and will be extremely time- and memory-intensive. To reduce the burden
              | of the large-scale SVD computation, a low-rank approximation of the singular value decomposition
              | [81] will be employed for the large-scale CFD problems encountered in Section 5.5.4. The random-
              | ized, low-rank SVD, summarized in Algorithm 8, computes the standard SVD of original matrix
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                         109
blank         | 
              | 
              | 
text          | Algorithm 7 Proper Orthogonal Decomposition for Heterogeneous Data with Span Preservation
blank         | 
text          |                                     Φ = PODHSP(Y1 , Y2 , Z1 , Z2 )
blank         |                                                                                                 
text          | Input: Heterogeneous snapshot matrix X = Y Z , where Y = Y1 Y2 and Z = Z1                          Z2 ,
              |     and truncation sizes, ky and kz
              | Output: Reduced-order basis Φ such that span Y1 ⊂ span Φ and span Z1 ⊂ span Φ
              |  1: Compute the thin SVD of Y2 : Y2 = UY ΣY VYT
              |  2: Compute the thin SVD of Z2 : Z2 = UZ ΣZ VZT
              |  3: Form matrix of dominant singular vectors
blank         |                                                                         
text          |                      W = Y1 Z1 (uY )1 · · · (uY )ky (uZ )1 · · · (uZ )kz
blank         | 
text          |  4:   Orthogonalize columns of W via QR factorization, ΦR = W
blank         | 
              | 
text          | projected into a low-dimensional subspace that is constructed through random linear combinations
              | of the matrix. Since a SVD computation scales linearly with the number of rows and quadratically
              | in the number of columns, a substantial performance improvement comes with performing the SVD
              | in the reduced space.
              |       Another bottleneck encountered with all variants of POD is that even low-rank modifications to
              | the underlying snapshots, in general, requires re-computing the SVD from scratch. This is significant
              | since appending new snapshots to the snapshot matrix or re-centering the snapshot matrix cannot
              | necessarily re-use the previous singular factors. A series of papers by Brand [25, 26] changed this
              | landscape as they introduced a series of algorithms for low-rank updates to the SVD. This algorithm,
              | summarized in Algorithm 9 for the case of appending new snapshots to the snapshot matrix and
              | Algorithm 10 for the case for re-centering the snapshot matrix, enables the singular factors of the
              | original SVD to be re-used to compute the SVD of the low-rank update to the snapshot matrix. The
              | cost is mostly independent of operations that scale with the size of the original snapshot matrix.
blank         | 
title         | Algorithm 8 Low-Rank Probabilistic SVD Approximation
blank         | 
text          |                                         U , Σ, V = ProbSVD(X, k, q)
              | Input: A ∈ Rm×n (usually n  m), approximation rank k, and number of power iterations q
              | Output: Approximate SVD of A ≈ UΣVT
              |  1: Generate n × 2k Gaussian test matrix Ω
              |  2: Form Y = (AAT )q AΩ
              |  3: Compute QR factorization of Y: Y = QR
              |  4: Form B = QT A
              |  5: Compute SVD of B = ŨΣVT
              |  6: Set U = QŨ
blank         | 
              | 
text          |       This completes the discussion of the algorithms that will prove useful in defining the trial basis
              | Φ from snapshot data. Chapters 5–6 will provide specific training methods that collect snapshots
              | according to the requirements of the trust region-based optimization algorithm. This section closes
              | with a brief note on the construction of the mask P when collocation-based hyperreduction is
              | employed. The sample mesh P̄ will not be discussed since it is determined uniquely from the mask,
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                          110
blank         | 
              | 
              | 
              | 
text          | Algorithm 9 Brand’s Algorithm for low-rank SVD updates: appending vector
blank         | 
text          |                            Ū , Σ̄, V̄ = BrandAppendSVD(U , Σ, V , Y )
              | Input: Data matrix X ∈ Rm×n of rank r, thin SVD of data matrix X = U ΣV T , and full-rank
              |     matrix of vectors Y ∈ Rm×k .                
              | Output: SVD of updated data matrix: X Y = Ū Σ̄V̄ T
              |  1: Compute M = U T Y ∈ Rr×k
              |  2: Compute P̄ = Y − U M ∈ Rm×k
              |  3: Compute QR  decomposition
              |                                 of P̄ = P RA , where P ∈ Rm×k , RA ∈ Rk×k
              |                 Σ M
              |  4: Form K =               ∈ R(r+k)×(r+k)
              |                  0 RA
              |  5: Compute SVD of K = CSD T , where C, S, D ∈ R(r+k)×(r+k)
              |  6:                                                                   
              |                                                                 V 0
              |                           Ū = U P C            Σ̄ = S      V̄ =         D
              |                                                                    0 I
blank         | 
              | 
              | 
              | 
text          | Algorithm 10 Brand’s Algorithm for low-rank SVD updates: translating columns
blank         | 
text          |                           Ū , Σ̄, V̄ = BrandTranslateSVD(U , Σ, V , a)
              | Input: Data matrix X ∈ Rm×n of rank r, thin SVD of data matrix X = U ΣV T , and desired
              |     translation vector, a ∈ Rm
              | Output: SVD of updated data matrix: X + a1T = Ū Σ̄V̄ T
              |  1: Compute n = V T 1, q = 1 − V n, q = kqk2 , and Q = 1q q
              |  2: Compute m = U T a ∈ Rr
              |  3: Compute p = a − U m ∈ Rm
              |  4: Define r̂ ∈ R and v ∈ RN such that: p = r̂v where kvk2 = 1
              |                  Σ + mnT qm
blank         |                                 
text          |  5: Form K =                       ∈ R(r+1)×(r+1)
              |                      r̂n     r̂q
              |  6: Compute SVD of K = CSD T , where C, S, D ∈ R(r+1)×(r+1)
              |  7:                                                               
              |                        Ū = U    v C        Σ̄ = S        V̄ = V    Q D
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        111
blank         | 
              | 
              | 
text          | as discussed in Section 4.2.2. The mask is constructed according to the DEIM algorithm introduced
              | in [41] and generalized in [31, 198]. The variant introduced in [198] will be employed in this work
              | due to its proven robustness in handling vector-valued PDE solutions where the variables in each
              | component have different scales and the flexibility afforded by injecting expert knowledge, which
              | proved crucial in large-scale CFD applications [198].
blank         | 
              | 
title         | 4.4     Summary
text          | With all of the ingredients for efficient and optimal projection-based model reduction introduced in
              | Sections 4.1–4.3, this section provides an overview of the overall framework and makes important
              | connections between the various components that will be leveraged in Chapters 5–6. The general
              | form of projection-based reduced-order models was introduced in (4.3)
blank         | 
text          |                                find ur ∈ Rku such that ΨT r(Φur , µ) = 0.                      (4.130)
blank         | 
text          | It is uniquely defined by a trial basis Φ and test basis Ψ, which may be chosen arbitrarily and
              | independently, in general. In order to ensure the reduced-order model possesses the minimum-
              | residual property, the test and trial basis must be related to one another and the optimality metric
              | Θ according to (4.14), i.e.,
              |                                                      ∂r
              |                                        Ψ(u, µ) = Θ      (u, µ)Φ.                               (4.131)
              |                                                      ∂u
              | The minimum-residual property is a desirable since it guarantees the approximation generated by
              | the reduced-order model monotonically improves (in terms of the residual norm in some metric)
              | as the trial basis is expanded and exactly reconstructs training data. These properties are known
              | as monotonicity and interpolation (Proposition 4.1). The interpolation property requires u(µ) ∈
              | col(Φ), where u(µ) is the solution of r( · , µ) = 0, which will be guaranteed using the span-preserving
              | variant of POD (PODSP in Algorithm 6). However, it will not be efficient or even practical to
              | require the reduced-order model be interpolatory at every µ ∈ RNµ . Instead, n interpolation points
              | {µ1 , . . . , µn } are selected and a snapshot matrix is constructed as
              |                                          h                      i
              |                                       X = u(µ1 ) · · ·    u(µn ) .                             (4.132)
blank         | 
text          | Additionally, let X 0 be any collection of primal snapshot to be used to construct the trial basis
              | whose columns will not necessarily be preserved in the span of the trial space. The trial basis is
              | then constructed as
              |                                           Φ = PODSP(X, X 0 ).                                  (4.133)
blank         | 
text          | In Chapter 5, X will consist of the high-dimensional snapshot at the trust region center, i.e., u(µk ),
              | since conditions (3.14) and (3.15) require a prescribed level of accuracy at the center.
              |    With the primal reduced-order model constructed, the sensitivity or adjoint methods introduced
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                              112
blank         | 
              | 
              | 
text          | in Sections 4.1.2–4.1.3 can be used to derive the gradients of any reduced quantities of interest. How-
              | ever, it was shown that for general minimum-residual reduced-order models, these computations will
              | require second derivatives of the governing residual r, which are expensive to compute and rarely
              | available in large-scale PDE implementations. An alternative that will break discrete consistency,
              | i.e., the computed gradient of the reduced quantity of interest will not match the true gradient of
              | the quantity, is to employ minimum-residual reduced-order models directly for the high-dimensional
              | sensitivity and adjoint equations. Despite breaking discrete consistency, these quantities are com-
              | putable since they do not require second derivatives of r and also possess the minimum-residual
              | properties of monotonicity and interpolation.
              |     The minimum-residual sensitivities, defined in (4.28), are uniquely defined through the specifi-
              | cation of a sensitivity basis Φ∂ and optimality metric Θ∂ , i.e.,
              |                                       T            d            T
              |          ∂u
              |          dr
              |                ku ×Nµ             ∂r ∂      ∂   ∂r ∂ ∂u  r      ∂r ∂       ∂r
              |     find    ∈R        such that      Φ    Θ        Φ       =−      Φ    Θ∂                           (4.134)
              |          ∂µ                       ∂u            ∂u     ∂µ       ∂u         ∂µ
blank         | 
text          | where all terms are evaluated at the (reconstructed) solution of the primal reduced-order model
              | Φur (µ; Φ, Ψ). Proposition 4.3 guarantees these two choices for the reduced sensitivities match
              | when the primal solution is exact or the test basis is constant, provided the relationships in (4.35)
              | hold, i.e.,
              |                                               Φ∂ = Φ
              |                                                        ∂r                                            (4.135)
              |                                        Ψ(u, µ) = Θ∂       (u, µ)Φ∂ ,
              |                                                        ∂u
              | which will be enforced in the remainder. As with the primal ROM, the minimum-residual property is
              | desirable since it ensures the reduced sensitivity model is monotonic and interpolatory. Interpolation
              |          ∂u                        ∂u
              | requires     (µ) ∈ col(Φ), where      (µ) is the solution of r ∂ (u(µ), · , µ) = 0 and the requirement
              |          ∂µ                        ∂µ
              | Φ∂ = Φ has been imposed. This condition, along with the requirement for interpolation of the
              | primal solution (u(µ) ∈ col(Φ)), will be enforced using the heterogeneous span-preserving variant
              | of POD. Define the sensitivity snapshot matrix
blank         |                                                                     
text          |                                          ∂u                  ∂u
              |                                      Y =    (µ ) · · ·          (µ ),                                (4.136)
              |                                          ∂µ 1                ∂µ n
blank         | 
text          | where {µ1 , . . . , µn } are the interpolation points previously defined, and let Y 0 be any other collection
              | of sensitivity snapshots. Then, the trial basis is defined according to
blank         | 
text          |                                       Φ = PODHSP(X, X 0 , Y , Y 0 ).                                 (4.137)
blank         | 
text          | This guarantees the primal and sensitivity reduced-order models will be interpolatory if they both
              |                                                                    ∂u
              | possess the minimum-residual property since u(µi ) ∈ col(Φ) and       (µ ) ∈ col(Φ). In Chapter 5, X
              |                                                                    ∂µ i
              | and Y will consist of the high-dimensional primal and sensitivity snapshots at the trust region center,
              |                  ∂u
              | i.e., u(µk ) and    (µ ), since conditions (3.14) and (3.15) require a prescribed level of accuracy at
              |                  ∂µ k
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                      113
blank         | 
              | 
              | 
text          | the center.
              |     The minimum-residual adjoint reduced-order model, defined in (4.56), is uniquely defined through
              | the specification of an adjoint basis Φλ and optimality metric Θλ , i.e.,
              |                                                  !T                     !                    !T
              |                                         ∂r T λ            λ    ∂r T λ               ∂r T λ             ∂f T
              |         find λ̂r ∈ R   ku
              |                             such that       Φ         Θ            Φ        λ̂r =       Φ         Θλ          (4.138)
              |                                         ∂u                     ∂u                   ∂u                 ∂u
blank         | 
text          | where all terms are evaluated at the (reconstructed) solution of the primal reduced-order model
              | Φur (µ; Φ, Ψ). Proposition 4.5 guarantees these two choices for the reduced adjoints match when
              | the primal solution is exact or the test basis is constant, provided the relationships in (4.63) hold,
              | i.e.,
              |                                                                −1
              |                                                      ∂r
              |                                         Φλ = Ψ = Θ λ    (u, µ)T     Φ                                         (4.139)
              |                                                      ∂u
              | which will be enforced in the remainder. As with the primal ROM, the minimum-residual property
              | is desirable since it ensures the reduced adjoint model is monotonic and interpolatory. Interpolation
              | requires λ(µ) ∈ col(Ψ) where λ(µ) is the solution of r λ (u(µ), · , µ) = 0 and the requirement
              | Φλ = Ψ has been imposed. Due to the relationship between Ψ, Φλ , Φ, and Θλ imposed in (4.63)
              | from Proposition 4.5, the following equivalence holds
blank         | 
text          |                                                               ∂r
              |                                  λ(µ) ∈ col(Ψ) ⇐⇒ Θλ             (u(µ), µ)T λ(µ) ∈ col(Φ)                     (4.140)
              |                                                               ∂u
blank         | 
text          | This condition, along with the requirement for interpolation of the primal solution (u(µ) ∈ col(Φ)),
              | will be enforced using the heterogeneous span-preserving variant of POD to construct the trial basis
              | Φ. Define the (modified) adjoint snapshot matrix
blank         |                                                                                              
text          |                         ∂r       λ                                     ∂r
              |                                                                         λ
              |                    Z= Θ    (u(µ1 ), µ1 )T λ(µ1 ) · · ·               Θ                 T
              |                                                                           (u(µn ), µn ) λ(µn )                (4.141)
              |                         ∂u                                             ∂u
blank         | 
text          | where {µ1 , . . . , µn } are the interpolation points previously defined, and let Z 0 be any other collection
              | of (modified) adjoint snapshots. Then, the trial basis is defined according to
blank         | 
text          |                                           Φ = PODHSP(X, X 0 , Z, Z 0 ).                                       (4.142)
blank         | 
text          | This guarantees the primal and adjoint reduced-order models will be interpolatory if they both
              | possess the minimum-residual property since u(µi ) ∈ col(Φ) and λ(µi ) ∈ col(Ψ). In Chapter 5, X
              | and Y will consist of the high-dimensional primal and adjoint snapshots at the trust region center,
              | i.e., u(µk ) and λ(µk ), since conditions (3.14) and (3.15) require a prescribed level of accuracy at
              | the center.
              |     There may be cases where it is desirable for the reduced-order model to be monotonic and
              | interpolatory in the primal, sensitivity, and adjoint states. The logical extension of the previous de-
              | velopment employs a minimum-residual reduced-order model for the primal, sensitivity, and adjoint
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                      114
blank         | 
              | 
              | 
text          | and defines the trial basis according to
blank         | 
text          |                                 Φ = PODHSP(X, X 0 , Y , Y 0 , Z, Z 0 )                       (4.143)
blank         | 
text          | where the above is the obvious extension in Algorithm 7 to three types of snapshots.
              |    Before closing this section, the abstract discussion regarding minimum-residual reduced-order
              | models is made concrete by considering the special case of a Galerkin and LSPG projection. Reduced-
              | order models based on a Galerkin projection take the test basis to be the same as the trial basis
blank         | 
text          |                                               Ψ(u, µ) = Φ
blank         | 
text          | for any u ∈ RNu and µ ∈ RNµ , rendering the test basis constant and immediately qualifying such
              | reduced-order models for the results of Propositions 4.3 and 4.5. Galerkin reduced-order models
              | possess the minimum-residual property in the metric
blank         | 
text          |                                            ∂r
              |                                   Θ=          (Φur (µ; Φ, Φ), µ)−T ,                         (4.144)
              |                                            ∂u
blank         | 
text          | provided the PDE Jacobian is symmetric, positive definite. Given this relation between test and
              | trial basis and requirement that the PDE Jacobian is SPD, the minimum-residual sensitivity and
              | adjoint reduced-order models follow from the choices
blank         | 
text          |                                                         ∂r
              |                      Φ∂ = Φλ = Φ           Θ∂ = Θ λ =      (Φur (µ; Φ, Φ), µ)−T              (4.145)
              |                                                         ∂u
blank         | 
text          | This relations also ensure (4.35) and (4.63) of Propositions 4.3 and 4.5 are satisfied, which implies
              | the true Galerkin sensitivities and adjoint match the minimum-residual counterparts. In contrast,
              | reduced-order models based on the Least-Squares Petrov-Galerkin projection take the test basis
              | according to
              |                                                      ∂r
              |                                        Ψ(u, µ) =        (u, µ)Φ                              (4.146)
              |                                                      ∂u
              | for any u ∈ RNu and µ ∈ RNµ , resulting in a non-constant test basis and the results of Proposi-
              | tions 4.3 and 4.5 will only hold when the primal solution of the reduced-order model is exact, i.e.,
              | u(µ) = Φur (µ; Φ, Ψ). The LSPG reduced-order model possesses the minimum-residual property
              | by construction in the metric Θ = I and therefore applies in the most general case, i.e., without
              | requiring SPD Jacobians. Given this relationship between the test and trial basis and the following
              | requirements from Propositions 4.3 and 4.5
blank         | 
text          |                                        Φ∂ = Φ         Φλ = Ψ,                                (4.147)
blank         | 
meta          | the minimum-residual sensitivity and adjoint reduced-order models for the LSPG projection follow
              | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                       115
blank         | 
              | 
              | 
text          | from the choices                               "        #−1
              |                                              ∂r T ∂r
              |                            Θ∂ = I       Θλ =                                   .              (4.148)
              |                                              ∂u ∂u
              |                                                           (Φur (µ; Φ, Ψ), µ)
blank         | 
text          | The above relationship satisfy all conditions in Propositions 4.3 and 4.5, thereby ensuring the true
              | and minimum-residual sensitivities and adjoints agree when the primal solution is exact.
              |    Most of the developments detailed in this section extend to the case where collocation-based
              | hyperreduced models are used in place of the pure projection-based reduced-order models. In par-
              | ticular, the relationship between the various bases and optimality metrics, when imposed only on
              | the hyperreduction mask, lead to a weaker form of the minimum-residual property, i.e., the masked
              | minimum-residual property (Definition 4.2). This ultimately leads to a weaker form of monotonicity
              | and interpolation that only holds under stricter assumptions on solutions of the discrete PDE. In this
              | work, the mask P is constructed solely from the primal reduced-order basis Φ and problem-specific
              | information following the approach in [198].
title         | Chapter 5
blank         | 
title         | Optimization via Model Reduction
              | and Residual-Based Trust Regions
blank         | 
text          | With the globally convergent, multifidelity trust region method introduced in Chapter 3 and projection-
              | based reduced-order models introduced in Chapter 4, these technologies are combined to yield an
              | efficient algorithm for deterministic PDE-constrained optimization. The approximation model will
              | be taken as the quantity of interest evaluated at the reconstructed reduced-order model solution and
              | residual-based error bounds (Appendix B) will define the objective and gradient error bounds that
              | are required for global convergence of the multifidelity trust region method. In addition to exploiting
              | inexpensive reduced-order (hyperreduced) models in the trust region subproblem, the flexible mul-
              | tifidelity trust region framework of Section 3.1.1 allows for several other opportunities for efficiency.
              | First, the objective accuracy condition (3.14), restated here for convenience,
blank         | 
text          |                                    ϑk (µk ) ≤ κϑ ∆k      κϑ ∈ (0, 1),
blank         | 
text          | implies the reduced-order model does not need to be exact at the trust region centers. This is
              | exploited by using partially converged solutions to build the reduced-order basis. Similarly, the
              | gradient accuracy condition (3.15)
blank         | 
text          |                            ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }         κϕ > 0
blank         | 
text          | allows for the use of partially converged sensitivity or adjoint snapshots. Partially converged pri-
              | mal and dual solutions can substantially reduce the burden of collecting snapshots, particularly in
              | large-scale applications encountered in computational fluid dynamics that require slowly converging
              | nonlinear solvers such as pseudo-transient continuation [104, 105] for robust convergence behavior.
              | Partially converged primal solutions are also used to efficiently evaluate the performance of a trust
              | region subproblem using the concepts outlined in Section 3.1.1. Sections 5.2 and 5.3 detail the use
blank         | 
              | 
              | 
meta          |                                                   116
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      117
blank         | 
              | 
              | 
text          | of partially converged solutions for these purposes. While most of this chapter focuses on approxi-
              | mation models based on projection-based reduced-order models without hyperreduction, Section 5.4
              | discusses the extension to collocation-based hyperreduced models. Finally, Section 5.5 provides sev-
              | eral numerical examples from various computational mechanics disciplines, including the large-scale
              | industrial demonstration of shape optimization of a full aircraft configuration, to study the proposed
              | approach.
blank         | 
              | 
title         | 5.1     Residual-Based Trust Region Method
text          | Consider the fully discrete partial differential equation r(u, µ) = 0, where u ∈ RNu is the state
              | vector and µ ∈ RNµ are the design or control parameters and f : RNu × RNµ → R is a quantity of
              | interest to be optimized. The reduced-space approach to PDE-constrained optimization (Section 2.3)
              | considers the optimization problem
              |                                            minimize F (µ),                                          (5.1)
              |                                              µ∈RNµ
blank         | 
text          | where F (µ) = f (u(µ), µ) and u(µ) is the solution of r( · , µ) = 0. Due to the large expense associ-
              | ated with the evaluation of F (µ) and ∇F (µ), the multifidelity trust region method and projection-
              | based model reduction techniques are combined to efficiently solve (5.1). The multifidelity trust
              | region method of Chapter 3 was completely specified in terms of the approximation model mk (µ),
              | the objective error indicator ϑk (µ) that satisfies (3.12), the gradient error indicator ϕk (µ) that sat-
              | isfies (3.13), and the inexact objective model ψk (µ) and error indicator θk (µ) that satisfy (3.21).
              | Therefore the focus of this section is the specification of these functions using projection-based
              | reduced-order (hyperreduced) models and error indicators from Chapter 4 and Appendix B, respec-
              | tively. From the overview of the multifidelity trust region method provided in Section 3.1.1, there
              | are two other critical pieces required to fully prescribe the method such that global convergence is
              | guaranteed: a trust region subproblem solver that ensures the fraction of Cauchy decrease (A.9)
              | is obtained and a refinement mechanism for mk (µ), ψk (µ) and the associated error indicators to
              | ensure the error conditions (3.14), (3.15), (3.22) are met. Due to the significant cost separation
              | between reduced-order (hyperreduced) models and the high-dimensional model, the trust region
              | subproblem is solved exactly to guarantee the FCD is satisfied. The error conditions will be met
              | through construction of the reduced-order basis, which will be detailed in Section 5.1.2.
blank         | 
              | 
title         | 5.1.1      Multifidelity Trust Region Ingredients
text          | At the kth iteration, the approximation model based on projection-based reduced-order models takes
              | the form
              |                                   mk (µ) = f (Φk ur (µ; Φk , Ψk ), µ),                              (5.2)
blank         | 
text          | where Φk is the reduced-order basis used at the kth iteration of the trust region method—details
              | pertaining to the construction of Φk will be deferred to Section 5.1.2 as they will be intimately
              | linked to the error conditions in (3.14), (3.15) and therefore the global convergence theory—and
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     118
blank         | 
              | 
              | 
text          | ur (µ; Φk , Ψk ) is the solution of the reduced-order model
blank         | 
text          |                                           ΨTk r(Φk ur , µ) = 0
blank         | 
text          | with test basis Ψk . The test basis is chosen according to (4.14) to ensure the reduced-order model
              | possesses the minimum-residual property, which in turn ensures it is monotonic and interpolatory.
              | The gradient of the model in (5.2) will be computed using the exact reduced sensitivity or adjoint
              | method if the test basis is constant. This ensures the gradients can be computed without requiring
              | second derivatives of r and will be consistent with the corresponding function. The minimum-
              | residual variants will be used if the test basis is not constant; however, the requirements between
              | the sensitivity/adjoint basis and optimality metric in (4.35) and (4.63) will be enforced to ensure
              | the minimum-residual gradients match the true gradients at any point where the primal reduced-
              | order model solution is exact. In Section 5.1.2, the reduced-order basis Φk will be constructed such
              | that the primal and sensitivity/adjoint reduced-order model is exact at the trust region center µk .
              | Following this discussion, the construction of the trial basis Φk and selection of minimum-residual
              | optimality metrics is sufficient to completely define the remaining ingredients of the reduced-order
              | model, i.e., the test basis Ψk , sensitivity basis Φ∂k = Φk , and adjoint basis Φλ
              |                                                                                  k = Ψk .
              |    There are two natural choices for the trust region constraint function. The first is the standard
              | Euclidean distance
              |                                           ϑk (µ) := kµ − µk k
blank         | 
text          | which leads to a traditional trust region algorithm and recovers a method similar to the original Trust
              | Region Proper Orthogonal Decomposition (TRPOD) method [10]. As discussed in Section 3.1.1, this
              | choice automatically satisfies requirements in (3.12) and (3.14), provided a gradient error indicator
              | ϕk (µ) is chosen that satisfies (3.13) and (3.15). Another choice for the trust region constraint that
              | was proposed in the author’s previous research [210], and earlier in [208] in the context of linear
              | PDEs, is the norm of the residual evaluated at the reconstructed ROM solution
blank         | 
text          |              ϑk (µ) := kr(Φk ur (µk ; Φk , Ψk ), µk )kΘk + kr(Φk ur (µ; Φk , Ψk ), µ)kΘk ,         (5.3)
blank         | 
text          | which leads to an error-aware trust region. With this choice of ϑk (µ), the bound in (3.12) is verified
              | as follows
blank         | 
text          |              |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ |F (µk ) − mk (µk )| + |F (µ) − mk (µ)|
              |                                                   ≤ ζ (kr(u, µk )k + kr(u, µ)k)
blank         |                                                                                       
text          |                                                   ≤ ζ̂ kr(u, µk )kΘk + kr(u, µ)kΘk
              |                                                   = ζ̂ϑk (µ),
blank         | 
text          |                                                                                    −1/2
              | where u = Φk ur (µk ; Φk , Ψk ), ζ > 0 is an arbitrary constant, and ζ̂ = Θk              ζ is a related
              | constant. The second inequality uses Lemma B.4 that bounds errors in quantities of interest by
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                              119
blank         | 
              | 
              | 
text          |                                                                                           −1/2
              | the primal residual norm and the third inequality invokes the identity kxk = Θk                  x        and the
              |                                                                                                      Θk
              | triangle inequality (or simply norm equivalence). The function used for the gradient error indicator
              | is also a residual-based quantity, but the specific form depends on whether the sensitivity or adjoint
              | method is employed in the gradient computation.
blank         | 
text          | Remark. Some of the optimality metrics— Θ, Θ∂ , Θλ — introduced in this document are parameter-
              | dependent. This is not an issue in Chapter 4 since the residual minimization problem was only posed
              | over the state space for a fixed parameter. In the context of PDE-constrained optimization, the met-
              | ric must be valid over the entire state and parameter space. Therefore, all parameter-dependent
              | metrics are fixed at the trust region center µk . For a Galerkin projection, the optimality metrics
              | become
              |                                      ∂r
              |                                Θk =     (Φk ur (µk ; Φk , Φk ), µk )−T
              |                                      ∂u
              |                                      ∂r
              |                                Θ∂k =    (Φk ur (µk ; Φk , Φk ), µk )−T
              |                                      ∂u
              |                                      ∂r
              |                                Θλ
              |                                 k =     (Φk ur (µk ; Φk , Φk ), µk )−T .
              |                                      ∂u
              | For a LSPG projection, the optimality metrics become
blank         | 
text          |                                Θk = I
              |                                Θ∂k = I
              |                                      "             #−1
              |                                     ∂r T ∂r
              |                                Θλ
              |                                 k =                                                   .
              |                                     ∂u ∂u
              |                                                        (Φk ur (µk ; Φk , Ψk ), µk )
blank         | 
              | 
text          |    In this work, the minimum-residual sensitivity and adjoint models are used to compute (approx-
              | imate) gradients of the projection-based reduced-order model that comprises the approximation
              | model. For the sensitivity method, the approximate gradient, denoted ∇m
              |                                                                      [k (µ), is computed ac-
              | cording to                                                                   !
              |                                                        ∂u
              |                                                        dr
              |                           ∇m
              |                           [k (µ) = g    ∂
              |                                             u,   Φ∂k           ∂    ∂
              |                                                           (µ; Φk , Θk , u), µ ,                             (5.4)
              |                                                        ∂µ
blank         | 
text          |        ∂u
              |        dr
              | where       is the solution of the minimum-residual sensitivity reduced-order model in (4.28). For
              |        ∂µ
              | the adjoint method, the approximate gradient is computed according to
blank         | 
text          |                             [k (µ) = g λ (u, Φλ
              |                             ∇m                         λ    λ
              |                                               k λ̂(µ; Φk , Θk , u), µ).                                     (5.5)
blank         | 
text          | where λ̂r is the solution of the minimum-residual adjoint reduced-order model in (4.56). In both of
              | the above expressions, u = Φk ur (µ; Φk , Ψk ) is the solution of the primal reduced-order model. The
              | relationships in (4.35) and (4.63) between Φ, Ψ, Φ∂ , Φλ , Θ∂k , and Θλ
              |                                                                       k are employed to guarantee,
              | by Propositions 4.3 and 4.5, that ∇m[k (µ) = ∇mk (µ) whenever Ψk is constant, i.e., a Galerkin
              | projection, or the primal solution is exact. Unfortunately, unless one of these criteria is satisfied
              | ∇m
              | [k (µ) 6= ∇mk (µ), which may cause convergence issues for the trust region subproblem. To ensure
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                   120
blank         | 
              | 
              | 
text          | these subproblems terminate, a maximum number of iterations is imposed on the solver, which may
              | slightly slow convergence of the overall trust region method. Once the optimization algorithm reaches
              | the vicinity of a local minima (of F (µ)), the reduced-order model is sufficiently accurate at (and
              | near) the new trust region center and the primal reduced-order model solution will be sufficiently
              | accurate that the two gradients closely match.
              |    For sensitivity-based gradient computations, the gradient error indicator is
              |                                                                                         !
              |                                                                   ∂u
              |                                                                   dr
              |               ϕk (µ) := α1 kr(u, µ)kΘk + α2 r     ∂
              |                                                        u,   Φ∂k           ∂    ∂
              |                                                                      (µ; Φk , Θk , u), µ         (5.6)
              |                                                                   ∂µ
              |                                                                                             Θ∂
              |                                                                                              k
blank         | 
              | 
              | 
text          |                                                                       ∂u
              |                                                                       dr
              | where u = Φk ur (µ; Φk , Ψk ) is the reconstructed primal solution and     is the solution of the
              |                                                                       ∂µ
              | minimum-residual reduced sensitivity equations (4.28). The primal trial basis Φk is also used as
              | the sensitivity basis Φ∂k to ensure the minimum-residual sensitivities agree with the true reduced-
              | order model sensitivities when the primal solution is exact or Ψk is constant (Proposition 4.3). For
              | adjoint-based gradient computations, the gradient error indicator is
blank         |                                                                              
text          |                ϕk (µ) := α1 kr(u, µ)kΘk + α2 r λ u, Φλ          λ    λ
              |                                                      k λ̂r (µ; Φk , Θk , u), µ                   (5.7)
              |                                                                                             Θλ
              |                                                                                              k
blank         | 
              | 
              | 
text          | where u = Φk ur (µ; Φk , Ψk ) is the reconstructed primal solution and λ̂r is the solution of the
              | minimum-residual reduced adjoint equations (4.56). The primal test basis Ψk is used as the adjoint
              | basis Φλ
              |        k to ensure the minimum-residual adjoints agree with the true reduced-order model adjoints
              | when the primal solution is exact or Ψk is constant (Proposition 4.5). In (5.6) and (5.7), α1 , α2 > 0
              | are user-defined constants intended to balance the contribution of the primal and dual residuals.
              | From Lemma B.7 and B.8, there exists a constant ξ > 0 such that
blank         | 
text          |                                   ∇F (µk ) − ∇m
              |                                              [k (µk ) ≤ ξϕk (µk ),                               (5.8)
blank         | 
text          | holds regardless of the values of α1 and α2 (provided they are positive) for both the sensitivity and
              | adjoint form of the gradient error indicator. An error bound of this form is a critical ingredient in
              | the global convergence theory of the proposed trust region method, as well as in related methods
              | [93, 108].
blank         | 
text          | Remark. The objective decrease condition (3.14) introduced in the proposed generalized trust region
              | method is considerably weaker than the conditions required for previous methods. The work by
              | Alexander introduced a trust region framework to manage the use of general approximation models to
              | solve constrained and unconstrained optimization problems with expensive optimization functionals
              | [4, 6, 5]. The trust region model management framework required the approximation model possess
              | first-order consistency at trust region centers
blank         | 
text          |                              mk (µk ) = F (µk )       ∇mk (µk ) = ∇F (µk ).                      (5.9)
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      121
blank         | 
              | 
              | 
text          | The Trust Region Proper Orthogonal Decomposition (TRPOD) method introduced in [10] and studied
              | extensively thereafter [57, 170, 186], removed the zeroth-order condition entirely and weakened the
              | first-order condition by replacing it with the Carter condition (see discussion to follow). Unlike the
              | present work, TRPOD strictly employed a tradition trust region constraint of the form kµ − µk k ≤
              | ∆k . The work in [208] generalized the TRPOD method to an error-aware trust region that required
              | a pointwise error bound on the objective at the trust region center (3.8), which is a considerably
              | stronger requirement than the objective decrease condition (3.14), as discussed in Chapter 3.
blank         | 
text          | Remark. The gradient condition (3.15) leveraged in the proposed generalized trust region method
              | was originally proposed in [93] and extensively used in [108, 109, 92, 166]. It is substantially more
              | flexible than the Carter condition [35]
blank         | 
text          |                         k∇F (µk ) − ∇mk (µk )k ≤ η k∇mk (µk )k             η ∈ (0, 1)              (5.10)
blank         | 
text          | that was used in the original TRPOD method [10] and a related method proposed that uses generalized
              | trust regions [208]. Global convergence is predicated on construction of a model that satisfies this
              | bound with any value of η that satisfies 0 < η < 1. Since global convergence relies critically on
              | value of η being in this range, it does not permit the use of error indicators since they are only
              | bounds when multiplied by an arbitrary constant. Therefore, ∇F (µk ) must be computed along with
              | ∇mk (µk ) corresponding to an increasingly refined basis until (5.10) is met.
blank         | 
text          |    An opportunity for efficiency afforded by the flexible trust region framework introduced in Sec-
              | tion 3.1.1 is the use of an approximation model to compute the ratio of actual-to-predicted ratio,
              | ρk . The true expression for ρk can be replaced with
blank         | 
text          |                                                   ψ(µk ) − ψ(µ̂k )
              |                                           ρk =                                                     (5.11)
              |                                                  mk (µk ) − mk (µ̂k )
blank         | 
text          | where ψk : RNµ → R is an approximation model that satisfies
blank         | 
text          |                 |F (µk ) − F (µ̂k ) + ψk (µ̂k ) − ψk (µk )| ≤ σθk (µ̂k )
              |                                                                                                    (5.12)
              |                                                   θkω (µ̂k ) ≤ η min{mk (µk ) − mk (µ̂k ), rk },
blank         | 
text          | without destroying global convergence of the overall algorithm. In (5.12), σ > 0 is an arbitrary
              | constant, rk → 0 is a forcing sequence, η < min{η1 , 1 − η2 }, and 0 < η1 < η2 < 1 and ω ∈ (0, 1)
              | are algorithmic constant. The error bound in (5.12) is identical to the required relationship between
              | mk (µ) and ϑk (µ) in (3.12). Thus, a natural and efficient choice is
blank         | 
text          |                               ψk (µ) = mk (µ)         and        θk (µ) = ϑk (µ),                  (5.13)
blank         | 
text          | which implies
              |                                   ψ(µk ) − ψ(µ̂k )      mk (µk ) − mk (µ̂k )
              |                           ρk =                        =                      = 1.                  (5.14)
              |                                  mk (µk ) − mk (µ̂k )   mk (µk ) − mk (µ̂k )
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      122
blank         | 
              | 
              | 
title         | Therefore, if the error condition
blank         | 
text          |                           ϑω           ω
              |                            k (µ̂k ) = θk (µ̂k ) ≤ η min{mk (µk ) − mk (µ̂k ), rk }                (5.15)
blank         | 
text          | is satisfied for the reduced-order basis used during the kth iteration, the step can automatically
              | be accepted and trust region radius increased without referring to the high-dimensional model. If
              | this condition is not satisfied, the exact expression for ratio of actual-to-predicted reduction (3.9) is
              | used, i.e., ψk (µ) = F (µ) and θk (µ) = 0. Section 5.3 introduces another choice for ψk and θk that
              | leverages partially converged solutions for enhanced efficiency.
              |     The choice of objective and gradient error indicators in (5.3) and (5.6), (5.7) provides a strong
              | connection to the minimum-residual theory of Chapter 4 since the norms are taken to exactly coincide
              | with the optimality metrics defining the minimum-residual reduced-order model. As a result, the
              | optimality property and monotonicity hold (Propositions 4.1, 4.2, 4.4). Optimality implies that
blank         | 
text          |                                       ϑk (µk ) ≤ 2 kr(Φk x, µk )kΘk                               (5.16)
blank         | 
text          | for any x ∈ Rku . A similar statement holds for ϕk (µ)
blank         |                                                                                   
text          |           ϕk (µk ) ≤ α1 kr(Φk y, µk )kΘk + α2 r ∂ Φk ur (µk ; Φk , Ψk ), Φ∂k w, µk
              |                                                                                        Θ∂
              |                                                                                         k
              |                                                                                                 (5.17)
              |           ϕk (µk ) ≤ α1 kr(Φk y, µk )kΘk + α2 r λ Φk ur (µk ; Φk , Ψk ), Φλk z, µk
              |                                                                                        Θλ
              |                                                                                         k
blank         | 
              | 
              | 
text          | for any y, z ∈ Rku and w ∈ Rku ×Nµ . Notice that this bound requires the sensitivity and adjoint
              | residual to be defined (linearized) about the primal reduced-order model solution, i.e., Φk ukr (µ).
              | Monotonicity means that hierarchically refining Φk can only reduce ϑ(µ), provided Θ is independent
              | of Φk . The same statement does not hold for ϕk (µ) since monotonicity, as defined in Proposition 4.2,
              | 4.4 requires linearization about a fixed primal solution and hierarchically refining either Φ∂k or Φλ
              |                                                                                                     k.
              | Given the relation between Φk , Φ∂k , and Φλ                                                   ∂
              |                                            k in (4.35) and (4.63), it is impossible to refine Φk or
              | Φλ
              |  k without also modifying Φk and therefore changing the primal reduced-order model solution (the
              | linearization point). For these reasons, the choice of ϑk (µ) and ϕk (µ) in (5.3) and (5.6), (5.7) are
              | highly desirable. On the other hand, these norms may be difficult to compute if the metric requires
              | computation of the Jacobian of r or its inverse, which will be the case for Galerkin reduced-order
              | models. In such cases, it is desirable to simply use the I-norm to define all terms in ϑk (µ) and
              | ϕk (µ), i.e.,
blank         | 
text          |    ϑk (µ) := kr(Φk ur (µk ; Φk , Ψk ), µk )k + kr(Φk ur (µ; Φk , Ψk ), µ)k
blank         |                                                                                               
text          |                                                     ∂                      ∂ur
              |    ϕk (µ) := α1 kr(Φk ur (µ; Φk , Ψk ), µ)k + α2 r Φk ur (µ; Φk , Ψ), Φk       (µ; Φk , Ψk ), µ
              |                                                                            ∂µ
              |    ϕk (µ) := α1 kr(Φk ur (µ; Φk , Ψk ), µ)k + α2 r λ (Φk ur (µ; Φk , Ψk ), Ψk λr (µ; Φk , Ψk ), µ)
              |                                                                                                   (5.18)
              | since the norms are trivial to evaluate given the corresponding residual. Fortunately for the case
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      123
blank         | 
              | 
              | 
text          | of LSPG-based reduced-order models, the primal and sensitivity optimality metrics are taken as
              | identity (Sections 4.1.1 and 4.1.2) and the definitions of ϑk (µ) and ϕk (µ) in (5.3)-(5.7) and (5.18)
              | agree. However, even when LSPG is used, the adjoint optimality metric is not the identity matrix
              | (Section 4.1.3). Furthermore, note that the interpolation property of minimum-residual reduced-
              | order models does not depend on the metric used in the residual norm, due to the equivalence
              | of norms in finite dimensions, i.e., for the definition of ϑk (µ) in (5.3) or (5.18), ϑk (µ) = 0 if
              | u(µ) ∈ col(Φk ), and similarly for ϕk (µ). This implies that the choice of ϑk (µ) and ϕk (µ) still
              | possesses this critical property that will be used in the next section. Finally, due to the equivalence
              | of norms in finite dimensions, the bounds in (3.12) and (3.13) will still hold (with different constants)
              | and therefore the choice of ϑk (µ) and ϕk (µ) in (5.18) will not destroy global convergence.
blank         | 
              | 
title         | 5.1.2    Basis Construction via Proper Orthogonal Decomposition and the
text          |          Method of Snapshots
              | The use of reduced-order models in the context of optimization has predominantly employed an
              | offline-online procedure [17, 149, 173] where expensive operations involving the HDM are performed
              | in the offline phase to build the reduced-order basis Φ, i.e., train the reduced-order model, and
              | the inexpensive reduced-order model is employed in the online optimization phase. A number of
              | drawbacks to this approach exist, the most critical ones being that global convergence can only
              | be established for relatively simple partial differential equations and it is difficult to train a robust
              | ROM in a high-dimensional parameter space. TRPOD [10] was among the first methods to break
              | the offline-online barrier and guarantee global convergence in a general setting. In TRPOD and the
              | many variants to follow [57, 1, 186], the reduced-order basis is constructed during the optimization
              | procedure such that conditions on the objective and gradient accuracy at trust region centers are
              | met, thereby avoiding the issue of sampling in possibly high-dimensional parameter spaces. This is
              | also the approach taken here.
              |    For the remainder of this chapter, only the residual-based constraint function is considered. From
              | the previous section, the choice of the reduced-order model approximation mk (µ) and residual-based
              | error indicators ϑk (µ) and ϕk (µ) satisfy the error bounds in (3.12), (3.13), regardless of the choice
              | of reduced-order basis. However, the accuracy criterion in (3.14) and (3.15)
blank         | 
text          |                                  ϑk (µk ) ≤ κϑ ∆k
              |                                                                                                   (5.19)
              |                                  ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }
blank         | 
text          | depend critically on the choice of reduced-order basis.
              |    At each iteration k, the reduced-order bases Φk , Φ∂k , Φλ
              |                                                             k are constructed to ensure
blank         | 
text          |                                           ∂u
              |                    u(µk ) ∈ col(Φk )         (µ ) ∈ col(Φ∂k )     λ(µk ) ∈ col(Φλ
              |                                                                                 k ).              (5.20)
              |                                           ∂µ k
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                          124
blank         | 
              | 
              | 
text          | The interpolation property of minimum-residual reduced-order models ensures the reconstructed so-
              | lutions exactly recover the high-dimensional counterparts. In turn, this ensures ϑk (µk ) = ϕk (µk ) = 0
              | and therefore the error conditions (3.14), (3.15) are trivially satisfied and global convergence is
              | guaranteed. As discussed in the previous section, the sensitivity and adjoint bases are chosen in
              | accordance with Proposition 4.3 and 4.5, i.e., Φ∂k = Φk and Φλ
              |                                                              k = Ψk , and the requirements in
              | (5.20) reduce to
blank         | 
text          |                                            ∂u
              |                      u(µk ) ∈ col(Φk )        (µ ) ∈ col(Φk )        λ(µk ) ∈ col(Ψk ).               (5.21)
              |                                            ∂µ k
blank         | 
text          | The condition between the test basis Ψk and adjoint optimality metric Θλ required in Proposition 4.5
              | reduces (5.21) to
blank         | 
text          |                           ∂u                                         ∂r
              |    u(µk ) ∈ col(Φk )         (µ ) ∈ col(Φk )      Θλ (u(µk ), µk )      (u(µk ), µk )T λ(µk ) ∈ col(Φk ).
              |                           ∂µ k                                       ∂u
              |                                                                                                      (5.22)
blank         | 
text          | Remark. In the case of a Galerkin projection (for problems with SPD Jacobians) with adjoint
              |                        ∂r
              | optimality metric Θλ =    (Φur (µ; Φ, Φ))−T , the adjoint snapshots reduce to
              |                        ∂u
blank         | 
text          |                                               ∂r
              |                                Θλ (u(µ), µ)      (u(µ), µ)T λ(µ) = λ(µ).
              |                                               ∂u
              |                                                                                "          #−1
              |                                                                     ∂r T ∂r
              |                                                                           λ
              | In the case of a LSPG projection with adjoint optimality metric Θ =                                             ,
              |                                                                     ∂u ∂u
              |                                                                                            (Φur (µ; Φ, Ψ), µ)
              | the adjoint snapshots reduce to
blank         | 
text          |                                       ∂r                   ∂r
              |                        Θλ (u(µ), µ)      (u(µ), µ)T λ(µ) =    (u(µ), µ)−1 λ(µ).
              |                                       ∂u                   ∂u
blank         | 
text          |     The above requirements reveal the nature of the snapshots that should be used in the construction
              | of the trial basis Φk . In practice, sensitivities and adjoints are rarely required simultaneously.
              | Usually the sensitivity method is employed when the number of constraints is larger than the number
              | of optimization variables and vice versa for the adjoint method. To generalize the notation such
              | that the sensitivity method and adjoint method can be considered simultaneously, define v(µ) as the
              | sensitivity or adjoint state, depending on which method is used to compute gradients of quantities
              | of interest, i.e.,                     
              |                                         ∂u (µ) sensitivity method
              |                                        
              |                                  v(µ) = ∂µ                                                            (5.23)
              |                                        λ(µ)
              |                                        
              |                                                 adjoint method
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     125
blank         | 
              | 
              | 
text          | and let v̂(µ) denote the corresponding snapshot, i.e.,
blank         | 
text          |                              ∂u
              |                            
              |                            
              |                                (µ)                         sensitivity method
              |                     v̂(µ) = ∂µ                                                                   (5.24)
              |                            Θλ (u(µ), µ) ∂r (u(µ), µ)T λ(µ) adjoint method.
              |                            
              |                                          ∂u
blank         | 
text          | With this notation, the requirements in (5.22) are weakened to
blank         | 
text          |                                   u(µk ) ∈ col(Φk )     v̂(µk ) ∈ col(Φk )                       (5.25)
blank         | 
text          | while still guaranteeing ϑk (µk ) = ϕk (µk ) = 0 where it is understood that ϕk (µ) corresponds to (5.6)
              | if the sensitivity method is employed and (5.7) for the adjoint method. The conditions in (3.14)
              | and (3.15) will be guaranteed using the heterogeneous span-preserving variant of POD (Section 4.3).
              | Define snapshot matrices at iteration k consisting of u(µ) and v̂(µ) at the trust region centers of
              | all previous iterations, i.e.,             h                       i
              |                                       Uk = u(µ0 ) · · · u(µk−1 )
              |                                            h                       i                             (5.26)
              |                                       V̂k = v̂(µ0 ) · · · v̂(µk−1 ) .
blank         | 
text          | and define the reduced-order basis as
blank         | 
text          |                                   Φk = PODHSP(u(µk ), Uk , v̂(µk ), V̂k ).                       (5.27)
blank         | 
text          | where PODHSP is defined in Algorithm 7. By construction, the conditions in (3.14) and (3.15) are
              | satisfied since u(µk ) and v̂(µk ) are preserved in the columnspace of Φk , which implies ϑk (µk ) =
              | ϕk (µk ) = 0 and global convergence is guaranteed. Even though the in the information in Uk and
              | Vk is not necessarily useful in satisfying the trust region error conditions, i.e., the information in
              | u(µk ) and v(µk ) is sufficient to do so, it provides the reduced-order model with additional fidelity,
              | which is useful in improving its robustness away from µk .
blank         | 
text          | Remark. There may be instances where the reduced-order basis defined at iteration k−1 is sufficient
              | to satisfy the error conditions (3.14), (3.15) at iteration k, i.e.,
blank         | 
text          |                                  ϑk−1 (µk ) ≤ κϑ ∆k
              |                                                                                                  (5.28)
              |                                  ϕk−1 (µk ) ≤ κϕ min{k∇mk−1 (µk )k , ∆k }.
blank         | 
text          | This is likely to occur when the initial trust region radius ∆0 is chosen too small. In this situation,
              | there is no need to update the reduced-order basis and the same model and error indicators are used,
              | i.e.,
              |                    mk (µ) := mk−1 (µ)       ϑk (µ) := ϑk−1 (µ)      ϕk (µ) := ϕk−1 (µ).          (5.29)
blank         | 
text          | This choice saves queries to the expensive high-dimensional model and still guarantees global con-
              | vergence when (5.28) is satisfied.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     126
blank         | 
              | 
              | 
text          |    As written, the above approach to compute Φk requires the singular value decomposition of the
              | snapshot matrices Uk and Vk that have an increasing number of columns. This quickly becomes
              | prohibitively expensive since the cost of the SVD scales quadratically in the number of columns [75].
              | However, the snapshot matrices satisfy the simple relation
              |                                            h                   i
              |                                       Uk = Uk−1       u(µk−1 )
              |                                            h                   i                                 (5.30)
              |                                       V̂k = V̂k−1     v̂(µk−1 ) ,
blank         | 
text          | and therefore the thin SVD updates in Algorithm 9 can be used to compute the SVD of Uk from the
              | SVD of Uk−1 . Only the QR decomposition of the compressed snapshot matrices must be recomputed
              | at each iteration.
blank         | 
text          | Remark. For time-dependent problems, exact preservation of u(µk ) in the column space of Φk may
              | be unrealistic since u(µk ) corresponds to an entire time history. In this case, POD (Algorithm 4)
              | can be applied to u(µk ) and v̂(µk ) with the level of compression set such that (3.14) and (3.15) are
              | satisfied. Then the basis can be defined as
blank         | 
text          |                           Φk = PODHSP(POD(u(µk )), Uk , POD(v̂(µk )), V̂k ).
blank         | 
text          | This is similar to the original TRPOD method [10] that constructs the reduced basis according to
              | Φk = POD(u(µk )) or the extension presented in [57] that also constructs a reduced-order model for
              | the adjoint that constructs the basis according to Φvk = POD(v(µk )).
blank         | 
text          |    To close this section, global convergence of Algorithm 11 is established based on Theorem A.1.
              | Suppose Assumptions (AF1)–(AF2) and (AM1)–(AM4) (Appendix A) hold and let {µk } denote
              | the sequence of iterations produced by Algorithm 11. To apply Theorem A.1 and conclude that
              | this algorithm is globally convergent, the choice of mk (µ), ϑk (µ), ϕk (µ) in (5.2), (5.3), (5.6)-(5.7)
              | must satisfy the error bounds in (3.12), (3.13) and the conditions in (3.14), (3.15). The objective
              | and gradient error bounds are established based on the residual-based error bounds detailed in
              | Appendix B. The construction of the reduced-order basis Φk in (5.27) combined with the fact
              | that Ψk is defined according to (4.14) to ensure the reduced-order model possesses the minimum-
              | residual property guarantees the objective (3.14) and gradient (3.15) error conditions. Therefore,
              | by Theorem A.1, the sequences of iterates produced by Algorithm 11 satisfies
blank         | 
text          |                                        lim inf k∇F (µk )k = 0.                                   (5.31)
              |                                            k→∞
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                         127
blank         | 
              | 
              | 
text          | Algorithm 11 Residual-based trust region method based on reduced-order models
              |  1:   Initialization: Given
              |                       µ0 , U−1 = ∅, V̂−1 = ∅, ∆0 , 0 < γ < 1, ∆max > 0, 0 < η1 < η2 < 1,
              |                             0 < κϑ < 1, 0 < κϕ , 0 < ω < 1, {rk }∞
              |                                                                  k=0 such that rk → 0
blank         | 
text          |  2:   Model and constraint update: If previous model and constraint are sufficient for convergence
blank         | 
text          |                      ϑk−1 (µk ) ≤ κϑ ∆k                ϕk−1 (µk ) ≤ κϕ min{k∇mk−1 (µk )k , ∆k },
blank         | 
text          |       re-use for the current iteration: mk (µ) := mk−1 (µ) and ϑk (µ) := ϑk−1 (µ). Otherwise, evaluate
              |       primal and sensitivity or adjoint solution of high-dimensional model
              |                                                 ∂u                          ∂r
              |                 uk := u(µk )           v̂k :=      (µ ) or Θλ
              |                                                             k (u(µk ), µk )    (u(µk ), µk )T λ(µk )
              |                                                 ∂µ k                        ∂u
blank         | 
text          |       and compute reduced-order basis via span-preserving variant of POD (Algorithm 7)
blank         | 
text          |                                               Φk = PODHSP(uk , Uk , v̂k , V̂k ),
blank         | 
text          |       define model and constraint as
              |                  mk (µ) = f (Φk ur (µ; Φk , Ψk ), µ)
              |                  ϑk (µ) = kr(Φk ur (µk ; Φk , Ψk ), µk )kΘk + kr(Φk ur (µ; Φk , Ψk ), µ)kΘk ,
blank         | 
text          |       and update snapshot matrices
blank         |                                                                                       
text          |                                  Uk+1 ← Uk−1           uk         V̂k+1 ← V̂k−1       v̂k .
blank         | 
              | 
text          |  3:   Step computation: Solve (exactly) the trust region subproblem
blank         | 
text          |                                        min mk (µ)          subject to      ϑk (µ) ≤ ∆k
              |                                     µ∈RNµ
blank         | 
text          |     for a candidate, µ̂k , using interior-point method of Section 3.1.2.
              |  4: Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio
              |                      
              |                      
              |                                  1            if ϑk (µ̂k )ω ≤ η min{mk (µk ) − mk (µ̂k ), rk }
              |                ρk =       F (µk ) − F (µ̂k )
              |                      
              |                                               otherwise
              |                         mk (µk ) − mk (µ̂k )
blank         | 
text          |     where η < min{η1 , 1 − η2 }
              |  5: Step acceptance:
blank         | 
text          |                 if        ρk ≥ η1       then         µk+1 = µ̂k         else    µk+1 = µk      end if
blank         | 
              | 
text          |  6:   Trust region update:
blank         | 
text          |                      if     ρk ≤ η 1                then        ∆k+1 ∈ (0, γϑk (µ̂k )]        end if
              |                      if     ρk ∈ (η1 , η2 )         then        ∆k+1 ∈ [γϑk (µ̂k ), ∆k ]      end if
              |                      if     ρk ≥ η 2                then        ∆k+1 ∈ [∆k , ∆max ]           end if
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                       128
blank         | 
              | 
              | 
title         | 5.2       Snapshots from Partially Converged Solutions
text          | In many large-scale applications, particularly those arising in turbulent computational fluid dynam-
              | ics, it is difficult and expensive to compute a steady-state solution, and the corresponding sensitivity
              | and adjoint solutions, to tight tolerances. In these cases, the generation of snapshots in Line 2 of
              | Algorithm 11 will dominate the cost of the trust region method. To speed up this step and leverage
              | the flexibility afforded by the trust region method of Section 3.1.1, partially converged solutions are
              | used as snapshots.
              |     Let u(µ; τu ) denote a partially converged primal solution of tolerance τu , defined as any point
              | that satisfies
              |                                                   kr( · , µ)kΘ ≤ τu .                                              (5.32)
blank         | 
text          | While the fully converged solution u(µ) is assumed to be unique (Assumption 2.2), there are many
              | points satisfying (5.32) for a given τu > 0. A simple method to find a point that satisfies (5.32) is to
              | use the chosen nonlinear solver (Newton-Raphson, Gauss-Newton, pseudo-transient continuation)
              | with (5.32) used as the convergence criteria1 . Similarly, let v(µ; τu , τv ) be a partially converged
              | sensitivity or adjoint solution of tolerance τv about a partially converged primal solution of tolerance
              | τu , defined as any point satisfying
blank         | 
text          |                                            kr v (u(µ; τu ), · , µ)kΘv ≤ τv                                         (5.33)
blank         | 
text          | where r v is the sensitivity or adjoint residual, depending on which method is used to compute
              | reduced-space gradients2 . Furthermore, these definition are extended to define the partially con-
              | verged snapshot v̂(µ; τu , τv ) as
              |                         
              |                         v(µ; τu , τv )                                                sensitivity method
              |     v̂(µ; τu , τv ) =                                                                                              (5.34)
              |                         Θλ (u(µ; τ ), µ) ∂r (u(µ; τ ), µ)T v(µ; τ , τ )               adjoint method.
              |                                    u                u             u   v
              |                                           ∂u
blank         | 
text          | With these definitions, the snapshot matrices of partially converged solutions are defined as
              |                                     h                                       i
              |                                Uk = u(µ0 ; τu0 ) · · · u(µk−1 , τuk−1 )
              |                                     h                                                   i                          (5.35)
              |                                V̂k = v̂(µ0 ; τu0 , τv0 ) · · · v̂(µk−1 , τuk−1 , τvk−1 )
blank         | 
text          | and the reduced-order basis is constructed from these snapshots using the heterogeneous span-
              | preserving variant of POD (Algorithm 7)
blank         | 
text          |                               Φk = PODHSP(u(µk ; τuk ), Uk , v̂(µk ; τuk , τvk ), V̂k ),                           (5.36)
              |    1 This is not a restrictive requirement since residual-based convergence criteria are usually, if not always, used for
blank         | 
text          | nonlinear solvers. However, using a norm other than the 2-norm is non-standard.
              |    2 This assumes an iterative solvers is used to solve the linear sensitivity or adjoint system since a direct solver will
blank         | 
text          | always return the solution to machine precision.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    129
blank         | 
              | 
              | 
text          | where τuk and τvk are iteration-dependent tolerances. This definition of Φk guarantees that the
              | partially converged primal and sensitivity/adjoint solutions are contained in the reduced subspace
              | to the exact accuracy at which they were computed, i.e.,
blank         | 
text          |                             u(µk ; τuk ) ∈ col(Φk )     v̂(µk ; τuk , τvk ) ∈ col(Φk ),          (5.37)
blank         | 
text          | which in turn implies
blank         | 
text          |                      ∂u
              |                         (µ ; τ k , τ k ) ∈ col(Φk )     or      λ(µk ; τuk , τvk ) ∈ col(Ψk ),   (5.38)
              |                      ∂µ k u v
blank         | 
text          | assuming the conditions in Propositions 4.3 or 4.5 are satisfied. If a minimum-residual primal
              | reduced-order model with optimality metric Θ is used, the optimality property gives
blank         | 
text          |                          kr(Φk ur (µk ; Φk , Ψk )kΘ ≤ r(u(µk ; τuk ), µk )       Θ
              |                                                                                      ≤ τuk       (5.39)
blank         | 
text          | The first inequality holds from the optimality property (Proposition 4.1) since u(µk ; τuk ) ∈ col(Φk )
              | and the second inequality holds from the definition of the partially converged solution in (5.32). For
              | the residual-based error indicators (5.3), (5.6), and (5.7), this implies
blank         | 
text          |                                            ϑk (µk ) ≤ 2τuk
              |                                                                                                  (5.40)
              |                                            ϕk (µk ) ≤ α1 τuk + α2 τvk ,
blank         | 
text          | provided minimum-residual reduced-order models are used. Therefore the objective error condition
              | (3.14) is satisfied if
              |                                                τuk ≤ (1/2)κϑ ∆k                                  (5.41)
blank         | 
text          | holds and the gradient error condition (3.15) is satisfied if
blank         | 
text          |                                      α1 τuk ≤ κϕ min{k∇mk (µk )k , ∆k }
              |                                                                                                  (5.42)
              |                                      α2 τvk ≤ κϕ min{k∇mk (µk )k , ∆k }
blank         | 
text          | holds. These bounds are combined to yield the following requirement on τuk and τvk
blank         | 
text          |                                   τuk ≤ (1/α1 )κϕ min{k∇mk (µk )k , κ∆k }
              |                                                                                                  (5.43)
              |                                   τvk ≤ (1/α2 )κϕ min{k∇mk (µk )k , ∆k },
blank         | 
text          | where κ = min{1, α1 κϑ /(2κϕ )}. Since this condition ensures (3.12)-(3.15), global convergence of the
              | resulting trust region method is guaranteed. The relationship in (5.43), and the results that follow,
              | only hold in the Θ-norm (Proposition 4.1) so the I-norm form of the residual-based error indicators
              | in (5.18) cannot be used, unless Θ = I (LSPG).
blank         | 
meta          | Remark. The value of τuk and τvk depend on k∇mk (µk )k, which in turn depends on the values of
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                         130
blank         | 
              | 
              | 
text          | τuk and τvk used to define snapshots for Φk . Therefore, the values of τuk and τvk cannot be simply
              | determined from (5.43). Instead, an iterative method is employed that begins initially selects large
              | values τuk and τvk and systematically reduces them, i.e., via backtracking, until the conditions in
              | (3.14), (3.15) are satisfied. This will lead to an efficient algorithm since the partially converged
              | primal and dual solutions for a given τuk and τvk can be used to warm-start the nonlinear solvers for
              | any smaller values for these tolerances. An alternate approach replaces the gradient condition in
              | (3.15) with
              |                                ϕk (µk ) ≤ κϕ min{ ∇mk−1 (µk−1 ) , ∆k }.                              (5.44)
blank         | 
text          | It can be verified that this will preserve the convergence result in Theorem A.1 of Appendix A. This
              | replaces the the gradient condition in (5.43) with
blank         | 
text          |                                       1
              |                                τuk ≤     κϕ min{ ∇mk−1 (µk−1 ) , κ∆k }
              |                                      2α1
              |                                                                                                      (5.45)
              |                                       1
              |                                τvk ≤     κϕ min{ ∇mk−1 (µk−1 ) , ∆k }.
              |                                      2α2
blank         | 
text          | This alternate gradient condition preserves global convergence and allows for the direct computation
              | of τuk and τvk since all terms on the right-hand side of the inequality are independent of τuk and τvk .
blank         | 
text          | Remark. The case with the traditional trust region constraint, ϑk (µ) = kµ − µk k, satisfies ϑk (µk ) =
              | 0 trivially and therefore the lighter restrictions on τuk and τvk in (5.42) can be used in place of those
              | in (5.43).
blank         | 
              | 
title         | 5.3      Efficient Trust Region Assessment with Partially Con-
              |          verged Solutions
text          | Another opportunity for efficiency afforded by the flexible trust region framework introduced in
              | Section 3.1.1, that has not been fully leveraged in the residual-based reduced-order model trust
              | region method of this chapter, is the use of an approximation model to compute the ratio of actual-to-
              | predicted ratio, ρk . Section 3.1.1 outlined the use of this flexibility to effectively skip the computation
              | of the actual-to-predicted reduction ratio by taking
blank         | 
text          |                               ψk (µ) = mk (µ)       and      θk (µ) = ϑk (µ)
blank         | 
text          | whenever ϑk (µ̂k )ω ≤ η min{mk (µk ) − mk (µ̂k ), rk }. In this situation, the approximation to the
              | actual-to-predicted reduction ratio is always unity and the step is accepted and the radius increased.
              | This is implies the trust region assessment step is effectively free since it does not require a query
              | to F (µ) and is guaranteed to preserve global convergence since it conforms to (3.21), (3.22). In
              | Section 5.1, the true value of ρk is computed (3.9) when ϑk (µ̂k ) fails to satisfy the above bound.
              | This section seeks to improve on this using the approximate form of ρk in (3.20) where ψk (µ)
              | leverages partially converged solutions.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                        131
blank         | 
              | 
              | 
text          |    In the event that the error condition in (3.22) is not satisfied, the choice ψk (µ) = mk (µ) is
              | not sufficient to ensure convergence. Instead, partially converged solutions are used as they can
              | substantially less expensive to compute than fully converged ones and can be tailored to exactly
              | meet the error criteria in (3.22). Consider the objective model ψk (µ) defined by the quantity of
              | interest evaluated at a partially converged steady state and the corresponding residual-based error
              | indicator
              |                          ψk (µ) = f (u(u; τ̂uk ), µ)
              |                                                                                                     (5.46)
              |                          θk (µ) = r(u(µk ; τ̂uk ), µk )      Θ
              |                                                                + r(u(µ; τ̂uk ), µ)   Θ
              |                                                                                        .
blank         | 
text          | Unlike in the previous section where partially converged solutions are used as snapshots, the Θ-norm
              | above can be freely replaced with the I-norm for simplicity (and computational efficiency), provided
              | partially converged solutions are defined with respect to the I-norm. Either norm can be used in
              | this case since the optimality property of minimum-residual ROMs (Proposition 4.1) is not required
              | as it was in the previous section. However, it is desirable to use the same norm in both cases since
              | the computation of u(µ̂k ; τ̂uk ), required to compute ψk (µ̂k ), will provide a better warm-start for the
              | snapshot computation u(µk ; τuk+1 ) at iteration k + 1.
              |    With these choices, the bound in (3.21) holds from an identical argument to that in (5.43). From
              | the definition of u(µ; τ̂uk ) in Section 5.2, the following relation holds
blank         | 
text          |                                                  θk (µ̂k ) ≤ 2τ̂uk .                                (5.47)
blank         | 
text          | Therefore, the accuracy condition in (3.22) holds provided
blank         | 
text          |                                         1                                   1/ω
              |                                τ̂uk ≤     [η min{mk (µk ) − mk (µ̂k ), rk }]                        (5.48)
              |                                         2
blank         | 
text          | and global convergence is ensured. In addition to being a less expensive option than fully converged
              | evaluations of F (µ), this method fits seamlessly with the use of partially converged solutions in the
              | snapshot computations of the previous section. When an iteration is accepted, i.e., µk+1 = µ̂k , the
              | partially converged solution u(µk+1 , τ̂uk ) = u(µ̂k , τ̂uk ) can be used to warm-start the computation
              | of u(µk+1 , τuk+1 ) that is required to compute snapshots for iteration k + 1. In fact, if τ̂uk ≤ τuk+1 the
              | computation can be skipped entirely since u(µ̂k , τ̂uk ) already satisfies
blank         | 
text          |                                          r(u(µ̂k , τ̂uk ), µk+1 ) ≤ τuk+1 .                         (5.49)
blank         | 
text          | The complete algorithm that uses partially converged solutions as snapshots in the model update
              | and in the computation of the actual-to-predicted reduction ratio is provided in Algorithm 12. To
              | establish global convergence of this algorithm based on Theorem A.1, suppose Assumptions (AF1)–
              | (AF2) and (AM1)–(AM4) (Appendix A) hold and let {µk } denote the sequence of iterates produced
              | by Algorithm 12. Section 5.1.1 already established that the choice of mk (µ), ϑk (µ), ϕk (µ) satisfy
              | the error bounds in (3.14), (3.15). The construction of the reduced-order basis Φk in (5.27) and the
              | requirements placed on the partially converged solutions in (5.43), combined with the fact that Ψk
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    132
blank         | 
              | 
              | 
text          | is defined according to (4.14) to ensure the reduced-order model possesses the minimum-residual
              | property guarantees the objective (3.14) and gradient (3.15) error conditions hold. Finally, ψk (µ)
              | and θk (µ) in (5.46) must satisfy the error bound (3.21) and condition (3.22) to preserve global
              | convergence when the approximate actual-to-predicted ratio is used to assess the trust region step.
              | The residual-based error bounds established in Lemma B.4, B.7, B.8 ensures the error bound holds.
              | The requirements on the partially converged solution in (5.47)-(5.48) ensure the error condition (3.22)
              | holds. Therefore, by Theorem A.1, the sequences of iterates produced by Algorithm 12 satisfies
blank         | 
text          |                                        lim inf k∇F (µk )k = 0.                                  (5.50)
              |                                           k→∞
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                 133
blank         | 
              | 
              | 
              | 
text          | Algorithm 12 Residual-based trust region method based on reduced-order models and partially
              | converged solutions
              |  1:   Initialization: Given
              |                      µ0 , U−1 = ∅, V̂−1 = ∅, ∆0 , 0 < γ < 1, ∆max > 0, 0 < η1 < η2 < 1,
              |                            0 < κϑ < 1, 0 < κϕ , 0 < ω < 1, {rk }∞
              |                                                                 k=0 such that rk → 0
blank         | 
text          |  2:   Model and constraint update: If previous model and constraint are sufficient for convergence
blank         | 
text          |                      ϑk−1 (µk ) ≤ κϑ ∆k             ϕk−1 (µk ) ≤ κϕ min{k∇mk−1 (µk )k , ∆k },
blank         | 
text          |       re-use for the current iteration: mk (µ) := mk−1 (µ) and ϑk (µ) := ϑk−1 (µ). Otherwise, evaluate
              |       primal and sensitivity or adjoint solution of high-dimensional model to tolerances τuk and τvk ,
              |       respectively,
blank         | 
text          |               uk := u(µk ; τuk )
              |                      ∂u                                            ∂r
              |               v̂k :=    (µ ; τ k , τ k ) or Θλ (u(µk ; τuk ), µk )    (u(µk ; τuk ), µk )T λ(µk ; τuk , τvk )
              |                      ∂µ k u v                                      ∂u
              |       with tolerances given by
              |                              1
              |                       τuk ≤     κϕ min{k∇mk (µk )k , κ∆k }             κ = min{1, α1 κϑ /(2κϕ )}
              |                             2α1
              |                              1
              |                       τvk ≤     κϕ min{k∇mk (µk )k , ∆k },
              |                             2α2
              |       and compute reduced-order basis via span-preserving variant of POD (Algorithm 7)
blank         | 
text          |                                           Φk = PODHSP(uk , Uk , v̂k , V̂k ),
blank         | 
text          |       define model and constraint as
              |                    mk (µ) = f (Φk ur (µ; Φk , Ψk ), µ)
              |                     ϑk (µ) = kr(Φk ur (µk ; Φk , Ψk ), µk )kΘ + kr(Φk ur (µ; Φk , Ψk ), µ)kΘ ,
blank         | 
text          |       and update snapshot matrices
blank         |                                                                                     
text          |                                 Uk+1 ← Uk−1          uk        V̂k+1 ← V̂k−1        v̂k .
blank         | 
text          |  3: Step computation: identical to Line 3 in Algorithm 11
              |  4: Computed-to-predicted reduction: Compute computed-to-predicted reduction ratio
              |                               
              |                               
              |                                          1           if ϑk (µ̂k )ω ≤ τ̂uk
              |                          ρk =     ψ (µ ) − ψk (µ̂k )
              |                                k k
              |                                                      otherwise
              |                                  mk (µk ) − mk (µ̂k )
              |                                     1                                    1/ω
              |       ψk (µ) := f (u(µ; τ̂uk ), µ)     τ̂uk =
              |                                       [η min{mk (µk ) − mk (µ̂k ), rk }]                     η < min{η1 , 1 − η2 }
              |                                     2
              |  5: Step acceptance: identical to Line 5 in Algorithm 11
              |  6: Trust region update: identical to Line 6 in Algorithm 11
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                        134
blank         | 
              | 
              | 
title         | 5.4      Extension to Hyperreduced Models
text          | To this point, projection-based reduced-order models have solely been considered as the trust region
              | approximation model. However, as discussed in Chapter 4, this will not be sufficient to realize non-
              | trivial speedups for nonlinear problems. For such problems, the computational complexity associated
              | with the evaluation of the reduced residual and Jacobian scales with the size of the original HDM
              | since they require reconstruction of the full state vector from the reduced coordinates, assembly over
              | the entire mesh, and subsequent projection onto the columnspace of the test basis. For this reason,
              | the developments in this chapter are extended to use collocation-based hyperreduced models as the
              | approximation model.
              |    The approximation model takes the same form as in the previous sections, i.e.,
blank         | 
text          |                                 mk (µ) = f (Φk ur (µ; Φk , Ψk , Pk ), µ),                           (5.51)
blank         | 
text          | with the exception that the reduced coordinates ukr (µ) are defined as the solution of the collocation-
              | based hyperreduced model
blank         | 
text          |                          (PkT Ψk )T PkT r(P̄k P̄kT Φk ur (µ; Φk , Ψk , Pk ), µ) = 0.                (5.52)
blank         | 
text          | The gradient ∇mk (µ) is computed according to the adjoint or sensitivity method presented in
              | Sections 4.2.5–4.2.6 or approximated using the minimum-residual variants. For the sake of efficiency,
              | the residual-based trust region constraint ϑk (µ) in (5.3) is replaced with the masked residual
blank         | 
text          |                                   ϑk (µ) = PkT r(P̄k P̄kT Φk ukr (µ), µ)                            (5.53)
blank         | 
text          | and the gradient error indicator ϕk (µ) is similarly replaced with its masked counterpart, i.e.,
blank         | 
text          |   ϕk (µ) = α1 PkT r(P̄k P̄kT Φk ur (µ; Φk , Ψk , Pk ), µ) +
blank         |                      
text          |                                                                 ∂ur
              |                                                                                            (5.54)
              |                 T ∂          T                              T
              |            α2 Pk r P̄k P̄k Φk ur (µ; Φk , Ψk , Pk ), P̄k P̄k Φk     (µ; Φk , Ψk , Pk ), µ .
              |                                                                 ∂µ
blank         | 
text          | Only the sensitivity method is considered since details pertaining to the hyperreduced minimum-
              | residual adjoint method is deferred to future work.
              |    For general nonlinear systems of equations r(u, µ) = 0, these choices of error indicators ϑk (µ)
              | and ϕk (µ) do not lead to the required bounds in (3.12) and (3.13) and global convergence cannot
              | be rigorously established. However, due to the concept of a stencil in the discretization of partial
              | differential equations, i.e., the fact that the ith entry of r depends on the jth entry of u for all j ∈ Si
              | (defined in Section 4.2.2), it is reasonable to expect such bounds to hold (with larger constants ζ
              | and ξ), provided the mask is sufficiently large.
              |    The details pertaining to the construction of Φk from fully (Section 5.1.2) or partially (Sec-
              | tions 5.2) converged solutions carries over to the case of collocation-based hyperreduced models;
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    135
blank         | 
              | 
              | 
text          | however, the bounds on the error indicators can no longer be guaranteed due to the introduction of
              | the mask in the reduced-order model. Once the trial basis is constructed, the mask Pk is constructed
              | according to the algorithm detailed in [198] that relies solely on Φk and possibly problem-specific
              | information. The approximation of the ratio of actual-to-predicted reduction that uses ψk (µ) and
              | θk (µ) based on partially converged solutions is not specific to the case of projection-based reduced-
              | order models and therefore trivially carries over to the hyperreduced case. The complete trust region
              | algorithm based on collocation-based hyperreduced models is identical to Algorithms 11 and 12, once
              | the step that constructs the mask Pk from Φk is added, with the above definitions of mk (µ), ϑk (µ),
              | and ϕk (µ).
blank         | 
              | 
title         | 5.5     Numerical Experiments
text          | In this section, the error-aware trust region method using projection-based reduced-order models as
              | the approximation model is applied to solve a number of problems in computational fluid dynamics,
              | ranging from optimal control of the 1D inviscid Burgers’ equation to shape optimization of a full
              | aircraft configuration.
blank         | 
              | 
title         | 5.5.1    Optimal Control of 1D Inviscid Burgers’ Equation
text          | This section presents a thorough investigation of the trust region methods proposed in this chap-
              | ter based on the various projection-based reduced-order models of Chapter 4. The model PDE-
              | constrained optimization problem considered is optimal control of the steady, inviscid, one-dimensional
              | Burgers’ equation in only a few control parameters. The optimization problem takes the form
blank         | 
text          |                                                  1
              |                                                      1
              |                                             Z
              |                                minimize                (u(µ, x) − ū(x))2 dx                    (5.55)
              |                                 µ∈Rnµ
              |                                              0       2
blank         | 
text          | where u(µ, x) is the solution of the inviscid Burgers’ equation under a specific parametrization of
              | the inflow boundary condition and control
blank         | 
text          |                               u(µ, x)∂x u(µ, x) = µ2 eµ3 x        x ∈ (0, 100)
              |                                                                                                 (5.56)
              |                                           u(µ, 0) = µ1
blank         | 
text          | and ū(x) is the target state. The PDE is discretized with a first-order, vertex-centered finite volume
              | method with 1000 vertices for a state space of dimension Nu = 999 after application of the inflow
              | boundary condition. The functional form of the control in (5.56) was made to minimize the number
              | of optimization parameters to allow the sensitivity-based approach to be included in the study. The
              | target state corresponds to the solution of the (5.56) at the target parameter configuration µ̄ =
              | (2.5, 0.02, 0.0425). Therefore, the target state is realizable since it lies within the parametrization
              | of the optimization problem and the optimal value of the objective function is 0. All methods
              | considered will start from an initial guess of µ0 = (1.0, 1.0, 0.0). Figure 5.1 shows the control
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                       136
blank         | 
              | 
              | 
text          |           1.5                                                        15
blank         | 
text          |            1                                                         10
blank         | 
              | 
              | 
              | 
text          |                                                            u(µ, x)
              | g(µ, x)
blank         | 
              | 
text          |           0.5                                                        5
blank         | 
text          |            0                                                         0
              |                 0   20    40       60     80     100                      0   20   40       60   80         100
              |                                x                                                        x
blank         | 
              | 
text          | Figure 5.1: Control (left) and corresponding solution (right) of the inviscid Burgers’ equation in
              | (5.56) at: the initial condition µ = (1.0, 1.0, 0.0) ( ), the target solution µ = (2.5, 0.02, 0.0425)
              | (    ), and solution of the baseline optimization method (     ).
blank         | 
              | 
text          | g(µ, x) and state vector u(µ, x) at the initial guess and optimal value of µ. Figure 5.2 shows the
              | contours of the objective function (after discretization) in the µ1 −µ2 plane at a slice of the parameter
              | space at µ3 = 0, with the initial condition µ0 and optimal solution µ∗ indicated.
blank         | 
title         | Trust region geometry and impact of snapshots
blank         | 
text          | Before studying the entire performance of the proposed optimization solvers on the optimal control
              | problem in (5.55), the geometry of the various trust region constraints presented in this document
              | are considered: the traditional trust region constraint
blank         | 
text          |                                                kµ − µ0 k ≤ ∆
blank         | 
text          | and the residual-based trust region constraint
blank         | 
text          |                          kr(Φur (µ0 ; Φ, Ψ), µ0 )k + kr(Φur (µ; Φ, Ψ), µ)k ≤ ∆.
blank         | 
text          | A trust region constraint based on the true error in the quantity of interest
blank         | 
text          |                                    |f (u(µ), µ) − f (Φur (µ; Φ, Ψ))| ≤ ∆
blank         | 
text          | is included in this study for illustration purposes only as it is far too expensive to use in practice.
              | The traditional trust region is purely geometric and therefore does not depend on the reduced-order
              | model, while the residual- and error-based trust regions are heavily dependent on the type of reduced-
              | order model employed and the trial subspace chosen. This section considers reduced-order models
              | based on a Galerkin and LSPG projection. Since the Jacobians of the discrete inviscid Burgers’
              | equation are not symmetric positive-definite, reduced-order models based on a Galerkin projection
              | do not necessarily possess the minimum-residual property. However, LSPG-based ROMs do possess
              | the minimum-residual property, by definition. The trial basis will be constructed in three different
              | ways following the developments in Section 5.1.2: from snapshots of the primal solution only, from
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      137
blank         | 
              | 
              | 
text          | snapshots of the primal and sensitivity solutions, and from snapshots of the primal and adjoint
              | solutions. All snapshots will be computed at the control corresponding to the initial condition of
              | the optimization problem, µ0 . That is,
blank         |                                                                                                    
text          |                                                                                          ∂u
              |   col(Φ) = span{u(µ0 )}        col(Φ) = span{u(µ0 ), λ(µ0 )}       col(Φ) = span u(µ0 ),    (µ )
              |                                                                                          ∂µ 0
blank         | 
text          | depending on which trial subspace is being considered.
              |    The trust regions for the reduced-order models based on a Galerkin projection are provided in
              | Figure 5.3 and those based on a LSPG projection are in Figure 5.4. These figures show the contours
              | of the reduced objective function f (Φur (Φ, Ψ), µ), which can be compared to the contours of
              | the true objective function f (u(µ), µ) in Figure 5.2. It can be seen that the residual-based trust
              | regions do not match the trust regions based on the true error. Even though the use of residuals as
              | a surrogate for the true error partially motivated the introduction of the error-aware trust region
              | theory in Chapter 3, it is not a requirement since the asymptotic bound (3.12) holds due to the
              | derivation in Appendix B. From Figures 5.3 and 5.4, a few more observations are made that agree
              | with the minimum-residual reduced-order model theory in Chapter 4. First, the residual-based trust
              | regions corresponding to a Galerkin ROM (Figure 5.3) are a subset of those corresponding to the
              | LSPG ROM (Figure 5.4). This is expected since LSPG minimizes the residual in the I-norm, which
              | is exactly the quantity defining the trust region. Despite the larger residual-based trust regions of
              | LSPG ROMs, the Galerkin ROMs have larger trust regions based on the true error. While non-
              | intuitive, this does not contradict the theory outlined in Chapter 4 since LSPG is only guaranteed to
              | minimize the residual over the trial subspace, not the error in a quantity of interest. Finally, for both
              | the Galerkin and LSPG ROMs, the trial subspaces built from primal states and sensitivities produce
              | larger residual- and error-based trust regions than only those that only use primal snapshots. There
              | is disagreement between the two types of reduced-order models when it comes to the use of adjoint
              | snapshots. For the Galerkin ROMs, the incorporation of adjoint snapshots improve the prediction
              | capability of the reduced-order model with respect to the quantity of interest, but have little influence
              | on the extent of the residual-based trust region. In contrast, the incorporation of adjoint snapshots
              | increases the extent of the residual-based trust region—as expected from the monotonicity property
              | of minimum-residual reduced-order models— however, they actually reduce the extent of the error-
              | based trust region. This provides some evidence that the incorporation of non-physical snapshots
              | can cause the residual minimization to produce worse solutions with respect to prediction of the
              | quantity of interest [198].
blank         | 
title         | Performance of proposed optimization solvers
blank         | 
text          | This section provides a thorough comparison of the variants of the multifidelity trust region method
              | based on reduced-order models and partially converged solutions in Algorithms 11 and 12. The
              | following aspects of the algorithms will be considered in this study:
blank         | 
text          |    • the type of reduced-order model underlying the approximation model: Galerkin and LSPG
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                           138
blank         | 
              | 
              | 
              | 
text          |                                    1.4
blank         | 
text          |                                    1.2
blank         | 
text          |                                    1.0
blank         | 
text          |                                    0.8
blank         | 
              | 
              | 
              | 
text          |                               µ2
              |                                    0.6
blank         | 
text          |                                    0.4
blank         | 
text          |                                    0.2
blank         | 
text          |                                    0.0
              |                                          0.0   0.2     0.4        0.6     0.8        1.0
              |                                                              µ1
blank         | 
              | 
text          | Figure 5.2: Contours of the objective function f (u(µ), µ) in (5.55) in the µ1 −µ2 plane corresponding
              | to a slice at µ3 = 0.0. The initial condition for the optimization problem and target solution are
              | shown with a red circle and blue square, respectively.
blank         | 
              | 
text          |       projections will be considered,
blank         | 
text          |    • the type of snapshots used to define the trial subspace: primal snapshot alone, primal and
              |       sensitivity snapshots, and primal and adjoint snapshots will be considered,
blank         | 
text          |    • the trust region constraint used to define the trust region subproblem: the traditional ball
              |       constraint and residual-based constraint (5.3) will be considered, and
blank         | 
text          |    • the optimization solver used for the trust region subproblem: the interior point method of
              |       Section 3.1.2 based on a Newton-CG solver3 will be used to exactly solve the subproblem and
              |       Steihaug-Toint CG will be used to approximately solve the subproblem (when the traditional
              |       trust region constraint is used).
blank         | 
text          | Table 5.1 summarizes the variants of Algorithms 11 and 12 considered in this section and pro-
              | vides appropriate names for convenient reference. For the Galerkin reduced-order models, the true
              | sensitivities and adjoints will be computed according to (4.20), (4.47) since this is amenable to imple-
              | mentation due to the constant test basis and will lead to consistency of the reduced functionals and
              | their gradients. For the LSPG reduced-order models, the minimum-residual sensitivity and adjoint
              | approximations in (4.28) and (4.56) will be employed (only guarantees consistency of functionals
              | and gradients at trust region centers). Finally, all numerical experiments use the following trust
              | region parameters:
blank         | 
text          |                           κϑ = 0.5        κϕ = 2.0   γ = 0.5      η1 = 0.25 η2 = 0.75
              |                                                                                                        (5.57)
              |                                          rk = 1/(k + 1)       ∆0 = 10−1         ∆max = 105 .
blank         | 
              | 
text          |   3 The interior point method based on a BFGS unconstrained solver of Algorithm 3 is replaced with a Newton-CG
              | unconstrained solver for fair comparison to the second-order Steihaug-Toint CG method.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                                               139
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   µ2
blank         | 
              | 
              | 
              | 
text          |                                                       µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   µ2
blank         | 
              | 
              | 
              | 
text          |                                                       µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   µ2
blank         | 
              | 
              | 
              | 
text          |                                                       µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           µ2
blank         | 
              | 
              | 
              | 
text          |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          | Figure 5.3: Contour of the reduced objective function f (Φur (µ; Φ, Ψ), µ) in (5.55) in the µ1 − µ2
              | plane corresponding to a slice at µ3 = 0.0. The reduced-order model employs a Galerkin projection
              | and the trial basis is constructed from: (top) the primal solution at µ0 , i.e., col(Φ) = span{u(µ0 )};
              | (middle) the primal and adjoint solution at µ0 , i.e., col(Φ)
              |                                                             = span{u(µ0      ), λ(µ0 )}; (bottom) the
              |                                                                      ∂u
              | primal and sensitivity solution at µ0 , i.e., col(Φ) = span u(µ0 ),     (µ ) . The green shaded re-
              |                                                                     ∂µ 0
              | gion indicates the areas where: (left) the Euclidean ball is bounded by 0.5, i.e., kµ − µ0 k ≤ 0.5,
              | (center) the error between the true and reduced objective function is bounded by 100, i.e.,
              | |f (u(µ), µ) − f (Φur (µ; Φ, Ψ), µ)| ≤ 100, and (right) the residual norm of the reconstructed
              | ROM solution is bounded by 10, i.e., kr(Φur (µ; Φ, Ψ), µ)k ≤ 10. The initial condition for the
              | optimization problem and target solution are shown with a red circle and blue square, respectively.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                                               140
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   µ2
blank         | 
              | 
              | 
              | 
text          |                                                       µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   µ2
blank         | 
              | 
              | 
              | 
text          |                                                       µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   µ2
blank         | 
              | 
              | 
              | 
text          |                                                       µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           µ2
blank         | 
              | 
              | 
              | 
text          |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                µ1                                                  µ1                                                  µ1
blank         | 
              | 
              | 
              | 
text          | Figure 5.4: Contour of the reduced objective function f (Φur (µ; Φ, Ψ), µ) in (5.55) in the µ1 − µ2
              | plane corresponding to a slice at µ3 = 0.0. The reduced-order model employs a LSPG projection
              | and the trial basis is constructed from: (top) the primal solution at µ0 , i.e., col(Φ) = span{u(µ0 )};
              | (middle) the primal and adjoint solution at µ0 , i.e., col(Φ)
              |                                                             = span{u(µ0      ), λ(µ0 )}; (bottom) the
              |                                                                      ∂u
              | primal and sensitivity solution at µ0 , i.e., col(Φ) = span u(µ0 ),     (µ ) . The green shaded re-
              |                                                                     ∂µ 0
              | gion indicates the areas where: (left) the Euclidean ball is bounded by 0.5, i.e., kµ − µ0 k ≤ 0.5,
              | (center) the error between the true and reduced objective function is bounded by 100, i.e.,
              | |f (u(µ), µ) − f (Φur (µ; Φ, Ψ), µ)| ≤ 100, and (right) the residual norm of the reconstructed
              | ROM solution is bounded by 10, i.e., kr(Φur (µ; Φ, Ψ), µ)k ≤ 10. The initial condition for the
              | optimization problem and target solution are shown with a red circle and blue square, respectively.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                        141
blank         | 
              | 
              | 
text          | Table 5.1: Variants of the multifidelity trust region method based on projection-based reduced-
              | order models introduced in Algorithms 11 and 12. The first three methods are not guaranteed to be
              | globally convergent since they do not necessarily satisfy the gradient condition (3.15). The methods
              | that employ the traditional trust region employ two trust region subproblem solvers: an exact
              | solver based on the interior point method in Algorithm 3 and the inexact Steihaug-Toint CG solver.
              | The methods that employ the residual-based trust region rely on the exact interior point solver
              | in Algorithm 3. The interior point solver considered in this section uses Newton-CG to solve the
              | unconstrained subproblem (instead of BFGS) for fair comparison with the second-order Steihaug-
              | Toint CG. The snapshot matrices Uk , Wk , Zk consist of state, sensitivity, and adjoint snapshots,
              | respectively, of the high-dimensional model at all previous trust region centers, i.e., µ0 , . . . , µk−1 .
blank         | 
text          |           Name            Reduced basis (Φk )             TR constraint (ϑk (µ))          TR solver
              |  prim-etr-intpt           PODSP(u(µk ), Uk )                residual-based (5.3)      Intpt Newton-CG
              |  prim-ctr-intpt           PODSP(u(µk ), Uk )                     kµ − µk k            Intpt Newton-CG
              |   prim-ctr-stcg           PODSP(u(µk ), Uk )                     kµ − µk k            Steihaug-Toint CG
              |                                        ∂u
              |  sens-etr-intpt    PODHSP(u(µk ), Uk , ∂µ (µk ), Wk )       residual-based (5.3)      Intpt Newton-CG
              |                                        ∂u
              |  sens-ctr-intpt    PODHSP(u(µk ), Uk , ∂µ (µk ), Wk )            kµ − µk k            Intpt Newton-CG
              |                                        ∂u
              |   sens-ctr-stcg    PODHSP(u(µk ), Uk , ∂µ (µk ), Wk )            kµ − µk k            Steihaug-Toint CG
              |    adj-etr-intpt    PODHSP(u(µk ), Uk , λk (µk ), Zk )      residual-based (5.3)      Intpt Newton-CG
              |    adj-ctr-intpt    PODHSP(u(µk ), Uk , λk (µk ), Zk )           kµ − µk k            Intpt Newton-CG
              |     adj-ctr-stcg    PODHSP(u(µk ), Uk , λk (µk ), Zk )           kµ − µk k            Steihaug-Toint CG
blank         | 
              | 
text          |    The convergence history of the methods in Table 5.1, in terms of the objective function and gra-
              | dient decrease, is provided in Figures 5.5 for reduced-order models that employ a Galerkin projection
              | and 5.6 for reduced-order models that employ an LSPG projection. The convergence of the baseline
              | solver, an L-BFGS linesearch method (without model reduction), is also included in the figures for
              | comparison. All of methods in Table 5.1 based on Galerkin ROMs converge to a first-order critical
              | point of tolerance at least 10−4 (9 orders of magnitude reduction from the initial control), even
              | though global convergence cannot be rigorously established for the methods that build the reduced
              | basis from only primal snapshots (‘prim-etr-intpt’, ‘prim-ctr-intpt’, ‘prim-ctr-stcg’). In contrast, the
              | methods based on LSPG ROMs that build the reduced basis from primal and adjoint snapshots
              | (‘adj-etr-intpt’, ‘adj-ctr-intpt’, ‘adj-ctr-stcg’) do not converge. These methods are supposed to be
              | globally convergent since the inclusion of adjoint snapshots ensures the error conditions (3.14) and
              | (3.15) holds. The failure of these methods is attributed to failed trust region subproblem solves that
              | results from using inconsistent gradients away from trust region centers. Figures 5.5 and 5.6 lead to
              | two more observations. First, all methods converge faster, in terms of major iterations, when exact
              | trust region subproblem solvers are used. Later in this section, the convergence rate will be assessed
              | in terms of a cost metric that accounts for the cost of each major iteration in the respective methods.
              | Second, the methods that include more information exhibit faster convergence. For example, the
              | methods that incorporate sensitivity information converge faster than those that incorporate adjoint
              | information which converge faster than those that consider solely primal snapshots. In addition to
              | the sensitivities providing more information than adjoints (there are 3 sensitivities and 1 adjoint for
              | this problem), the information is also richer since they equip the basis with first order information
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                  142
blank         | 
              | 
              | 
text          |                      104                                                                    105
blank         | 
              | 
              | 
              | 
text          |                                                                       k∇f (u(µk ), µk )k
              | f (u(µk ), µk )
              |                     10−6                                                                    100
blank         | 
              | 
text          |                    10−16                                                                   10−5
blank         | 
              | 
text          |                    10−26                                                               10−10
              |                            0     10         20         30       40                                0   10         20         30         40
              |                                       Major iterations                                                     Major iterations
blank         | 
text          | Figure 5.5: Convergence history of various optimization solvers for optimal control of the inviscid
              | Burgers’ equation when Galerkin reduced-order model defines the approximation model. Optimiza-
              | tion solvers considered: L-BFGS solver with only HDM evaluations (               ), prim-etr-intpt (     ),
              | prim-ctr-intpt (     ), prim-ctr-stcg (     ), sens-etr-intpt (    ), sens-ctr-intpt (    ), sens-ctr-stcg
              | (    ), adj-etr-intpt (    ), adj-ctr-intpt (    ), adj-ctr-stcg (    ).
blank         | 
              | 
text          | [52, 210].
              |                   The increased convergence rate, in terms of major iterations (and therefore HDM evaluations),
              | of Algorithms 11 and 12 comes at the price of a large number of ROM evaluations. Figure 5.7 shows
              | the cumulative number of primal ROM queries as a function of major iteration and a histogram
              | of the number of primal ROM evaluations at a given reduced basis size (ku ). The methods that
              | use the residual-based trust region constraint constitute more difficult trust region subproblems and
              | require more ROM evaluations than those that use a traditional trust region. The benefit of using
              | the residual-based trust region is fewer major iterations, and thus HDM queries (Figures 5.5 and
              | 5.6). Another observation is that, as expected, the inexact trust region solver (Steihaug-Toint CG)
              | requires far fewer ROM queries than the exact solver (interior point Newton-CG), at the cost of
              | additional major iterations (HDM evaluations).
              |                   To assess the speedups that can be realized by the variants of the proposed ROM-based trust
              | region methods in Table 5.1, the following simplified cost model is introduced
blank         | 
text          |                                                  C = nhp + nhs + τ −1 (nrp + nrs )                                          (5.58)
blank         | 
text          | where C is the total cost associated with a particular method in the units of equivalent number of
              | primal HDM queries, nhp is the number of primal HDM queries, nhs is the number of sensitivity
              | HDM queries, nrp is the number of primal ROM queries, nrs is the number of sensitivity ROM
              | queries, and τ is the ratio of the cost of a primal HDM query to a primal ROM query. This cost
              | model assume the cost of computing the primal HDM (ROM) solution is the same as computing all
              | three sensitivities. Under this cost model, Figure 5.8 contains the convergence rates of the various
              | algorithms as a function of cost for three values of τ : two moderate values for the expected speedup
              | of the reduced-order model (τ = 20, 50) and the asymptotic case of a free reduced-order model
              | (τ = ∞). All variants of the trust region method outperform the baseline L-BFGS method, with
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                      143
blank         | 
              | 
              | 
text          |                            104                                                                 105
blank         | 
              | 
              | 
              | 
text          |                                                                          k∇f (u(µk ), µk )k
              |  f (u(µk ), µk )
              |                           10−6                                                                 100
blank         | 
              | 
text          |                          10−16                                                                10−5
blank         | 
              | 
text          |                          10−26                                                            10−10
              |                                  0       10         20         30   40                                0   10         20         30         40
              |                                               Major iterations                                                 Major iterations
blank         | 
text          | Figure 5.6: Convergence history of various optimization solvers for optimal control of the inviscid
              | Burgers’ equation when LSPG reduced-order model defines the approximation model. Optimization
              | solvers considered: L-BFGS solver with only HDM evaluations (                ), prim-etr-intpt (    ), prim-
              | ctr-intpt (     ), prim-ctr-stcg (      ), sens-etr-intpt (   ), sens-ctr-intpt (    ), sens-ctr-stcg (    ),
              | adj-etr-intpt (      ), adj-ctr-intpt (     ), adj-ctr-stcg (   ).
blank         | 
text          |                          800
              | number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                          600                                                             400
blank         | 
text          |                          400
              |                                                                                          200
              |                          200
blank         | 
text          |                            0
              |                                                                                               0
              |                                  0   2           4      6      8                                  0            5           10
              |  number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                          103
              |                                                                                           102
              |                          102
blank         | 
text          |                                                                                           101
              |                          101
blank         | 
              | 
text          |                                  0   10       20      30       40                                     0   20       40                60
              |                                           Major iterations                                                ROM size (ku )
blank         | 
              | 
text          | Figure 5.7: Left: Cumulative number of primal ROM queries as a function of major iteration in the
              | trust region algorithm based on reduced-order models (Algorithm 11) as applied to optimal control
              | of the inviscid Burgers’ equation. Right: Histogram of the number of primal ROM queries at a
              | given basis size. Data separated into the top and bottom rows to deal with the disparate x-scales.
              | All reduced-order models use a Galerkin projection. Optimization solvers considered: prim-etr-
              | intpt (     ), prim-ctr-intpt (    ), prim-ctr-stcg (      ), sens-etr-intpt (    ), sens-ctr-intpt ( ),
              | sens-ctr-stcg (    ), adj-etr-intpt (    ), adj-ctr-intpt (     ), adj-ctr-stcg (    ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     144
blank         | 
              | 
              | 
text          | the variants based on exact trust region solvers (‘sens-etr-intpt’ and ‘sens-ctr-intpt’) outperforming
              | the inexact solver (‘sens-ctr-intpt’), even if ROM queries are only 20× faster than HDM queries.
              | Depending on the speedup of the ROM, a given value of the objective function or gradient can be
              | achieved by methods ‘sens-etr-intpt’ or ‘sens-ctr-intpt’ at roughly 10 − 50% the cost required by the
              | baseline method.
              |    The section closes with a study of the convergence behavior of the trust region method that uses
              | a residual-based trust region constraint when Algorithms 11 (fully converged solutions as snapshots
              | and for trust region assessment) and 12 (partially converged solutions as snapshots and for trust
              | region assessment) are used. Figure 5.9 contains the convergence history of the objective function
              | and approximation model at trust region centers and candidate steps. Figures 5.10 and 5.11 contain
              | the same information for the gradient and trust region constraint, respectively. From these figures,
              | the model is first-order consistent at trust region centers for Algorithm 11 (left plots) since the
              | basis is constructed with the span-preserving variant of POD (Algorithm 7) and uses fully converged
              | snapshots. This is not the case for Algorithm 12 (right plots) that uses partially converged snapshots.
              | Despite relatively poor agreement of the model and objective (and the corresponding gradients) at
              | trust region centers and candidate steps, rapid progress is made toward the optimal solution. From
              | Figure 5.11, the trust region constraints are active at early iterations of the trust region algorithm
              | and inactive later. This suggests that, as the optimal solution is approached, the reduced-order
              | model is only queried in regions of the parameter space where it is very accurate, i.e., near training
              | points. Finally, Algorithm 12 requires one additional iteration than Algorithm 11 to converge to a
              | similar tolerance. This is expected since Algorithm 12 utilizes partially converged solutions in the
              | construction of the reduced basis, an additional level of inexactness. These observations are verified
              | in Tables 5.2–5.5 that contains the convergence history of the relevant trust region quantities for the
              | variants ‘sens-etr-intpt’ and ‘sens-ctr-stcg’ of Algorithms 11 and 12.
blank         | 
              | 
title         | 5.5.2    Optimal Control of 1D Viscous Burgers’ Equation
text          | The investigation into the methods introduced in this chapter continues in this section with an
              | emphasis on problems where the number of parameters is sufficiently large that gradients must be
              | computed with the adjoint method. The model PDE-constrained optimization problem considered is
              | optimal control of the steady, viscous, one-dimensional Burgers’ equation. The optimization problem
              | takes the form
              |                                Z     1                                     1                 
              |                                           1                         α
              |                                                                         Z
              |                    minimize                 (u(µ, x) − ū(x))2 dx +             z(µ, x)2 dx       (5.59)
              |                        nµ
              |                      µ∈R          0       2                         2   0
blank         | 
              | 
text          | where u(µ, x) is the solution of the viscous Burgers’ equation with a general parametrization of the
              | control z(µ, x)
blank         | 
text          |                      −ν∂xx u(µ, x) + u(µ, x)∂x u(µ, x) = z(µ, x) x ∈ (0, 1)
              |                                                                                                   (5.60)
              |                                            u(µ, 0) = 1      u(µ, 1) = 0
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                               145
blank         | 
              | 
              | 
              | 
text          |                     104                                                                     105
blank         | 
              | 
              | 
              | 
text          |                                                                       k∇f (u(µk ), µk )k
              | f (u(µk ), µk )
blank         | 
              | 
              | 
              | 
text          |                   10−6                                                                      100
blank         | 
              | 
text          |                   10−16                                                                    10−5
blank         | 
              | 
text          |                   10−26                                                                10−10
              |                           0   20    40      60        80        100                               0   20    40      60        80        100
blank         | 
text          |                     104                                                                     105
blank         | 
              | 
              | 
              | 
text          |                                                                       k∇f (u(µk ), µk )k
              | f (u(µk ), µk )
blank         | 
              | 
              | 
              | 
text          |                   10−6                                                                      100
blank         | 
              | 
text          |                   10−16                                                                    10−5
blank         | 
              | 
text          |                   10−26                                                                10−10
              |                           0   20     40          60        80                                     0   20     40          60        80
blank         | 
text          |                     104                                                                     105
              |                                                                       k∇f (u(µk ), µk )k
              | f (u(µk ), µk )
blank         | 
              | 
              | 
              | 
text          |                   10−6                                                                      100
blank         | 
              | 
text          |                   10−16                                                                    10−5
blank         | 
              | 
text          |                   10−26                                                                10−10
              |                           0    20     40         60         80                                    0    20     40         60         80
              |                                      Cost                                                                    Cost
blank         | 
text          | Figure 5.8: Convergence of the objective function (left) and gradient (right) as a function of the cost
              | metric in (5.58) for several values of the speedup factor of the reduced-order model: τ = 20 (top row),
              | τ = 50 (middle row), τ = ∞ (bottom row) for optimal control of the inviscid Burgers’ equation. All
              | reduced-order models use a Galerkin projection. Optimization solvers considered: L-BFGS solver
              | with only HDM evaluations (         ), sens-etr-intpt (    ), sens-ctr-intpt (  ), sens-ctr-stcg (  ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     146
blank         | 
              | 
              | 
              | 
text          |    105                                                      105
blank         | 
              | 
              | 
text          | 10−10                                                    10−10
blank         | 
              | 
              | 
text          | 10−25                                                    10−25
              |          0       1         2         3         4                  0    1      2        3        4          5
              |                      Major iteration                                        Major iteration
blank         | 
              | 
text          | Figure 5.9: Convergence history of the objective quantities for optimal control of the inviscid Burgers’
              | equation using Algorithm 11 (left – fully converged solutions as snapshots and in the evaluation of
              | trust region steps) and Algorithm 12 (right – partially converged solutions as snapshots and in the
              | evaluation of trust region steps): F (µk ) (   ), F (µ̂k ) (   ), mk (µk ) (    ), mk (µ̂k ) (   ). The
              | variant ‘sens-etr-intpt’ (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based
              | reduced-order models is used. Since the approximation model in the left plot is first-order consistent
              | at trust region centers, mk (µk ) is omitted.
blank         | 
              | 
              | 
              | 
text          |    106                                                      106
blank         | 
              | 
              | 
text          |  10−3                                                     10−3
blank         | 
              | 
              | 
text          | 10−12                                                    10−12
              |          0       1         2         3         4                  0    1      2        3        4          5
              |                      Major iteration                                        Major iteration
blank         | 
              | 
text          | Figure 5.10: Convergence history of the gradient quantities for optimal control of the inviscid Burg-
              | ers’ equation using Algorithm 11 (left – fully converged solutions as snapshots and in the evaluation
              | of trust region steps) and Algorithm 12 (right – partially converged solutions as snapshots and
              | in the evaluation of trust region steps): k∇F (µk )k (       ), k∇F (µ̂k )k (   ), k∇mk (µk )k (     ),
              | k∇mk (µ̂k )k (      ). The variant ‘sens-etr-intpt’ (Table 5.1) of the multifidelity trust region algo-
              | rithm with Galerkin-based reduced-order models is used. Since the approximation model in the left
              | plot is first-order consistent at trust region centers, k∇mk (µk )k is omitted.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                   147
blank         | 
              | 
              | 
text          |           101                                                             101
blank         | 
              | 
              | 
text          |   10−6                                                                   10−6
blank         | 
              | 
              | 
text          | 10−13                                                               10−13
              |                 0         1         2         3       4                         0    1      2        3       4            5
              |                               Major iteration                                             Major iteration
blank         | 
              | 
text          | Figure 5.11: Convergence history of the constraint quantities for optimal control of the inviscid
              | Burgers’ equation using Algorithm 11 (left – fully converged solutions as snapshots and in the
              | evaluation of trust region steps) and Algorithm 12 (right – partially converged solutions as snapshots
              | and in the evaluation of trust region steps): ϑk (µk ) (      ), ϑk (µ̂k ) ( ), ∆k (   ). The variant
              | ‘sens-etr-intpt’ (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based reduced-
              | order models is used.
blank         | 
text          |            8
              |                                                                           1.5
              |            6
              |                                                                u(µ, x)
              | z(µ, x)
blank         | 
              | 
              | 
              | 
text          |                                                                            1
              |            4
              |            2                                                              0.5
              |            0                                                               0
              |                0    0.2        0.4       0.6   0.8   1                          0   0.2    0.4       0.6    0.8       1
              |                                      x                                                           x
blank         | 
              | 
text          | Figure 5.12: Control (left) and corresponding solution (right) of the viscous Burgers’ equation in
              | (5.60) at: the initial guess for the optimization problem (   ) and the optimal solution of (5.59)
              | (    ).
blank         | 
              | 
text          | and ū(x) is the target state. The viscosity is fixed at ν = 10−2 and the PDE is discretized with
              | 1000 linear finite elements for a state space of dimension Nu = 999, after application of the essential
              | boundary conditions. The target state is chosen as the constant solution ū(x) ≡ 1, which is not
              | reachable due to the boundary conditions on the PDE. The control is parametrized with 50 cubic
              | splines with clamped boundary conditions for a total of 53 optimization variables4 , i.e., Nµ = 53.
              | The control is parametrized in this way, instead of the standard approach [78, 96, 108, 109] of
              | interpolating the control using the underlying finite element shape functions to avoid a parameter
              | space whose dimension is comparable to that of the state space, i.e., Nµ = O(Nu ), as this case
              | requires special consideration (Appendix C).
              |           Even though the number of parameters does not scale with the dimension of the state space,
              | the large number of parameter (Nµ = 53) calls for the adjoint approach to compute gradients of
              |    4 The optimization variables are the value of each spline knot (the location of each knot is fixed) and the slope of
blank         | 
meta          | the curve its boundaries.
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                      148
blank         | 
              | 
              | 
              | 
text          | |f (u(µk ), µk ) − f (u(µ∗ ), µ∗ )|
              |                                   10−1                                                             10−1
blank         | 
              | 
              | 
              | 
text          |                                                                                   k∇f (u(µk ), µk )k
              |                                   10−3                                                             10−3
blank         | 
              | 
text          |                                   10−5                                                             10−5
blank         | 
              | 
text          |                                   10−7                                                             10−7
blank         | 
text          |                                          0   10     20      30     40     50                              0   10     20      30     40      50
              |                                                   Major iterations                                                 Major iterations
blank         | 
text          | Figure 5.13: Convergence history of various optimization solvers for optimal control of the viscous
              | Burgers’ equation when Galerkin reduced-order model defines the approximation model. Optimiza-
              | tion solvers considered: L-BFGS solver with only HDM evaluations (          ), adj-etr-intpt (    ),
              | adj-ctr-intpt (   ), adj-ctr-stcg (  ).
blank         | 
              | 
text          | quantities of interest; therefore, this section only studies ‘adj-etr-intpt’, ‘adj-ctr-intpt’, and ‘adj-ctr-
              | stcg’ from Table 5.1. Furthermore, this section only considers reduced-order models based on a
              | Galerkin projection to ensure consistent gradients, which is a particularly important consideration
              | when the number of parameters is large. The convergence of these methods, as a function of
              | major iteration, is provided in Figure 5.13, along with the convergence of the baseline method that
              | uses an L-BFGS method (without model reduction). The trust region method with a residual-based
              | constraint converges most rapidly and, similar to the previous section, the methods that employ exact
              | trust region solvers outperform the inexact Steihaug-Toint CG solver. In fact, the method based on
              | the Steihaug-Toint CG solver is converging; however, after the maximum number of iterations (50)
              | the iterates are not close enough to the solution for quadratic convergence to be realized and does
              | not converge to the same tolerance as the other methods.
              |                         The increased convergence rate, in terms of major iterations (and therefore HDM evaluations),
              | of Algorithm 11 comes at the price of a large number of ROM evaluations. Figure 5.14 shows the
              | cumulative number of primal ROM queries as a function of major iteration and a histogram of the
              | number of primal ROM evaluations at a given reduced basis size (ku ). Similar to the previous
              | section, the inexact solver requires far fewer ROM queries than the exact solvers. However, unlike
              | the previous section, the number of ROM queries required by the residual-based trust region and
              | traditional trust region are not significantly different.
              |                         To assess the speedups that can be realized by the variants of the proposed ROM-based trust
              | region methods in Table 5.1, the following simplified cost model is introduced
blank         | 
text          |                                                           C = nhp + nha /2 + τ −1 (nrp + nra /2)                                   (5.61)
blank         | 
text          | where C is the total cost associated with a particular method in the units of equivalent number of
              | primal HDM queries, nhp is the number of primal HDM queries, nha is the number of adjoint HDM
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                           149
blank         | 
              | 
              | 
              | 
text          | number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                                                                                   150
              |                                  1,000
blank         | 
              | 
              | 
text          |                                         500                                       100
blank         | 
              | 
              | 
text          |                                           0
              |                                                                                   50
              |                                               0   2        4     6     8     10         0       5        10          15          20
              |                 number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                                         150
              |                                                                                    8
              |                                         100
              |                                                                                    6
              |                                          50
              |                                                                                    4
              |                                           0
              |                                               0   10      20     30     40   50             0       20        40            60
              |                                                        Major iterations                             ROM size (ku )
blank         | 
              | 
text          | Figure 5.14: Left: Cumulative number of primal ROM queries as a function of major iteration in the
              | trust region algorithm based on reduced-order models (Algorithm 11) as applied to optimal control
              | of the viscous Burgers’ equation. Right: Histogram of the number of primal ROM queries at a
              | given basis size. Data separated into the top and bottom rows to deal with the disparate x-scales.
              | All reduced-order models use a Galerkin projection. Optimization solvers considered: adj-etr-intpt
              | (    ), adj-ctr-intpt (  ), adj-ctr-stcg (   ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    150
blank         | 
              | 
              | 
text          | queries, nrp is the number of primal ROM queries, nra is the number of adjoint ROM queries, and τ
              | is the ratio of the cost of a primal HDM query to a primal ROM query. This cost model assume the
              | cost of computing the primal HDM (ROM) solution is twice that of computing an adjoint solution.
              | Under this cost model, Figure 5.15 contains the convergence rates of the various algorithms as a
              | function of cost for three values of τ : two moderate values for the expected speedup of the reduced-
              | order model (τ = 50, 100) and the asymptotic case of a free reduced-order model (τ = ∞). The
              | variants of the trust region method based on the exact trust region solver (‘adj-etr-intpt’ and ‘adj-
              | ctr-intpt’) outperform the baseline L-BFGS method, even if ROM queries are only 50× faster than
              | HDM queries. Depending on the speedup of the ROM, a given value of the objective function or
              | gradient can be achieved by methods ‘adj-etr-intpt’ at less than 50% the cost required by the baseline
              | method.
              |    This section closes with a study of the convergence behavior of the trust region method that
              | uses a residual-based trust region constraint. Figure 5.16 contains the convergence history of the
              | objective function and approximation model (left) and their gradients (right) at trust region centers
              | and candidate steps. The approximation model is first-order consistent at trust region centers
              | since the basis is constructed with the span-preserving variant of POD (Algorithm 7) and uses
              | fully converged snapshots. Despite relatively poor agreement of the model and objective (and the
              | corresponding gradients) at the candidate steps, rapid progress is made toward the optimal solution.
              | These observations are verified in Tables 5.6–5.7 that contains the convergence history of the relevant
              | trust region quantities for methods ‘adj-etr-intpt’ and ‘adj-ctr-intpt’.
blank         | 
              | 
title         | 5.5.3     Shape Optimization of Airfoil in Inviscid, Subsonic Flow
text          | In this section, we consider the inverse shape design of an airfoil in inviscid, subsonic flow: given
              | only the pressure distribution of a target shape—the RAE2822 airfoil, in this case—the goal is to use
              | shape optimization to recover the underlying shape. The initial guess for the optimization problem
              | is the symmetric NACA0012 airfoil.
blank         | 
title         | Shape parametrization and problem setup
blank         | 
text          | A plethora of shape parametrization techniques exist [177, 9], each with strengths and weaknesses.
              | They typically trade-off between efficiency and flexibility. A subset of these techniques have been
              | studied in the context of model order reduction [174]. In this work, the SDESIGN software [129, 127,
              | 128], based on the design element approach [99, 61], is used for shape parametrization (Section 2.1.2).
              | Here, a single “cubic” design element is used to parametrize the deformation of the NACA0012 airfoil.
              | Such a design element has 8 control nodes. They are used to define cubic Lagrangian polynomials
              | to describe the displacement field along the horizontal edges of the element, and linear functions
              | to define the displacement field along its vertical edges. For this application, the set of admissible
              | shapes is further restricted by constraining the control nodes to move in the vertical direction only.
              | This results in a parametrization with 8 variables where each of them represents the displacement
              | of a control node in the vertical direction. The case where all parameters are equal, µ = c1 for
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                    151
blank         | 
              | 
              | 
              | 
text          | |f (u(µk ), µk ) − f (u(µ∗ ), µ∗ )|
              |                                   10−1                                                10−1
blank         | 
              | 
              | 
              | 
text          |                                                                      k∇f (u(µk ), µk )k
              |                                   10−3                                                10−3
blank         | 
              | 
text          |                                   10−5                                                10−5
blank         | 
              | 
text          |                                   10−7                                                10−7
blank         | 
text          |                                          0   50          100   150                           0   50          100    150
              | |f (u(µk ), µk ) − f (u(µ∗ ), µ∗ )|
blank         | 
              | 
              | 
              | 
text          |                                   10−1                                                10−1
blank         | 
              | 
              | 
              | 
text          |                                                                      k∇f (u(µk ), µk )k
              |                                   10−3                                                10−3
blank         | 
              | 
text          |                                   10−5                                                10−5
blank         | 
              | 
text          |                                   10−7                                                10−7
blank         | 
text          |                                          0   50          100   150                           0   50          100    150
              | |f (u(µk ), µk ) − f (u(µ∗ ), µ∗ )|
blank         | 
              | 
              | 
              | 
text          |                                   10−1                                                10−1
              |                                                                      k∇f (u(µk ), µk )k
blank         | 
              | 
              | 
              | 
text          |                                   10−3                                                10−3
blank         | 
              | 
text          |                                   10−5                                                10−5
blank         | 
              | 
text          |                                   10−7                                                10−7
blank         | 
text          |                                          0   50          100   150                           0   50          100    150
              |                                                   Cost                                                Cost
blank         | 
text          | Figure 5.15: Convergence of the objective function (left) and gradient (right) as a function of the
              | cost metric in (5.61) for several values of the speedup factor of the reduced-order model: τ = 50
              | (top row), τ = 100 (middle row), τ = ∞ (bottom row) for optimal control of the viscous Burgers’
              | equation. All reduced-order models use a Galerkin projection. Optimization solvers considered: L-
              | BFGS solver with only HDM evaluations (        ), adj-etr-intpt ( ), adj-ctr-intpt (  ), adj-ctr-stcg
              | (    ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                              152
blank         | 
              | 
              | 
              | 
text          | 10−2                                                          10−2
blank         | 
              | 
text          | 10−5                                                          10−5
blank         | 
              | 
text          | 10−8                                                          10−8
              |        0      2      4      6       8        10                      0      2      4      6       8        10
              |                     Major iteration                                               Major iteration
blank         | 
text          | Figure 5.16: Convergence history of the objective (left) and gradient (right) quantities for optimal
              | control of the viscous Burgers’ equation using Algorithm 11 (fully converged solutions as snapshots
              | and in the evaluation of trust region steps). Left: |F (µk ) − F (µ∗ )| (   ), |F (µ̂k ) − F (µ∗ )| (   ),
              | |mk (µ̂k ) − F (µ∗ )| (   ). Right: k∇F (µk )k (      ), k∇F (µ̂k )k (    ), k∇mk (µ̂k )k (        ). The
              | variant ‘adj-etr-intpt’ (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based
              | reduced-order models is used. Since the approximation model is first-order consistent at trust region
              | centers mk (µk ) and k∇mk (µk )k are omitted.
blank         | 
              | 
text          | c ∈ R, corresponds to a rigid translation in the vertical direction. Because such a translation does
              | not affect the definition of a shape, it is eliminated by constraining one of the displacement variables
              | to zero. Furthermore, because the control nodes are allowed to move only in the vertical direction,
              | rigid rotations are automatically eliminated. A visualization of the vertices of the design element
              | and the deformation induced by perturbing each design variable is given in Figure 5.17. While
              | SDESIGN is used to deform the surface nodes of the airfoil, a robust mesh motion algorithm based
              | on a structural analogy is used to deform the surrounding body-fitted CFD mesh accordingly.
              |    The flow over the airfoil is modeled using the compressible Euler equations, and these are solved
              | numerically using AERO-F [68]. Because this flow solver is three-dimensional, the two-dimensional
              | fluid domain around the airfoil is represented as a slice of a three-dimensional domain. This slice is
              | discretized using a body-fitted CFD mesh with 54 816 tetrahedra and 19 296 nodes (Figure 5.18a).
              | Specifically, the flow equations are semi-discretized by AERO-F on this CFD mesh using a second-
              | order finite volume method based on Roe’s flux [169].
              |    For each airfoil configuration generated during the iterative optimization procedure, the steady
              | state solution of the flow problem is computed iteratively using pseudo-transient continuation. For
              | this purpose, each sought-after steady state solution is initialized using the best previously computed
              | steady state solution available in the database5 . The best steady state solution is defined here as that
              | steady state solution available in the database which, for the given airfoil configuration, minimizes
              | the residual of the discretized steady state Euler equations. Because the database of steady state
              | flow solutions is initially empty, the iterative computation of the steady state flow over the initial
              | shape—in this case, that of the NACA0012 airfoil—is initialized with the uniform flow solution.
              |    The trust region method described in this chapter that employs ROMs as the approximation
              | model is used to solve the aerodynamic shape optimization problem. At each HDM sample, the
              |    5 In this context, the database refers to the flow solutions computed for all shapes previously visited by the
blank         | 
text          | optimization trajectory.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                   153
blank         | 
              | 
              | 
              | 
text          |                        (a) µ1 = 0.1                                 (b) µ2 = 0.1
blank         | 
              | 
              | 
              | 
text          |                        (c) µ3 = 0.1                                 (d) µ4 = 0.1
blank         | 
              | 
              | 
              | 
text          |                        (e) µ5 = 0.1                                 (f) µ6 = 0.1
blank         | 
              | 
              | 
              | 
text          |                        (g) µ7 = 0.1                                 (h) µ8 = 0.1
blank         | 
text          | Figure 5.17: Shape parametrization of a NACA0012 airfoil using a cubic design element (the notation
              | µi designates the i-th component of the vector µ which refers to the i-th displacement degree of
              | freedom of the shape parametrization)
blank         | 
              | 
text          | steady state solution and sensitivities with respect to shape parameters are computed and used as
              | snapshots. As the chosen shape parametrization has 8 parameters, 9 snapshots are generated per
              | HDM sample: one snapshot corresponding to the steady state solution and 8 solution sensitivities. A
              | ROB is extracted from these snapshots using the heterogeneous span-preserving variant of the POD
              | method in Algorithm 7. Because very few snapshots are generated for this problem, the truncation
              | step in the POD algorithm is skipped. Consequently, the size of the constructed ROB is ku = 9s,
              | where s is the number of sampled HDMs. The nonlinear least-squares problem describing the ROM
              | is solved using the Gauss-Newton method equipped with a backtracking linesearch algorithm. The
              | python interface to the SNOPT [70] software, pyOpt [151], is used to solve the optimization problem
              | itself.
              |     At this point, it is noted that since the exact profile of the RAE2822 airfoil does not lie in
              | the space of admissible airfoil profiles defined by the cubic design element parametrization, it is
              | approximated by the closest admissible profile. This approximation is referred to in the remainder
              | of this section as the Cub-RAE2822 airfoil. It is graphically depicted in Figure 5.19 which also shows
              | the pressure isolines computed for this airfoil at the free-stream Mach number M∞ = 0.5 and angle
              | of attack α = 0.0◦ .
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    154
blank         | 
              | 
              | 
              | 
text          |  (a) CFD mesh for the NACA0012 airfoil               (b) Pressure field (M∞ = 0.5, α = 0.0◦ )
              |  (undeformed, 19 296 nodes)
blank         | 
text          |   Figure 5.18: NACA0012 mesh and pressure distribution at Mach 0.5 and zero angle of attack.
blank         | 
              | 
title         | Subsonic inverse design
blank         | 
text          | The free-stream conditions of interest are set to the subsonic Mach number M∞ = 0.5 and zero
              | angle of attack (α = 0◦ ), and the following optimization problem is considered
blank         | 
text          |                                             1                              2
              |                             minimize          p(u(µ)) − p(u(µRAE2822 ))    2
              |                               µ∈RNµ         2
              |                                                                                                 (5.62)
              |                             subject to µ3 = 0
              |                                             µl ≤ µ ≤ µu
blank         | 
text          | where p(u) is the vector of nodal pressures, and µRAE2822 designates the parameter solution vector
              | morphing the NACA0012 airfoil into the Cub-RAE2822 airfoil. The first constraint is introduced
              | to eliminate the rigid body translation in the vertical direction as discussed in the previous section.
              | The box constraints prohibit the optimization trajectory from going through highly distorted shapes
              | that would cause the flow solver to fail.
              |    To obtain a reference solution that can be used for assessing the performance of the proposed
              | ROM-based optimization method, problem (5.62) is first solved using the HDM as the constraining
              | PDE. In this case, the optimizer is found to reduce the initial value of the objective function by
              | 9 orders of magnitude, before numerical difficulties cause it to terminate (Figure 5.20). Relevant
              | statistics associated with this HDM-based reference solution of the optimization problem are gath-
              | ered in Table 5.8. Essentially, 24 optimization iterations are required to obtain a solution with a
              | relative error well below 0.1%. These iterations incur a total of 29 HDM queries (including those
              | associated with the linesearch iterations). Figure 5.21 shows the pressure distribution associated
              | with this reference solution matches the target pressure distribution very well.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                       155
blank         | 
              | 
              | 
              | 
text          |      (a) CFD mesh for the Cub-RAE2822 airfoil            (b) Pressure field (M∞ = 0.5, α = 0.0◦ )
blank         | 
text          | Figure 5.19: Cub-RAE2822 mesh and pressure isolines computed at Mach 0.5 and zero angle of
              | attack.
blank         | 
              | 
text          |    Next, the ROM-based trust region method developed in this chapter and summarized in Algo-
              | rithm 11 is applied to solve problem (5.62), which solves a sequence of trust region subproblems of
              | the form
              |                                        1                                    2
              |                           minimize       p(Φk ur (µ)) − p(u(µRAE2822 ))     2
              |                            µ∈RNµ       2
              |                           subject to µ3 = 0
              |                                                                                                     (5.63)
              |                                        µl ≤ µ ≤ µu
              |                                        1                  2
              |                                          kr(Φk ur (µ), µ)k2 ≤ ∆k .
              |                                        2
              |    The HDM is sampled at the initial configuration and the resulting 9 snapshots are used to build
              | a ROB using Algorithm 7, without truncation. The resulting ROB is used to construct a reduced-
              | order model based on a LSPG projection and the corresponding minimum-residual sensitivity model
              | to solve (5.63). Indeed, as the minimum-residual sensitivity computation described in Section 4.1.2
              | is not consistent with the true reduced sensitivities for large residuals, convergence of the optimiza-
              | tion problem is not guaranteed. To address this issue, an upper bound is set on the number of
              | optimization iterations (25 in this case) and the goal of the reduced optimization problem is set to
              | finding an improvement to the current solution before updating the ROB. The HDM is sampled at
              | the termination point of each reduced optimization problem yielding 9 additional snapshots which
              | are appended to the ROB using Algorithm 9. Linear independence of the basis is maintained by
              | truncating vectors corresponding to singular values below some tolerance. For the present applica-
              | tion, such truncation was not necessary as the snapshots added to the ROB at a given iteration were
              | not contained in the span of the snapshots from previous iterations.
              |    Using only 7 HDM samples, the progressive ROM optimization framework reduces the initial
              | pressure discrepancy by 18 orders of magnitude, to essentially machine zero. Interestingly, this is 4
              | times fewer HDM queries than required by the HDM-based optimization. Figure 5.21 shows that
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                156
blank         | 
              | 
              | 
              | 
text          |                                      10−1
blank         | 
text          |                                  2
              |                                  2
              |                      RAE2822 ))k2
              |                                  2
              |       k               RAE2822
              |                               ))
              |                                      10−3
              |      2 p(u(µ))−p(u(µ
blank         | 
title         |      2 kp(u(0))−p(u(µ
blank         | 
              | 
              | 
              | 
text          |                                      10−5
blank         | 
              | 
text          |                                      10−7
              |       k
              |      1
              |      1
blank         | 
              | 
              | 
              | 
text          |                                      10−9
blank         | 
text          |                                             0   2   4   6   8       10    12    14    16   18   20   22   24
              |                                                                 Optimization iterations
blank         | 
text          | Figure 5.20: Progression of the objective function during the HDM-based optimization. The initial
              | guess is defined as the 0th optimization iteration.
blank         | 
              | 
text          | both the shape of the airfoil and the associated pressure distribution discovered by the ROM-based
              | optimization method match the target shape and pressure distribution very well.
              |    Surprisingly, the ROM-based optimization process achieves a lower value of the objective function
              | than the HDM-based one. This can be traced to convergence tolerance on the HDM sensitivity
              | analysis. The HDM-based sensitivities are obtained by solving the multiple right-hand side linear
              | system of equations in (2.87) using GMRES. The convergence tolerance is kAx − bk2 ≤ γ kbk2
              | for solving the linear system of equations Ax = b, with γ = 10−10 in this case. If kbk is large
              | (b = ∂r/∂µ in this case), the convergence requirement may be rather flexible. Conversely, the
              | minimum-residual ROM sensitivities in (4.28) are solved to machine precision using a direct QR
              | factorization.
              |    Recall from Chapter 4 that the minimum-residual reduced sensitivities approach the true sen-
              | sitivities for LSPG projection as the HDM residual approaches zero. Figure 5.24 verifies that the
              | HDM residual is small after 6 HDM samples are taken, which implies the minimum-residual ROM
              | sensitivities are (nearly) consistent with the true ROM sensitivities. This consistency will guaran-
              | tee convergence of the reduced optimization problem when using a globally convergent optimization
              | solver. Additionally, the small HDM residual implies that the ROM is highly accurate in this region,
              | making it likely that the reduced optimization problem will converge to a point close to the true
              | optimum.
              |    Figure 5.22 reports on the evolution of the objective function with the number of optimization
              | iterations, and marks each new HDM query along the optimization trajectory. The reader can
              | observe that the proposed ROM-based optimization method performs a total of 160 trust region
              | subproblem iterations (Figure 5.23) requiring 346 ROM evaluations (see Table 5.8) and 7 HDM
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                  157
blank         | 
              | 
              | 
text          |           0.6
              |                                                                     Initial
              |                                                                     Target                0.6
              |           0.4
              |                                                              HDM-based optimization
              |                                                              ROM-based optimization
              |           0.2                                                                             0.5
blank         | 
              | 
              | 
              | 
text          |                                                                                                  Distance Transverse to Centerline
              |             0                                                                             0.4
blank         | 
              | 
text          |          −0.2                                                                             0.3
              |    -Cp
blank         | 
              | 
              | 
              | 
text          |          −0.4                                                                             0.2
blank         | 
              | 
text          |          −0.6                                                                             0.1
blank         | 
text          |          −0.8                                                                             0
blank         | 
text          |           −1                                                                              −0.1
blank         | 
text          |          −1.2
              |                 0   0.1   0.2    0.3    0.4     0.5    0.6      0.7    0.8    0.9     1
              |                                        Distance along airfoil
blank         | 
text          | Figure 5.21: Subsonic inverse design of the airfoil Cub-RAE2822: initial shape (NACA0012) and
              | associated Cp function, and final shape (Cub-RAE2822) and associated Cp functions delivered by
              | the HDM- and ROM-based optimizations, respectively.
blank         | 
              | 
text          | queries. From a computational complexity viewpoint, this compares favorably with the 24 HDM-
              | based optimization iterations requiring 29 HDM queries (see Table 5.8). Figure 5.23 graphically
              | depicts the progression of the reduced objective function across all reduced optimization problems
              | using a dashed line to indicate a new HDM sample and a subsequent update of the ROB. For each
              | optimization problem, it also reports the size of the ROM.
              |    Finally, Figure 5.24 shows the evolution of the HDM residual evaluated at the solution of the
              | ROM—which is an indicator of the ROM error—across all reduced optimization problems, along
              | with the trust region radius ∆k . It is common practice in nonlinear programming software to allow
              | violation of nonlinear constraints during an optimization procedure, which explains the residual
              | bound violation seen in this figure. Figure 5.24 also shows that the ROM solution coincides with
              | the HDM solution at the initial condition of each optimization problem, as expected from the
              | interpolation property of minimum-residual reduced-order models. In the first few major iterations
              | that are far the optimal solution, the residual grows rapidly as the iterates move into areas of the
              | parameter space away from HDM samples. However, near the optimal solution, the residual remains
              | small as the optimization iterates remain in a small neighborhood of the most recent HDM sample.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                     158
blank         | 
              | 
              | 
text          | Remark. There are two mechanisms that prevent the reduced optimization problem from venturing
              | into regions of the parameter space where it lacks accuracy: (1) the objective function and (2) the
              | nonlinear trust region. In the present inverse design example, the objective function is mostly suffi-
              | cient to keep the ROM in regions of accuracy, as can be seen from Figure 5.24 where the trust region
              | bound is only reached once and the upper bound always increases. For other objective functions such
              | as drag, the nonlinear trust region will be necessary as it is likely that an inaccurate ROM can predict
              | a lower value in such objective functions than is actually present. In practice, inaccurate ROMs have
              | been observed to predict the nonphysical situation of negative drag (i.e. thrust), which motivates the
              | need for the residual-based trust region.
blank         | 
              | 
              | 
              | 
text          |                                        10−2
              |                                    2
              |                                    2
              |                                    2
              |                                ))k2
              |        k               RA E2822
              |                                 ))
              |                        RAE2822
blank         | 
              | 
              | 
              | 
text          |                                        10−6
              |       2 p(u(µ))−p(u(µ
blank         | 
title         |       2 kp(u(0))−p(u(µ
blank         | 
              | 
              | 
              | 
text          |                                        10−10
blank         | 
              | 
text          |                                        10−14
              |        k
              |       1
              |       1
blank         | 
              | 
              | 
              | 
text          |                                                                                     HDM-based optimization
              |                                                                                     ROM-based optimization
              |                                        10−18
              |                                                1   3   5   7   9   11 13 15 17 19          21   23   25   27   29
              |                                                                    Number of HDM queries
blank         | 
text          | Figure 5.22: Objective function versus number of queries to the HDM: ROM-based optimization
              | (red) and HDM-based optimization (black).
blank         | 
              | 
              | 
text          | 5.5.4                          Shape Optimization of the Common Research Model in Viscous,
              |                                Turbulent Flow
              | This section applies the proposed trust region method based on reduced-order approximation models
              | to shape design of a full aircraft configuration—the Common Research Model (CRM)—in viscous,
              | turbulent flow. The goal of the optimization problem is to maximize the lift-to-drag ratio of the
              | aircraft while maintaining a constant lift. The flow is modeled using the Reynolds’ Averaged Navier-
              | Stokes (RANS) equations with a Spalart-Allmaras turbulence model. The freestream Mach number
              | and angle of attack are taken as M = 0.85 and α = 2.32◦ , which are standard operating conditions
              | for a commercial aircraft of this size. The Reynolds’ number is Re = 5 × 106 , which is based on wind
              | tunnel model conditions and the reference chord length in the undeformed configuration. The chosen
              |    6 The last HDM sample in Figure 5.22 was not included in this count as the residual-based error indicator is small
blank         | 
text          | at this configuration (Figure 5.24). A similar argument could also be made for the 7th HDM sample.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                           159
blank         | 
              | 
              | 
              | 
text          |   2
              |                                                                                                                                60
              |   kp(Φur (µ))−p(Φur (µRAE2822 ))k2
              |                                                       10−2
              |                                        RAE2822 ))k2
              |                                                   2
blank         | 
              | 
text          |                                                       10−6
blank         | 
              | 
              | 
              | 
text          |                                                                                                                                     ROM size
              |                                                                                                                                40
              |                        2 kp(u(0))−p(u(µ
blank         | 
              | 
              | 
              | 
text          |                                                       10−10
blank         | 
text          |                                                                                                                                20
              |                                                         −14
              |                                                       10
              |                        1
              |    1
              |    2
blank         | 
              | 
              | 
              | 
text          |                                                                   HDM sample
              |                                                       10−18                                                                    0
              |                                                               0   20    40         60      80      100       120   140   160
              |                                                                                Reduced optimization iterations
blank         | 
text          | Figure 5.23: Progression of reduced objective function: dashed line indicates an HDM sample and a
              | subsequent update of the ROB.
blank         | 
              | 
text          | freestream Mach number place the flow in the transonic regime, which implies shocks will develop
              | on the wing of the aircraft. The governing equations are discretized with a second-order, vertex-
              | centered finite volume scheme using the AERO-F software [68] and the resulting system of nonlinear
              | equations are solved using pseudo-transient continuation. The mesh employed was validated for
              | these freestream conditions [198] and consists of 11 454 702 nodes for a total of 68 728 212 degrees of
              | freedom.
              |        The design problem (5.64) looks to maximize the lift-to-drag ratio at a constant lift subject to
              | box constraints over a four-dimensional shape design space
blank         | 
text          |                                                                                maximize
              |                                                                                     4
              |                                                                                           Lz (µ)/Lx (µ)
              |                                                                                  µ∈R
blank         | 
text          |                                                                                subject to Lz (µ) = Lz (0)                           (5.64)
blank         | 
text          |                                                                                           µl ≤ µ ≤ µu .
blank         | 
text          | The four shape parameters considered in this problem are: wingspan (µ1 ), localized sweep (µ2 ),
              | twist (µ3 ), and localized dihedral (µ4 ); see Figure 5.25 for an illustration of each parameter. The
              | lift constraint is included to ensure the optimized aircraft can carry the same payload as the original
              | aircraft. The box constraints are included to ensure the shape changes are reasonable and the com-
              | putational mesh does not tangle. The optimization problem in (5.64) is initialized from a perturbed
              | CRM configuration that shortens the wing and adds negative twist. The optimized configuration
              | achieves a drag count reduction of 2.2 by lengthening the wing and adding positive sweep, dihedral,
              | and twist. This solution was obtained using by embedding a L-BFGS-B [215] bound-constrained op-
              | timization solver in an augmented Lagrangian framework to handle the nonlinear equality constraint
              | and solve (5.64) directly. This method, which solely relies on HDM solves for objective and gradient
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    160
blank         | 
              | 
              | 
text          |                            1011
blank         | 
              | 
              | 
text          |       2                     105
              |        kr(Φur (µ), µ)k2
blank         | 
              | 
text          |                           10−1
              |        1
              |        2
blank         | 
              | 
              | 
              | 
text          |                           10−7              Residual Norm
              |                                       Residual Norm Bound (∆k )
              |                                             HDM Sample
              |                           10−13
              |                                   0   20      40       60      80      100       120   140   160
              |                                                    Reduced optimization iterations
blank         | 
text          | Figure 5.24: Progression of HDM residual: dashed line indicates an HDM sample and a subsequent
              | update of the ROB.
blank         | 
              | 
text          | queries, will serve as a baseline for comparison with the proposed hyperreduced trust region method
              | in the remainder. Figure 5.26 provides two different views of the initial and optimized shapes to
              | illustrate the changes that occur during the maximization process. Figure 5.27 shows the initial
              | and optimized shapes colored by the pressure coefficient distribution on the surface. The optimized
              | shape weakens the shock near the wing tip, which explains the 2.2 drag count reduction.
              |    The proposed trust region method based on masked minimum-residual hyperreduced approxi-
              | mation models (with an underlying LSPG projection) is applied to solve the optimization problem
              | in (5.64). Gradients are computed according to the masked minimum-residual sensitivity method
              | and primal/sensitivity snapshots are used in the heterogeneous, span-preserving variant of POD
              | without truncation. Therefore, the size of the reduced-order model increases by 5 at each iteration
              | since a single primal snapshot and four sensitivity snapshots are added to the reduced-order basis.
              | Due to the presence of the nonlinear equality constraint, the unconstrained trust region method is
              | wrapped in the augmented Lagrangian framework described in Section 3.2.1. Figure 5.28 shows the
              | convergence history of the drag count reduction as a function of the number of HDM queries for the
              | baseline method and the hyperreduced trust region method. The hyperreduced trust region method
              | requires half as many queries to the HDM to converge to a prescribed tolerance. However, this
              | does not account for all sources of cost in the hyperreduced trust region method since there is cost
              | associated with solving the trust region subproblem (hyperreduced model queries) and construction
              | of the hyperreduced model at each iteration. Figure 5.29 includes these additional sources of cost
              | by showing the convergence of the drag count reduction as a function of wall time normalized by
              | the wall time of a single HDM solve. When properly accounting for all sources of cost, the speedup
              | of the hyperreduced trust region method decreases marginally from 2× to 1.6× (80% efficiency).
              | Finally, Figure 5.30 shows the sample mesh used at intermediate of the hyperreduced trust region
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                          161
blank         | 
              | 
              | 
              | 
text          | Figure 5.25: Parametrization of CRM. Left: Undeformed CRM configuration. Right: Deformed
              | CRM configuration with positive perturbation to the wingspan µ1 (top row), localized sweep µ2
              | (second row), twist µ3 (third row), and localized dihedral µ4 (bottom row).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    162
blank         | 
              | 
              | 
              | 
text          | Figure 5.26: Two different views of the initial guess (gray) and solution (red) of the optimization
              | problem in (5.64). The displacement from the undeformed configuration to the optimal solution
              | (red) is magnified by 2×. There is a 2.2 drag count reduction from the initial to optimized shape.
blank         | 
              | 
              | 
              | 
text          | Figure 5.27: Left: Initial guess for optimization problem in (5.64). Right: Solution of optimization
              | problem in (5.64). Both plots are colored by the coefficient of pressure Cp . There is a 2.2 drag count
              | reduction from the initial to optimized shape.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    163
blank         | 
              | 
              | 
              | 
text          |                              15
blank         | 
text          |                              10
              |      Drag Count Reduction
              |                               5
blank         | 
text          |                               0
blank         | 
text          |                             −5
blank         | 
text          |                             −10
blank         | 
text          |                             −15
blank         | 
text          |                             −20
blank         | 
text          |                                   −2   0   2   4   6   8  10 12 14 16 18       20   22   24   26
              |                                                        Number of HDM queries
blank         | 
text          | Figure 5.28: Convergence history of the baseline PDE-constrained optimization solver without model
              | reduction (     ) and proposed trust region method based on hyperreduced approximation models
              | (    ). A yellow square ( ) indicates an augmented Lagrangian update. The reduction in drag
              | count is taken as the performance metric and the number of primal HDM queries is the cost model.
              | With respect to this cost metric, the ROM-based optimization solver converges 2× faster than the
              | HDM-based solver.
blank         | 
              | 
text          | method, which contains only 72 110 nodes—0.6% of the original mesh.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                          164
blank         | 
              | 
              | 
              | 
text          |                              15
blank         | 
              | 
text          |      Drag Count Reduction    10
blank         | 
text          |                               5
blank         | 
text          |                               0
blank         | 
text          |                             −5
blank         | 
text          |                             −10
blank         | 
text          |                             −15
blank         | 
text          |                             −20
blank         | 
text          |                                   0   2   4   6     8    10 12 14 16 18 20           22   24   26   28
              |                                                   Equivalent number of HDM queries
blank         | 
text          | Figure 5.29: Convergence history of the baseline PDE-constrained optimization solver without model
              | reduction (      ) and proposed trust region method based on hyperreduced approximation models
              | (     ). A yellow square ( ) indicates an augmented Lagrangian update. The reduction in drag count
              | is taken as the performance metric and the total wall time of the optimization procedure (normalized
              | by the wall time of a single primal HDM solve) is the cost model. With respect to this cost metric,
              | the ROM-based optimization solver converges 1.6× faster than the HDM-based solver.
blank         | 
              | 
              | 
              | 
text          | Figure 5.30: The sample mesh (72 × 103 nodes) used at an intermediate iteration of the trust region
              | method based on hyperreduced (collocation) approximation models.
              | Table 5.2: Convergence history of Algorithm 11 applied to optimal control of the inviscid Burgers’ equation using method ‘sens-etr-intpt’
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (µk )      mk (µk )      F (µ̂k )     mk (µ̂k )    k∇F (µk )k        ρk           ∆k        Success?
              |               1.5922e+04    1.5922e+04    2.4783e+02    2.4528e+02   5.3000e+04    9.9984e-01    1.0000e-01   1.0000e+00
              |               2.4783e+02    2.4783e+02    1.4391e+02    1.4451e+02   5.2213e+02    1.0057e+00    2.0000e-01   1.0000e+00
              |               1.4391e+02    1.4391e+02    1.4317e-05    2.2758e-03   6.1988e+02    1.0000e+00    4.0000e-01   1.0000e+00
              |               1.4317e-05    1.4317e-05    6.3338e-20    8.7340e-16   1.4083e+00    1.0000e+00    8.0000e-01   1.0000e+00
              |               6.3338e-20    8.7340e-16         -             -       3.6015e-07         -             -            -
blank         | 
              | 
              | 
              | 
text          | Table 5.3: Convergence history of Algorithm 12 applied to optimal control of the inviscid Burgers’ equation using method ‘sens-etr-intpt’
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (µk )      mk (µk )      F (µ̂k )     mk (µ̂k )    k∇F (µk )k        ρk           ∆k         Success?
              |               1.6431e+04    1.6033e+04    1.8873e+02    1.9717e+02   5.3540e+04    1.0257e+00    1.0000e-01    1.0000e+00
              |               1.8873e+02    1.9784e+02    1.0874e+02    1.0973e+02   7.9026e+02    9.0777e-01    2.0000e-01    1.0000e+00
              |               1.0874e+02    1.0884e+02    1.2551e-03    5.9436e-03   6.7856e+02    9.9911e-01    4.0000e-01    1.0000e+00
              |               1.2551e-03    1.3252e-04    5.3325e-08    4.4413e-05   7.8316e+01    1.4245e+01    8.0000e-01    1.0000e+00
              |               5.3325e-08    5.3325e-08    4.4166e-23    3.1841e-20   1.2847e-01    1.0000e+00    1.6000e+00    1.0000e+00
              |               4.4166e-23    3.1841e-20         -             -       3.4743e-09         -             -             -
meta          |                                                                                                                                             CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
              |                                                                                                                                             165
text          | Table 5.4: Convergence history of Algorithm 11 applied to optimal control of the inviscid Burgers’ equation using method ‘sens-ctr-stcg’ using
              | reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (µk )       mk (µk )       F (µ̂k )     mk (µ̂k )    k∇F (µk )k         ρk            ∆k         Success?
              |                1.5922e+04    1.5922e+04    1.1372e+04     1.1346e+04    5.3000e+04    9.9439e-01     1.0000e-01    1.0000e+00
              |                1.1372e+04    1.1372e+04    5.5895e+03     5.5895e+03    3.8805e+04    1.0000e+00     2.0000e-01    1.0000e+00
              |                5.5895e+03    5.5895e+03    8.9676e+02     8.9676e+02    2.0968e+04    1.0000e+00     4.0000e-01    1.0000e+00
              |                8.9676e+02    8.9676e+02    8.0315e+05     3.4795e+04    4.4001e+03    2.3666e+01     8.0000e-01    0.0000e+00
              |                8.9676e+02    8.9676e+02    6.0483e+02     6.0483e+02    4.4001e+03    1.0000e+00     1.6000e-01    1.0000e+00
              |                6.0483e+02    6.0483e+02    4.0276e+02     4.0276e+02    1.3683e+03    1.0000e+00     3.2000e-01    1.0000e+00
              |                4.0276e+02    4.0276e+02    5.1981e+05     6.9325e+03    1.7695e+03    7.9545e+01     6.4000e-01    0.0000e+00
              |                4.0276e+02    4.0276e+02    3.3109e+02     3.3109e+02    1.7695e+03    1.0000e+00     1.0240e-01    1.0000e+00
              |                3.3109e+02    3.3109e+02    2.1174e+02     2.1174e+02    1.2858e+03    1.0000e+00     2.0480e-01    1.0000e+00
              |                2.1174e+02    2.1174e+02    4.9377e+02     4.9378e+02    6.7866e+02    9.9998e-01     4.0960e-01    0.0000e+00
              |                2.1174e+02    2.1174e+02    1.9116e+02     1.9116e+02    6.7866e+02    1.0000e+00     4.1943e-02    1.0000e+00
              |                1.9116e+02    1.9116e+02    1.4938e+02     1.4938e+02    9.5647e+02    1.0000e+00     8.3886e-02    1.0000e+00
              |                1.4938e+02    1.4938e+02    9.0046e+01     9.0046e+01    5.5266e+02    1.0000e+00     1.6777e-01    1.0000e+00
              |                9.0046e+01    9.0046e+01    7.9533e+01     7.9533e+01    3.8955e+03    1.0000e+00     3.3554e-01    1.0000e+00
              |                7.9533e+01    7.9533e+01    1.8002e+02     1.8002e+02    3.5646e+02    1.0000e+00     6.7109e-01    0.0000e+00
              |                7.9533e+01    7.9533e+01    7.1672e+01     7.1672e+01    3.5646e+02    1.0000e+00     2.3488e-02    1.0000e+00
              |                7.1672e+01    7.1672e+01    5.6588e+01     5.6588e+01    7.3693e+02    1.0000e+00     4.6976e-02    1.0000e+00
              |                5.6588e+01    5.6588e+01    3.2573e+01     3.2573e+01    6.6885e+02    1.0000e+00     9.3951e-02    1.0000e+00
              |                3.2573e+01    3.2573e+01    2.6527e+01     2.6527e+01    1.5016e+03    1.0000e+00     1.8790e-01    1.0000e+00
              |                2.6527e+01    2.6527e+01    5.4958e+00     5.4958e+00    1.3171e+04    1.0000e+00     3.7581e-01    1.0000e+00
              |                5.4958e+00    5.4958e+00    4.6810e+00     4.6810e+00    2.3990e+03    1.0000e+00     7.5161e-01    1.0000e+00
              |                4.6810e+00    4.6810e+00    6.7425e+00     6.7425e+00    1.1796e+02    1.0000e+00     1.5032e+00    0.0000e+00
              |                4.6810e+00    4.6810e+00    4.4251e+00     4.4251e+00    1.1796e+02    1.0000e+00     3.1837e-03    1.0000e+00
              |                4.4251e+00    4.4251e+00    3.9356e+00     3.9356e+00    1.1226e+02    1.0000e+00     6.3673e-03    1.0000e+00
              |                3.9356e+00    3.9356e+00    3.0492e+00     3.0492e+00    9.4010e+01    1.0000e+00     1.2735e-02    1.0000e+00
              |                3.0492e+00    3.0492e+00    1.6303e+00     1.6303e+00    1.7259e+02    1.0000e+00     2.5469e-02    1.0000e+00
              |                1.6303e+00    1.6303e+00    3.8792e-01     3.8792e-01    3.3882e+02    1.0000e+00     5.0939e-02    1.0000e+00
              |                3.8792e-01    3.8792e-01    1.1734e-01     1.1734e-01    1.8628e+03    1.0000e+00     1.0188e-01    1.0000e+00
              |                1.1734e-01    1.1734e-01    4.6898e-03     4.6898e-03    8.2406e+01    1.0000e+00     2.0375e-01    1.0000e+00
              |                4.6898e-03    4.6898e-03    2.7171e-05     2.7171e-05    2.5736e+02    1.0000e+00     4.0751e-01    1.0000e+00
              |                2.7171e-05    2.7171e-05    9.5916e-06     9.5916e-06    1.8011e+00    1.0000e+00     8.1502e-01    1.0000e+00
              |                                                                                                                                                  CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
blank         | 
              | 
              | 
              | 
text          |                9.5916e-06    9.5916e-06    2.5087e-11     2.5087e-11    1.1313e-01    1.0000e+00     1.6300e+00    1.0000e+00
              |                2.5087e-11    2.5087e-11    3.7967e-12     3.7967e-12    1.7368e-02    1.0000e+00     3.2601e+00    1.0000e+00
              |                                                                                                                                                  166
blank         | 
              | 
              | 
              | 
text          |                3.7967e-12    3.7967e-12    2.5080e-18     2.5047e-18    1.4714e-04    1.0000e+00     6.5202e+00    1.0000e+00
              |                2.5080e-18    2.5080e-18         -              -        4.2318e-06         -              -             -
              | Table 5.5: Convergence history of Algorithm 12 applied to optimal control of the inviscid Burgers’ equation using method ‘sens-ctr-stcg’ using
              | reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (µk )       mk (µk )       F (µ̂k )     mk (µ̂k )    k∇F (µk )k         ρk            ∆k         Success?
              |                1.6431e+04    1.6033e+04    1.1573e+04     1.1464e+04    5.3540e+04    1.0634e+00     1.0000e-01    1.0000e+00
              |                1.1573e+04    1.1372e+04    5.6146e+03     5.5885e+03    3.9065e+04    1.0303e+00     2.0000e-01    1.0000e+00
              |                5.6146e+03    5.5885e+03    9.0886e+02     8.9613e+02    2.1019e+04    1.0028e+00     4.0000e-01    1.0000e+00
              |                9.0886e+02    8.9613e+02    8.8161e+07     7.2819e+04    4.6131e+03    1.2258e+03     8.0000e-01    0.0000e+00
              |                9.0886e+02    8.9613e+02    5.9847e+02     6.0460e+02    4.6131e+03    1.0647e+00     1.6000e-01    1.0000e+00
              |                5.9847e+02    6.0459e+02    3.9555e+02     4.0246e+02    1.2978e+03    1.0039e+00     3.2000e-01    1.0000e+00
              |                3.9555e+02    4.0246e+02    2.4110e+05     2.6063e+04    2.0151e+03    9.3805e+00     6.4000e-01    0.0000e+00
              |                3.9555e+02    4.0246e+02    3.3074e+02     3.3080e+02    2.0151e+03    9.0433e-01     1.0240e-01    1.0000e+00
              |                3.3074e+02    3.3080e+02    2.1140e+02     2.1150e+02    1.2856e+03    1.0004e+00     2.0480e-01    1.0000e+00
              |                2.1140e+02    2.1150e+02    4.9353e+02     4.9353e+02    6.7904e+02    1.0003e+00     4.0960e-01    0.0000e+00
              |                2.1140e+02    2.1150e+02    1.9084e+02     1.9093e+02    6.7904e+02    1.0000e+00     4.1943e-02    1.0000e+00
              |                1.9084e+02    1.9093e+02    1.4906e+02     1.4917e+02    9.5568e+02    1.0003e+00     8.3886e-02    1.0000e+00
              |                1.4906e+02    1.4917e+02    8.9792e+01     8.9903e+01    5.5440e+02    1.0001e+00     1.6777e-01    1.0000e+00
              |                8.9792e+01    8.9903e+01    7.9260e+01     7.9375e+01    3.8951e+03    1.0003e+00     3.3554e-01    1.0000e+00
              |                7.9260e+01    7.9375e+01    1.7709e+02     1.7709e+02    3.5553e+02    1.0012e+00     6.7109e-01    0.0000e+00
              |                7.9260e+01    7.9375e+01    7.1456e+01     7.1569e+01    3.5553e+02    9.9979e-01     2.3349e-02    1.0000e+00
              |                7.1456e+01    7.1569e+01    5.6462e+01     5.6577e+01    7.4645e+02    1.0002e+00     4.6697e-02    1.0000e+00
              |                5.6462e+01    5.6577e+01    3.2554e+01     3.2667e+01    6.5370e+02    9.9991e-01     9.3395e-02    1.0000e+00
              |                3.2554e+01    3.2667e+01    2.7533e+01     2.7533e+01    1.4597e+03    9.7789e-01     1.8679e-01    1.0000e+00
              |                2.7533e+01    2.7533e+01    5.5226e+00     5.5226e+00    1.3537e+04    1.0000e+00     3.7358e-01    1.0000e+00
              |                5.5226e+00    5.5226e+00    4.6138e+00     4.6138e+00    2.5382e+03    1.0000e+00     7.4716e-01    1.0000e+00
              |                4.6138e+00    4.6138e+00    6.6933e+00     6.6933e+00    1.2584e+02    1.0000e+00     1.4943e+00    0.0000e+00
              |                4.6138e+00    4.6138e+00    4.3612e+00     4.3612e+00    1.2584e+02    1.0000e+00     3.1598e-03    1.0000e+00
              |                4.3612e+00    4.3612e+00    3.8793e+00     3.8793e+00    1.0487e+02    1.0000e+00     6.3196e-03    1.0000e+00
              |                3.8793e+00    3.8793e+00    3.0053e+00     3.0053e+00    1.0029e+02    9.9998e-01     1.2639e-02    1.0000e+00
              |                3.0053e+00    3.0053e+00    1.6074e+00     1.6075e+00    1.5696e+02    1.0000e+00     2.5279e-02    1.0000e+00
              |                1.6074e+00    1.6075e+00    3.7861e-01     3.7862e-01    3.2953e+02    1.0000e+00     5.0557e-02    1.0000e+00
              |                3.7861e-01    3.7862e-01    1.1784e-01     1.1784e-01    1.8295e+03    9.9998e-01     1.0111e-01    1.0000e+00
              |                1.1784e-01    1.1784e-01    4.5753e-03     4.5761e-03    7.9865e+01    9.9998e-01     2.0223e-01    1.0000e+00
              |                4.5753e-03    4.5761e-03    3.4610e-05     3.4584e-05    2.5399e+02    9.9982e-01     4.0446e-01    1.0000e+00
              |                3.4610e-05    3.4584e-05    1.3090e-05     1.3090e-05    1.8009e+00    1.0012e+00     8.0891e-01    1.0000e+00
              |                                                                                                                                                  CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
blank         | 
              | 
              | 
              | 
text          |                1.3090e-05    1.3090e-05    4.7238e-11     4.7238e-11    1.3249e-01    1.0000e+00     1.6178e+00    1.0000e+00
              |                4.7238e-11    4.7238e-11    6.3287e-12     6.3287e-12    2.4075e-02    1.0000e+00     3.2357e+00    1.0000e+00
              |                6.3287e-12    6.3287e-12    5.0572e-21     5.0530e-21    1.9695e-04    1.0000e+00     6.4713e+00    1.0000e+00
              |                                                                                                                                                  167
blank         | 
              | 
              | 
              | 
text          |                5.0572e-21    5.0572e-21    4.4579e-24     5.0572e-21    2.6757e-07        inf        1.2943e+01    1.0000e+00
              |                4.4579e-24    5.0572e-21         -              -        8.1378e-11         -              -             -
              | Table 5.6: Convergence history of Algorithm 11 applied to optimal control of the viscous Burgers’ equation using method ‘dual-etr-intpt’
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                   F (µk )     mk (µk )     F (µ̂k )     mk (µ̂k )   k∇F (µk )k      ρk            ∆k         Success?
              |                 3.8620e-02   3.8620e-02   3.2002e-02   3.4377e-02   1.2191e-02   1.5596e+00   1.0000e-01   1.0000e+00
              |                 3.2002e-02   3.2002e-02   2.2060e-02   1.6674e-02   7.0854e-03   6.4861e-01   2.0000e-01   1.0000e+00
              |                 2.2060e-02   2.2060e-02   1.5559e-02   1.5606e-02   2.0611e-03   1.0072e+00   1.8000e-01   1.0000e+00
              |                 1.5559e-02   1.5559e-02   1.5527e-02   1.5521e-02   6.9906e-05   8.4404e-01   3.6000e-01   1.0000e+00
              |                 1.5527e-02   1.5527e-02   1.5524e-02   1.5524e-02   1.6373e-05   9.3144e-01   7.2000e-01   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   1.5633e-06   9.7164e-01   1.4400e+00   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   9.1808e-08   9.9643e-01   2.8800e+00   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   5.5276e-07   9.9994e-01   5.7600e+00   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   3.2305e-08   1.0020e+00   1.1520e+01   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   6.9441e-08   1.0042e+00   2.3040e+01   1.0000e+00
              |                 1.5524e-02   1.5524e-02        -            -       6.1859e-08        -            -            -
blank         | 
              | 
              | 
              | 
text          | Table 5.7: Convergence history of Algorithm 11 applied to optimal control of the viscous Burgers’ equation using method ‘dual-ctr-intpt’
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                   F (µk )     mk (µk )      F (µ̂k )    mk (µ̂k )   k∇F (µk )k       ρk           ∆k         Success?
              |                 3.8620e-02   3.8620e-02   3.3959e-02   3.4919e-02   1.2191e-02   1.2596e+00   1.0000e-01    1.0000e+00
              |                 3.3959e-02   3.3959e-02   2.9644e-02   2.9645e-02   8.7651e-03   1.0003e+00   2.0000e-01    1.0000e+00
              |                 2.9644e-02   2.9644e-02   2.6406e-02   2.6406e-02   5.1460e-03   1.0000e+00   4.0000e-01    1.0000e+00
              |                 2.6406e-02   2.6406e-02   2.3727e-02   2.3727e-02   2.6859e-03   1.0000e+00   8.0000e-01    1.0000e+00
              |                 2.3727e-02   2.3727e-02   2.1113e-02   2.1113e-02   1.7850e-03   1.0000e+00   1.6000e+00    1.0000e+00
              |                 2.1113e-02   2.1113e-02   1.8771e-02   1.8771e-02   1.1968e-03   1.0000e+00   3.2000e+00    1.0000e+00
              |                 1.8771e-02   1.8771e-02   1.6877e-02   1.6877e-02   7.2259e-04   1.0000e+00   6.4000e+00    1.0000e+00
              |                 1.6877e-02   1.6877e-02   1.5700e-02   1.5700e-02   3.8119e-04   1.0000e+00   1.2800e+01    1.0000e+00
              |                                                                                                                                            CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
blank         | 
              | 
              | 
              | 
text          |                 1.5700e-02   1.5700e-02   1.5524e-02   1.5524e-02   1.1431e-04   1.0000e+00   2.5600e+01    1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   3.7482e-08   1.0042e+00   5.1200e+01    1.0000e+00
              |                 1.5524e-02   1.5524e-02        -            -       6.3774e-08        -            -             -
meta          |                                                                                                                                            168
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                 169
blank         | 
              | 
              | 
              | 
text          |        Table 5.8: Performance of the HDM- and ROM-based optimization methods.
blank         | 
text          |                                       HDM-based            ROM-based
              |                                       optimization         optimization
              |           # of HDM Evaluations            29                    76
              |           # of ROM Evaluations             -                   346
              |               µ∗ − µRAE2822
              |                                      2.28 × 10−3 %        4.17 × 10−6 %
              |                kµRAE2822 k
title         | Chapter 6
blank         | 
title         | Model Reduction and Sparse Grids
              | for Efficient Stochastic
              | Optimization
blank         | 
text          | To this point, all partial differential equations, and the corresponding optimization problems, have
              | been posed in a deterministic setting, that is, the PDE itself and all its data are assumed known. This
              | is not a realistic assumption since all PDEs are merely mathematical models of physical phenomena
              | and even if the PDE is an accurate approximation of reality, its data—coefficients, boundary con-
              | ditions, source terms, etc—will rarely be known with certainty. This is particularly true in physical
              | systems characterized by a high degree of volatility or those where physical measurements are diffi-
              | cult to take. In such settings, the uncertainty must be incorporated into the optimization problem
              | if a robust, risk-averse design or control is to be attained. In this work, parametrized uncertainties
              | are considered and risk-averse measures (Section 2.2.1) of quantities of interest will be used as the
              | objective and constraint functions for the stochastic PDE-constrained optimization problem. The
              | mathematical construction and discretized of parametrized stochastic partial differential equations
              | is provided in Section 2.2, including the introduction of a complete probability space, the finite
              | noise assumption, spatio-temporal discretization of a realization of the stochastic partial differential
              | equation, and collocation-based discretization of the stochastic space. Since risk-averse measures
              | usually require the computation of an integral over the stochastic space, a single query to an op-
              | timization function requires the evaluation of an integral whose integrand depends on the solution
              | of a realization of the stochastic partial differential equation. In general, this requires a (possibly
              | large) ensemble of deterministic PDE solves and makes stochastic PDE-constrained optimization
              | problems potentially many orders of magnitude more expensive than the deterministic counterparts.
              | In fact, if there are large number of stochastic parameters, it is difficult to evaluate an integral
              | over a high-dimensional space even if the integrand is inexpensive to evaluate due to the curse of
              | dimensionality. A straightforward or brute force approach to solve such optimization problems is
blank         | 
              | 
meta          |                                                   170
text          | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 171
blank         | 
              | 
              | 
text          | infeasible for all but the simplest problems.
              |    To address the large cost of PDE-constrained optimization under uncertainty, a multifidelity trust
              | region method based on the theory introduced in Chapter 3 is developed. The approximation model
              | incorporates two levels of inexactness: dimension-adaptive sparse grids for efficient collocation-
              | based integration in moderate-to-high dimensional spaces and reduced-order models to reduce the
              | cost of queries to a realization of the stochastic partial differential equation. Both levels of the
              | approximation will be incorporated into the required error indicators. A two-level greedy method is
              | proposed to construct the sparse grid and reduced-order basis such that, at a given iteration of the
              | trust region method, the required error conditions are satisfied, thus ensuring global convergence.
              | The proposed method is demonstrated on a one-dimensional optimal flow control problem. For
              | simplicity, the remainder of this document will consider only the risk-neutral measure, or expectation,
              | of a quantity of interest. Extension to other risk-averse measures will be deferred to later work.
blank         | 
              | 
title         | 6.1     Background
text          | This chapter begins with an overview of ingredients that will be necessary to develop the proposed
              | trust region method based on the two-level approximation of risk measures of quantities of interest
              | of stochastic PDEs: stochastic reduced-order models and anisotropic sparse grids.
blank         | 
              | 
title         | 6.1.1    Stochastic High-Dimensional Model
text          | Consider the discrete collocation-based stochastic PDE introduced in Section 2.2
blank         | 
text          |                                         r(u, µ, y) = 0         ∀y ∈ Ξ                             (6.1)
blank         | 
text          | where u ∈ RNu is the state vector, µ ∈ RNµ is the parameter vector, y ∈ Ξ are the stochastic
              | variables, and Ξ ⊂ RNy is the stochastic space. The existence of a continuously differentiable
              | function u(µ; y), defined as the solution of r( · , µ, y) = 0, is guaranteed by Theorem 2.1, under
              | suitable assumptions. Depending on the nature of the stochastic variables, the quantity of interest
              | may be stochastic as well and a realization will take the form
blank         | 
text          |                                                  f (u, µ, y)                                      (6.2)
blank         | 
text          | for y ∈ Ξ, which can be considered only a function of µ and y using the implicit definition u(µ; y)
blank         | 
text          |                                        F (µ; y) = f (u(µ; y), µ, y).                              (6.3)
blank         | 
text          | The risk-neutral measure of the QoI, which will be used as the objective for the stochastic optimiza-
              | tion problem in this work, is
blank         | 
text          |                                 F (µ) = E[f (u(µ; · ), µ, · )] = E[F (µ, · )].                    (6.4)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 172
blank         | 
              | 
              | 
text          | Generalization to other risk-averse measures proceeds by replacing E[·] with R[·] defined in (2.51)-
              | (2.55); however, special care will be required in the construction of quadrature rules for non-smooth
              | risk-averse measures.
              |    The gradient of the risk-neutral measure of the QoI, ∇F (µ), is computed via the sensitivity or
              | adjoint method, depending on the number of QoIs versus the number of parameters (Nµ ). Since
              | the differentiation operation (with respect to µ) can be pulled inside the expectation operation
              | (integration with respect to y), the gradient takes the form
blank         | 
text          |                           ∇F (µ) = E [∇f (u(µ; · ), µ, · )] = E [∇F (µ, · )] .                    (6.5)
blank         | 
text          | This illustrates that the computation of the gradient of the risk-neutral measure of the QoI reduces
              | to an integral over realizations of the QoI gradients, i.e., for a fixed y ∈ Ξ and µ ∈ RNµ . The
              | gradient of a particular realization proceeds exactly according to the adjoint or sensitivity method
              | outlined in Sections 2.3.3 and 2.3.4. Following the procedures outlined in that section, the stochastic
              | variant of the sensitivity and adjoint residuals are
blank         | 
text          |                                               ∂f             ∂f
              |                          r ∂ (u, w, µ, y) :=     (u, µ, y) +     (u, µ, y)w
              |                                               ∂µ             ∂u
              |                                                                                                   (6.6)
              |                                               ∂f                 ∂r
              |                           r λ (u, z, µ, y) :=    (u, µ, y) − z T    (u, µ, y).
              |                                               ∂µ                 ∂µ
blank         | 
text          | Then, a realization of the sensitivity problem for a fixed y ∈ Ξ and µ ∈RNµ is: given the  primal
              |                                                          ∂u                         ∂u
              | solution u(µ; y) that satisfies r( · , µ, y) = 0, find       such that r ∂ u(µ; y),    , µ, y = 0.
              |                                                          ∂µ                         ∂µ
              | Similarly, a realization of the adjoint problem for a fixed y ∈ Ξ and µ ∈ RNµ is: given the primal
              | solution u(µ; y), find λ such that r λ (u(µ; y), λ, µ, y) = 0. The sensitivity and adjoint solution,
              |                                                           ∂u
              | for a particular y ∈ Ξ and µ ∈ RNµ , will be denoted         (µ; y) and λ(µ; y), respectively. The
              |                                                           ∂µ
              | reconstruction of the gradient of a QoI from a sensitivity or adjoint solution are generalized from
              | the deterministic case in (2.90), (2.102) to the stochastic case as
blank         | 
text          |                                               ∂f             ∂f
              |                          g ∂ (u, w, µ, y) :=     (u, µ, y) +     (u, µ, y)w
              |                                               ∂µ             ∂u
              |                                                                                                   (6.7)
              |                                               ∂f                 ∂r
              |                           g λ (u, z, µ, y) :=    (u, µ, y) + z T    (u, µ, y).
              |                                               ∂µ                 ∂µ
blank         | 
text          | With these definitions, a realization of the gradient of a QoI corresponding to y ∈ Ξ and µ ∈ RNµ
              | takes the form
blank         |                                                  
text          |                            ∂       ∂u
              |             ∇F (µ, y) = g u(µ; y),    (µ; y), µ, y = g λ (u(µ; y), λ(µ; y), µ, y)                 (6.8)
              |                                    ∂µ
blank         | 
text          | and the gradient of the risk-neutral measure in (6.4) is
blank         |                                                 
text          |                       ∂          ∂u
              |                                                      = E g λ (u(µ; · ), λ(µ; · ), µ, · ) .
blank         |                                                                                        
text          |           ∇F (µ) = E g u(µ; · ),    (µ; · ), µ, ·                                                 (6.9)
              |                                  ∂µ
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 173
blank         | 
              | 
              | 
title         | 6.1.2    Stochastic Reduced-Order Model
text          | The dimension of the discretized stochastic PDE in (6.1) is reduced through the introduction of the
              | model reduction ansatz u = Φur from (4.2) into (6.1), where Φ ∈ RNu ×ku is the trial basis that
              | defines a subspace that (approximately) contains the solution of any realization of the stochastic
              | PDE, i.e., u(µ, y) for µ ∈ RNµ and y ∈ Ξ. The result is an overdetermined nonlinear system
              | of equations r(Φur , µ, y) = 0 for any realization y ∈ Ξ. Projection of these equations onto the
              | columnspace of the test basis Ψ ∈ RNu ×ku leads to the projection-based reduced-order model with
              | ku equations and unknowns
blank         | 
text          |                                     rr (ur , µ, y) := ΨT r(Φur , µ, y) = 0.                    (6.10)
blank         | 
text          | This work will primarily be consider minimum-residual reduced-order models (Definition 4.1), which
              | completely prescribes the test basis Ψ based on the trial basis Φ and optimality metric Θ. For a
              | given µ ∈ RNµ and realization y ∈ Ξ, the solution of (6.10) will be denoted ur (µ; y, Φ, Ψ), which
              | will be shortened to ur (µ; y) when there is no risk of confusion regarding the choice of test and
              | trial basis. From Theorem 2.1, ur (µ; y, Φ, Ψ) is a continuously differentiable function of µ. A
              | realization of the reduced quantity of interest takes the form f (Φur , µ, y), which can be considered
              | solely a function of µ and y through the implicit solution of (6.10)
blank         | 
text          |                               Fr (µ; y, Φ, Ψ) = f (Φur (µ; y, Φ, Ψ), µ, y).                    (6.11)
blank         | 
text          | Finally, the risk-neutral measure of the reduced quantity of interest, which serves as an approxima-
              | tion for the risk-neutral measure of the true quantity of interest in (6.4), is
blank         | 
text          |                  Fr (µ; Φ, Ψ) = E[f (Φur (µ; · , Φ, Ψ), µ, · )] = E[Fr (µ; · , Φ, Ψ)].         (6.12)
blank         | 
text          | Following the exposition in Sections 4.1.2 and 4.1.3, the gradient of the risk-neutral measure is
              | computed according to the sensitivity or adjoint method as
blank         |                                                                                   
text          |                                    ∂                       ∂ur
              |                ∇Fr (µ; Φ, Ψ) = E g Φur (µ; · , Φ, Ψ), Φ        (µ; · , Φ, Ψ), µ, ·
              |                                                            ∂µ                                  (6.13)
              |                              = E g λ (Φur (µ; · , Φ, Ψ), Ψλr (µ; · , Φ, Ψ), µ, · ) .
blank         |                                                                                  
              | 
              | 
text          |                                                                                    ∂ur
              | where Φur (µ; y, Φ, Ψ) is the reconstructed primal solution for realization y ∈ Ξ,      (µ; y, Φ, Ψ)
              |                                                                                    ∂µ
              | is the reduced sensitivity, and λr (µ; y, Φ, Ψ) is the reduced adjoint. The minimum-residual variants
              | of the reduced gradient computation in (6.13) can be used in place of ∇Fr (µ)
              |                                           "                                             !#
              |                           ∂     ∂             ∂    ∂u
              |                                                    dr
              |                                                         ∂         ∂    ∂
              |         ∇F
              |         dr (µ; Φ, Ψ, Φ , Θ ) = E g     u( · ), Φ        (µ; · , Φ , Θ , u( · )), µ, ·
              |                                                     ∂µ                                         (6.14)
              |                                   h                                                  i
              |         dr (µ; Φ, Ψ, Φλ , Θλ ) = E g λ u( · ), Φλ λ̂r (µ; · , Φλ , Θλ , u( · )), µ, ·
              |         ∇F
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 174
blank         | 
              | 
              | 
text          | where u(y) = Φur (µ; y, Φ, Ψ) is the reconstructed primal solution for realization y ∈ Ξ,
              | ∂u
              |  dr
              |     (µ; y, Φ∂ , Θ∂ , u(y)) is the solution of the minimum-residual sensitivity equations in (4.28)
              |  ∂µ
              | and λ̂r (µ; y, Φλ , Θλ , u(y)) is the solution of the minimum-residual adjoint equations in (4.56)
              | for the realization corresponding to y ∈ Ξ. Using Propositions 4.3 and 4.5 as motivation, the
              | sensitivity/adjoint bases and optimality metrics are chosen according to (4.35) and (4.63). This
              | implies that the selection of Φ and Ψ completely specify Φ∂ and Φλ . From these propositions,
              | the exact and minimum-residual sensitivities will only agree if the test basis Ψ is constant (such
              | as a Galerkin projection) or the primal reduced-order model solution is exact for each realization
              | y ∈ Ξ. Since training a reduced-order model to be exact for all y ∈ Ξ is impractical, the relation
              | ∇Fr (µ) = ∇F
              |           dr (µ) will only hold if the test basis is constant.
              |    The residual-based error bounds derived in Appendix B hold for a particular realization y ∈ Ξ
              | of the stochastic PDE, provided Assumptions (AR1)–(AR8), (AQ1)–(AQ4) hold for this realization.
              | The primal, sensitivity, and adjoint residual error bounds for a realization y ∈ Ξ are
blank         | 
text          |                            |f (u(µ, y), µ, y) − f (u, µ, y)| ≤ ζ kr(u, µ, y)k
blank         |                                      
text          |                        ∂u
              |      g∂       u(µ, y),    (µ, y), µ, y − g ∂ (u, w, µ, y) ≤ κ kr(u, µ, y)k + τ r ∂ (u, w, µ, y)
              |                        ∂µ
              |           g λ (u(µ, y), λ(µ, y), µ, y) − g λ (u, z, µ, y) ≤ κ kr(u, µ, y)k + τ r λ (u, z, µ, y)
              |                                                                                                (6.15)
              | where u = Φur (µ; y, Φ, Ψ) is the reconstructed primal solution, w is the reconstructed reduced sen-
              |                                                   ∂ur                         ∂u
              |                                                                               dr
              | sitivity (exact or minimum-residual), i.e., w = Φ     (µ; y, Φ, Ψ) or w = Φ∂      (µ; y, Φ∂ , Θ∂ , u),
              |                                                   ∂µ                          ∂µ
              | and z is the reconstructed reduced adjoint (exact or minimum-residual), i.e., z = Ψλr (µ; y, Φ, Ψ)
              | or z = Φλ λr (µ; y, Φλ , Θλ , u).
              |    Finally, the stochastic generalization of the collocation-based hyperreduced models of Section 4.2
              | follows immediately from the construction in that section and takes the form
blank         | 
text          |                                 (P T Ψ)T P T r(Φur , µ, y) = 0   ∀y ∈ Ξ                         (6.16)
blank         | 
text          | The case of stochastic hyperreduction will not be considered further as only problems amenable
              | to precomputations (polynomial nonlinearities) will be considered in the numerical experiments
              | (Section 6.4).
              |    While the introduction of the stochastic reduced-order and hyperreduced models in this section
              | reduces the cost of evaluating risk-averse measures of PDE quantities of interest, e.g., for stochastic
              | optimization, they may still be prohibitively expensive due to the curse of dimensionality. The
              | next section introduces anisotropic sparse grids to mitigate or delay the impact of the curse of
              | dimensionality when evaluating risk-averse measures in moderate-to-large dimensional stochastic
              | spaces.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 175
blank         | 
              | 
              | 
title         | 6.1.3     Anisotropic Sparse Grids
text          | Consider the difficult problem of evaluating the expectation of a smooth function g : RNy → R
              |                                               Z
              |                                        E[g] =    ρ(y)g(y) dy                                (6.17)
              |                                                     Ξ
blank         | 
              | 
text          | where Ξ ⊂ [−1, 1]Ny is the stochastic space and ρ : Ξ → R+ is the joint probability density
              | function with marginal probability density functions ρk : Ξk → R+ for k = 1, . . . , Ny such that
              | ρ = ρ1 ⊗ · · · ⊗ ρNy ). When Ny is moderate-to-large, the evaluation of the integral in (6.17) is difficult
              | since multidimensional quadrature rules derived from optimal 1D quadrature rules suffer from the
              | curse of dimensionality. Isotropic sparse grids, originally introduced in [184] and extensively studied
              | since [144, 66, 145, 156, 18, 157], generate efficient quadrature rules that delay the influence of the
              | curse of dimensionality and allows for larger stochastic spaces to be considered. Anisotropic sparse
              | grids [67] further optimize the quadrature rules by leveraging anisotropy of the integrand.
              |      The sparse grid construction begins with the definition of a one-dimensional quadrature rule of
              | level i that will be used in the kth dimension, Eik . The level is an integer used to indicate refinement
              | of the one-dimensional quadrature rule such that
              |                                             Z
              |                          Eik [h] → Ek [h] =   ρk (y)h(y) dy              as i → ∞.                  (6.18)
              |                                                Ξk
blank         | 
              | 
text          | for h : Ξk → R. Let Ξik ⊂ [−1, 1] be the quadrature nodes associated with the quadrature rule Eik .
              | While the sparse grid construction to follow holds for any valid and refinable quadrature rule that
              | satisfies (6.18), only nested quadrature rules will be considered. That is, the nodes at level i are a
              | subset of the nodes at level i + 1, Ξik ⊂ Ξi+1
              |                                            k . The nested property will not be used in the sparse
              | grid construction, but leads to an efficient implementation since, at level i + 1, only h(y) must be
              | evaluated for y ∈ Ξi+1
              |                    k   \ Ξik .
              |      From the one-dimensional quadrature rules, the corresponding difference operators are defined
              | as
              |                           ∆1k := E1k    and      ∆ik := Eik − Ei−1
              |                                                                k           for i ≥ 2.               (6.19)
blank         | 
text          | The requirement in (6.18) on the quadrature rules implies ∆ik [g] → 0 as i → 0. The one-dimensional
              | quadrature rule Eik is recovered by summing over all difference operators in dimension k up through
              | level i
              |                                                         i
              |                                                         X
              |                                               Eik =           ∆jk                                   (6.20)
              |                                                         j=1
blank         | 
text          | since the sum telescopes due to the definition of ∆ik in (6.19). A multi-dimensional difference
              | operator is constructed from a tensor product of one-dimensional difference operators, each possibly
              | at a different level of refinement
              |                                                                     iN
              |                                         ∆i := ∆i11 ⊗ · · · ⊗ ∆Nyy .                                 (6.21)
              |                       N
              | A multi-index i ∈ N+y with components i = (i1 , . . . , iNy ) is used to track the refinement level of
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 176
blank         | 
              | 
              | 
text          | each one-dimensional difference operator, i.e., ik is the refinement level of the difference operator in
              | dimension k. From the multi-dimensional difference operator, a quadrature rule EI is defined by
              |                                                                          N
              | summing over all multi-indices in a multi-index set I ⊂ N+y
              |                                            X               X                     iN
              |                                     EI =         ∆i :=           ∆i11 ⊗ · · · ⊗ ∆Nyy .            (6.22)
              |                                            i∈I             i∈I
blank         | 
              | 
text          | Let ΞI ⊂ [−1, 1]Ny denote the quadrature nodes associated with the multi-dimensional quadrature
              | rule EI . The use of nested, one-dimensional quadrature rules implies ΞI ⊂ ΞJ for I, J multi-index
              | sets such that I ⊂ J . In the multi-dimensional case, this leads to substantial savings as evaluations
              | of g can be recycled as the sparse grid is refined.
              |                                                                                           N
              |    For EI to be a convergent quadrature rule, i.e., EI → E as I → N+y , a telescoping property
              | similar to that in (6.20) must hold. This requirement is satisfied if the multi-index I is admissible
              | in the sense of Definition 6.1.
              |                                            N
              | Definition 6.1. An index set I ⊂ N+y is admissible if for all k ∈ I,
blank         | 
text          |                                    k − ej ∈ I        for       1 ≤ j ≤ Ny , k j > 1               (6.23)
blank         | 
text          |    This completes the construction of general, anisotropic sparse grids. From this general construc-
              | tion, some well-known special cases can be recovered. The tensor product quadrature rule of level
              |                                                                         N
              | i, Ei1 ⊗ · · · ⊗ EiNy , can written as EI∞
              |                                          i
              |                                                  i
              |                                            with I∞ = {i ∈ N+y | |i|∞ ≤ i}, i.e.,
              |                                                       X
              |                           Ei1 ⊗ · · · ⊗ EiNy [g] =             (∆i1 ⊗ · · · ⊗ ∆iNy )[g] = EI∞
              |                                                                                             i .   (6.24)
              |                                                      |i|∞ ≤i
blank         | 
              | 
text          | Figure 6.1 provides an example of a tensor product quadrature rule, and the corresponding index
              | set, based on Clenshaw-Curtis quadrature rules; the index set is dense and leads to a quadrature
              | rule with the maximum number of nodes. The isotropic Smolyak sparse grid of level i is
              |                                        X
              |                                                  (∆i1 ⊗ · · · ⊗ ∆iNy )[g] = EIiso
              |                                                                               i ,                 (6.25)
              |                                   |i|1 ≤i+Ny +1
blank         | 
              | 
text          |        i              N
              | where Iiso = {i ∈ N+y | |i|1 ≤ i + Ny − 1}. See Figure 6.2 for an illustration of the quadrature
              | nodes and index set; the index set is only refined along the diagonal, which leads to much sparser
              | quadrature rules than direct tensor products. Finally, Figure 6.3 illustrates an anisotropic sparse
              | grid, including quadrature nodes and index set, which further reduces the number of quadrature
              | compared to the other options and (potentially) takes advantage of anisotropy in the integrand g(y)
              | and probability density function ρ(y).
              |    The neighbor of a sparse grid is the final concept introduced in this section and will be used
              | extensively in assessing the truncation error that arises from approximating E[g] by EI [g]. The set
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 177
blank         | 
              | 
              | 
text          |                                                           N                            N
              | of neighbors corresponding to a sparse grid I ⊂ N+y , denoted N (I) ⊂ N+y is defined as
blank         | 
text          |                                N (I) := {i ∈ I c | I ∪ {i} is admissible}                        (6.26)
blank         | 
text          |                                                                   N                        N
              | where I c is the complement of the multi-index set I in N+y , i.e., I c = {i ∈ N+y | i 6∈ I}. Figure 6.4
              | shows the quadrature nodes and index set corresponding to the anisotropic sparse grid, including
              | neighbors, in Figure 6.3. Following the work in [67, 108, 109], the truncation error, which can be
              | written as the infinite sum
              |                                                   X                       iN
              |                                E[g] − EI [g] =           (∆i11 ⊗ · · · ⊗ ∆Nyy )[g]               (6.27)
              |                                                  i∈I c
blank         | 
              | 
text          | can be approximated as
              |                                                  X                         iN
              |                               E[g] − EI [g] ≈             (∆i11 ⊗ · · · ⊗ ∆Nyy )[g].             (6.28)
              |                                                 i∈N (I)
blank         | 
              | 
text          | The concept and notation used to represent neighbors of a sparse grid is easily extended to handle
              | j layers of neighbors, that is, N (I) is the 1st layer of neighbors, N (N (I)) is the 2nd layer, and
              | N j (I) := N
              |            | ◦ ·{z
              |                  · · ◦ N} ◦I is the jth layer. A more accurate approximation of the truncation error
              |              j terms
              | is attainable by including more distant neighbors, but expense of the corresponding computation
              | rapidly increases. For this reason, usually only the first layer of neighbors is used to approximate
              | the truncation error [67, 108, 109], which is the approach taken in this work.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 178
blank         | 
              | 
              | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |      i2   4                                     4                                    4
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
              |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                 1 2 3 4 5 6                           1 2 3 4 5 6                          1 2 3 4 5 6
              |                      i1                                    i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | ξ2
blank         | 
              | 
              | 
              | 
text          |                                       ξ2
blank         | 
              | 
              | 
              | 
text          |                                                                            ξ2
              |           0                                     0                                    0
blank         | 
text          |      −0.5                                  −0.5                                 −0.5
blank         | 
text          |       −1                                    −1                                   −1
              |               −1 −0.5 0     0.5   1                 −1 −0.5 0    0.5   1                 −1 −0.5 0    0.5   1
              |                       ξ1                                    ξ1                                   ξ1
blank         | 
              | 
text          |                 Figure 6.1: Full tensor product based on Clenshaw-Curtis (levels 1, 3, 5)
blank         | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |           4                                     4                                    4
              |      i2
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
blank         | 
text          |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                 1 2 3 4 5 6                           1 2 3 4 5 6                          1 2 3 4 5 6
              |                      i1                                    i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | ξ2
blank         | 
              | 
              | 
              | 
text          |                                       ξ2
blank         | 
              | 
              | 
              | 
text          |                                                                            ξ2
blank         | 
              | 
              | 
              | 
text          |           0                                     0                                    0
blank         | 
text          |      −0.5                                  −0.5                                 −0.5
blank         | 
text          |       −1                                    −1                                   −1
              |               −1 −0.5 0     0.5   1                 −1 −0.5 0    0.5   1                 −1 −0.5 0    0.5   1
              |                       ξ1                                    ξ1                                   ξ1
blank         | 
              | 
text          |                 Figure 6.2: Isotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 179
blank         | 
              | 
              | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |      i2   4                                     4                                    4
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
              |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                  1 2 3 4 5 6                          1 2 3 4 5 6                          1 2 3 4 5 6
              |                       i1                                   i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | ξ2
blank         | 
              | 
              | 
              | 
text          |                                       ξ2
blank         | 
              | 
              | 
              | 
text          |                                                                            ξ2
              |           0                                     0                                    0
blank         | 
text          |      −0.5                                  −0.5                                 −0.5
blank         | 
text          |       −1                                    −1                                   −1
              |               −1 −0.5 0     0.5   1                 −1 −0.5 0    0.5   1                 −1 −0.5 0    0.5   1
              |                       ξ1                                    ξ1                                   ξ1
blank         | 
              | 
text          |                Figure 6.3: Anisotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5)
blank         | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |           4                                     4                                    4
              |      i2
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
blank         | 
text          |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                  1 2 3 4 5 6                          1 2 3 4 5 6                          1 2 3 4 5 6
              |                       i1                                   i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | ξ2
blank         | 
              | 
              | 
              | 
text          |                                       ξ2
blank         | 
              | 
              | 
              | 
text          |                                                                            ξ2
blank         | 
              | 
              | 
              | 
text          |           0                                     0                                    0
blank         | 
text          |      −0.5                                  −0.5                                 −0.5
blank         | 
text          |       −1                                    −1                                   −1
              |               −1 −0.5 0     0.5   1                 −1 −0.5 0    0.5   1                 −1 −0.5 0    0.5   1
              |                       ξ1                                    ξ1                                   ξ1
blank         | 
              | 
text          | Figure 6.4: Anisotropic sparse grid based on Clenshaw-Curtis with all (including non-admissible)
              | forward neighbors (levels 1, 3, 5)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 180
blank         | 
              | 
              | 
title         | 6.2     Two Levels of Approximation of Risk-Averse Measures
text          | The two approximation technologies introduced—anisotropic sparse grids for the efficient approxima-
              | tion of high-dimensional integrals and stochastic reduced-order models to reduce the cost associated
              | with solving a realization of the SPDE—are combined to yield an inexpensive approximation of
              | risk-averse measures of quantities of interest of high-fidelity partial differential equations. For the
              | remainder of this section, suppose a sparse grid I and reduced-order model (Φ, Ψ) are given – the
              | construction of each will be considered in detail in Section 6.3.2. The two-level approximation of
              | the risk-averse measure of the quantity of interest in (6.4) is
blank         | 
text          |                             Fr (µ; Φ, Ψ, I) := EI [f (Φur (µ; ·, Φ, Ψ), µ, · )].                 (6.29)
blank         | 
text          | The introduction of the sparse grid introduces a truncation error into the evaluation of the integral
              | and the reduced-order model introduces an error in the evaluation of the quantity of interest at
              | each collocation node. The benefit of such an approximation is that the many high-dimensional
              | model solutions required to evaluate F (µ) are replace by few reduced-order model solutions to
              | evaluate Fr (µ). The introduction of the sparse grid further benefits the reduced-order model since
              | it only needs to be trained on the collocation nodes instead of everywhere in Ξ. The gradient of the
              | approximation in (6.29) is computed according to the sensitivity or adjoint method as
blank         |                                                                                  
text          |                                                            ∂ur
              |              ∇Fr (µ; Φ, Ψ, I) = EI g ∂ Φur (µ; ·, Φ, Ψ), Φ     (µ; ·, Φ, Ψ), µ, ·
              |                                                            ∂µ                                    (6.30)
              |                                    λ                                            
              |                               = EI g (Φur (µ; ·, Φ, Ψ), Ψλr (µ; · , Φ, Ψ), µ, · ) .
blank         | 
text          | If the true sensitivity and adjoint of the stochastic reduced-order model are too cumbersome to
              | compute, i.e., if second derivatives of r are required (see Chapter 4), the minimum-residual variants
              | can be used to compute an approximation to ∇Fr (µ) as
              |                                           "                                              !#
              |                         ∂     ∂               ∂      ∂u
              |                                                      dr
              |                                                           ∂         ∂    ∂
              |       ∇F
              |       dr (µ; Φ, Ψ, Φ , Θ , I) = EI g     u( · ), Φ        (µ; ·, Φ , Θ , u( · )), µ, ·
              |                                                       ∂µ                                         (6.31)
              |                                     h                                                  i
              |       dr (µ; Φ, Ψ, Φλ , Θλ , I) = EI g λ u( · ), Φλ λ̂r (µ; · , Φλ , Θλ , u( · )), µ, ·
              |       ∇F
blank         | 
text          | where u(y) = Φur (µ; y, Φ, Ψ) is the reconstructed primal reduced-order model solution of the
              | realization corresponding to y ∈ Ξ.
              |    The error incurred by approximating F (µ) with Fr (µ; Φ, Ψ, I) must account for both the
              | truncation error introduced by the sparse grid and the pointwise error in the reduced-order model.
              | These terms arise naturally from a simple application of the triangle inequality to the error
blank         | 
text          |        |F (µ) − Fr (µ; Φ, Ψ, I)| = |E[f (u(µ; · ), µ, · ) − EI [f (Φur (µ; · , Φ, Ψ), µ, · )]|
              |                                    ≤ E[|f (u(µ; · ), µ, · ) − f (Φur (µ; · , Φ, Ψ), µ, · )|]     (6.32)
              |                                           + EI c [|f (Φur (µ; · , Φ, Ψ), µ, · )|]
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 181
blank         | 
              | 
              | 
text          | where EI c := E − EI was used (Section 6.1.3). The first term in the error bound is the integrated
              | reduced-order model error and the second is the truncation error that results from using the sparse
              | grid I to integrate the reduced quantity of interest. While the error bound is instructive in un-
              | derstanding the sources of error, it can not be efficiently computed due to the presence of the true
              | error in the first integrand and the infinite sum required to compute the expectations in both terms
              | (quadrature over an infinite set of collocation nodes to compute the integral exactly). The residual-
              | based error bounds from Appendix B are used to circumvent the first issue by bounding the true
              | error by an arbitrary constant (ζ > 0) times the residual norm
blank         | 
text          |   |F (µ) − Fr (µ; Φ, Ψ, I)| ≤ ζE[kr(Φur (µ; · , Φ, Ψ), µ, · )k] + EI c [|f (Φur (µ; · , Φ, Ψ), µ, · )|]
              |                              = ζE[kr(Φur (µ; · , Φ, Ψ), µ, · )k] + EI c [|Fr (µ; · , Φ, Ψ)|],
              |                                                                                                    (6.33)
              | where the definition of Fr , introduced in Section 6.1.2, was used in the second line. The infinite sums
              | required to compute both expectations are reduced to finite sums by approximating the complement
              | of the sparse grid I c (infinite set of collocation points) with the forward neighbors of the sparse grid
              | N (I) (finite set of collocation points). With this approximation, the expectation operator E and
              | truncation operator EI c become
blank         | 
text          |                           E := EI∪I c ≈ EI∪N (I)       and      EI c ≈ EN (I) .                    (6.34)
blank         | 
text          | The introduction of this approximation into the error bound in (6.33) reduces the uncomputable
              | right-hand side (due to the infinite sums required for evaluation of the expectation and truncation
              | operators) to
blank         | 
text          |    |F (µ) − Fr (µ; Φ, Ψ, I)| . ζEI∪N (I) [kr(Φur (µ; · , Φ, Ψ), µ, · )k] + EN (I) [|Fr (µ; · , Φ, Ψ)|],
              |                                                                                                    (6.35)
              | which is amenable to computation as only finite summations are required. In general, the right-
              | hand side of (6.35) does not bound the left-hand side due to the introduction of the approximation
              | I c ≈ N (I). While this approximation does not necessarily preserve the error bound in (6.33), it
              | leads to an inexpensive error indicators: the right-hand side of (6.35) only requires reduced-order
              | models solves and residual evaluations on the sparse grid I and its neighbors N (I).
              |    An identical procedure is carried out to convert the pointwise error bounds in (6.13) for a given
              | realization of the stochastic PDE to an inexpensive error indicator. The error indicator for gradients
              | computed via the sensitivity method takes the form
blank         | 
text          |    |∇F (µ) − ∇Fr (µ; Φ, Ψ)| . κEI∪N (I) [kr(Φur (µ; · , Φ, Ψ), µ, · )k] +
blank         |                                                                                              
text          |                                                                     ∂ur
              |                               τ EI∪N (I) r ∂ (Φur (µ; · , Φ, Ψ), Φ       (µ; · , Φ, Ψ), µ, · ) +
              |                                                                      ∂µ
              |                                    EN (I) [k∇Fr (µ; Φ, Ψ)k]
              |                                                                                                    (6.36)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 182
blank         | 
              | 
              | 
text          | and for gradients computed via the adjoint method, it takes the form
blank         | 
text          |     |∇F (µ) − ∇Fr (µ; Φ, Ψ)| . κEI∪N (I) [kr(Φur (µ; · , Φ, Ψ), µ, · )k] +
              |                                τ EI∪N (I) r λ (Φur (µ; · , Φ, Ψ), Ψλr (µ; · , Φ, Ψ), µ, · ) +
blank         |                                                                                           
              | 
text          |                                     EN (I) [k∇Fr (µ; Φ, Ψ)k] .
              |                                                                                                 (6.37)
              | Similar to the error indicator in (6.35) for the value of quantity of interest, the gradient error
              | indicators in (6.36) and (6.37) have terms that separately account for the reduced-order model error
              | and integral truncation error. The three terms in these error indicators account for the error in the
              | primal reduced-order model solution, the error in the reduced-order model sensitivity/adjoint, and
              | truncation error from approximating the expectation operators with the sparse grid I, respectively.
              | The gradient error indicators must include the terms that accounts for the error in the primal solution
              | since, in general, the sensitivity/adjoint equations are defined about an approximate linearization
              | point. The gradient error indicators for the minimum-residual sensitivity and adjoint reduced-order
              | model follow in a similar manner
blank         | 
text          |           dr (µ; Φ, Ψ, Φ∂ , Θ∂ )| . κEI∪N (I) [kr(u( · ), µ, · )k] +
              | |∇F (µ) − ∇F
              |                                               "                                                     #
              |                                                                ∂ ∂ur
              |                                                                  d
              |                                                   ∂                           ∂   ∂
              |                                     τ EI∪N (I) r (u( · ), Φ          (µ; · , Φ , Θ ; u( · )), µ, · ) +
              |                                                                  ∂µ
              |                                             h                               i
              |                                       EN (I) ∇Fdr (µ; Φ, Ψ, Φ∂ , Θ∂ )
blank         | 
text          |           dr (µ; Φ, Ψ, Φλ , Θλ )| . κEI∪N (I) [kr(Φur (µ; · , Φ, Ψ), µ, · )k] +
              | |∇F (µ) − ∇F
              |                                               h                                                     i
              |                                     τ EI∪N (I) r λ (u( · ), Φλ λ̂r (µ; · , Φλ , Θλ , u( · )), µ, · ) +
              |                                             h                               i
              |                                       EN (I) ∇Fdr (µ; Φ, Ψ, Φλ , Θλ ) .
              |                                                                                                 (6.38)
              | where u(y) = Φur (µ; y, Φ, Ψ) is the reconstructed primal solution for realization y ∈ Ξ.
              |    At this point, the proposed two-level approximation of risk-averse measures of quantities of inter-
              | est based on anisotropic sparse grids and model reduction has been introduced and relevant details
              | pertaining to gradients and computable error indicators have been discussed. The next section uses
              | this technology as the approximation model in the multifidelity trust region method of Chapter 3 to
              | yield an efficient algorithm to solve stochastic PDE-constrained optimization problems. To simplify
              | the exposition in the next section, details pertaining to the use of the minimum-residual sensitiv-
              | ity/adjoint reduced-order models to approximate ∇Fr (µ) with ∇F    dr (µ) will be dropped. These
              | details follow in a straightforward manner from those corresponding to the exact sensitivity/adjoint
              | method to compute ∇Fr (µ). Furthermore, the numerical experiments in Section 6.4 will solely
              | consider a reduced-order models based on a Galerkin projection Ψ = Φ, which implies the test
              | basis is constant and ∇F
              |                       dr (µ) = ∇Fr (µ), provided the sensitivity and adjoint bases are chosen
              | according to (4.35), (4.63). Therefore, the distinction between the exact and minimum-residual
              | sensitivity/adjoint methods is irrelevant since they are identical in this case.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 183
blank         | 
              | 
              | 
title         | 6.3       Multifidelity Trust Region Method Based on Two-Level
              |           Approximation
text          | This section presents the primary contribution of this chapter: the use of sparse grids and model
              | reduction in the multifidelity trust region framework of Chapter 3 to yield an efficient algorithm for
              | stochastic PDE-constrained optimization. The approximation model, mk (µ), that is central to the
              | trust region theory will be taken as the two-level approximation of risk-averse measures of quantities
              | of interest introduced in the previous section. The error indicators required for the trust region
              | theory are inspired from the error indicators in (6.35), (6.36)-(6.37). A two-level greedy algorithm
              | will be introduced in Section 6.3.2 that combines dimension-adaptive sparse grid construction [67]
              | with a classical reduced basis greedy method [149, 173]. The purpose of the greedy algorithm is to
              | simultaneously construct a sparse grid Ik and reduced-order model Φk , Ψk such that the two-level
              | approximation is sufficiently accurate to guarantee convergence based on the requirements (3.14),
              | (3.15), (3.22) detailed in Chapter 3.
              |    Before proceeding to the exposition of the multifidelity trust region method, additional notation
              | will be introduced for convenience. In particular, each component of the error indicators in (6.35)
              | and (6.36)-(6.37) are separated into individual terms. Define the following primal error terms from
              | (6.35)
              |                        E1 (Φ, Ψ, I, µ) := EI∪N (I) [kr(Φur (µ; · , Φ, Ψ), µ, · )k]
              |                                                                                                (6.39)
              |                        E2 (Φ, Ψ, I, µ) := EN (I) [|f (Φur (µ; · , Φ, Ψ), µ, · )|] ,
blank         | 
text          | where E1 is the (integrated) reduced-order model error indicator and E2 is the truncation error
              | indicator associated with using the sparse grid I in place of the true expectation. The gradient error
              | terms depend on whether the sensitivity or adjoint method are used in the gradient computation.
              | If the sensitivity method is employed, define the error terms
blank         |                                                                                         
text          |                                          ∂                   ∂ur
              |          E3 (Φ, Ψ, I, µ) := EI∪N (I) r (Φur (µ; · , Φ, Ψ), Φ     (µ; · , Φ, Ψ), µ, · )
              |                                                              ∂µ
              |                                                                                              (6.40)
              |                                      ∂                     ∂ur
              |          E4 (Φ, Ψ, I, µ) := EN (I) g (Φur (µ; · , Φ, Ψ), Φ     (µ; · , Φ, Ψ), µ, · ) .
              |                                                            ∂µ
blank         | 
text          | Otherwise, the adjoint method is used and the error terms are defined as
blank         | 
text          |           E3 (Φ, Ψ, I, µ) := EI∪N (I) r λ (Φur (µ; · , Φ, Ψ), Ψλr (µ; · , Φ, Ψ), µ, · )
blank         |                                                                                        
text          |                                                                                                (6.41)
              |           E4 (Φ, Ψ, I, µ) := EN (I) g λ (Φur (µ; · , Φ, Ψ), Ψλr (µ; · , Φ, Ψ), µ, · ) .
blank         |                                                                                      
              | 
              | 
text          | Regardless of whether the sensitivity or adjoint method is used, E3 is the (integrated) sensitiv-
              | ity/adjoint reduced-order model error indicator and E4 is the truncation error indicator associated
              | with using the sparse grid I in place of the true expectation in the gradient computation. With the
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 184
blank         | 
              | 
              | 
text          | above definitions of the individual error terms, the error indicators in (6.35), (6.36)-(6.37) become
blank         | 
text          |           |F (µ) − Fr (µ; Φ, Ψ)| ≤ ζE1 (Φ, Ψ, I, µ) + E2 (Φ, Ψ, I, µ)
              |                                                                                                       (6.42)
              |      k∇F (µ) − ∇Fr (µ; Φ, Ψ)k ≤ κE1 (Φ, Ψ, I, µ) + τ E3 (Φ, Ψ, I, µ) + E4 (Φ, Ψ, I, µ),
blank         | 
text          | where ζ, κ, τ > 0 are arbitrary constants.
blank         | 
              | 
title         | 6.3.1    Trust Region Ingredients
text          | This section will detail the various ingredients required to leverage the two-level approximation of
              | risk-averse measures of quantities of interest in the multifidelity trust region framework of Chapter 3.
              | In particular, the approximation model mk (µ), objective decrease error indicator ϑk (µ), gradient
              | error indicator ϑk (µ), and inexact objective model ψk (µ) and associated error indicator θk (µ) will
              | be specified using the developments of Section 6.2. For the remainder of this section, it is assumed
              | that, at iteration k, the sparse grid Ik and reduced-order model Φk , Ψk have been constructed.
              | Details pertaining to their construction will be provided in the next section.
              |    At the kth iteration, the approximation model is taken as the two-level approximation of the
              | risk-averse measure of the PDE quantity of interest, i.e.,
blank         | 
text          |                 mk (µ) := Fr (µ; Φk , Ψk , Ik ) = EIk [f (Φk ur (µ, · , Φk , Ψk ), µ, · )] .          (6.43)
blank         | 
text          | Similar to the trust region method detailed in Chapter 5, there are two options for the objective
              | decrease error indicators: (1) the two-level residual-based indicator introduced in Section 6.2 and
              | (2) the classical trust region constraint. The residual-based error indicator requires the pointwise
              | form of the objective condition (3.14) to leverage the error terms E1 and E2
blank         | 
text          |      |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ |F (µk ) − mk (µk )| + |F (µ) − mk (µ)|
              |                                               . ζ(E1 (Φk , Ψk , Ik , µk ) + E1 (Φk , Ψk , Ik , µ))+   (6.44)
              |                                                    E2 (Φk , Ψk , Ik , µk ) + E2 (Φk , Ψk , Ik , µ).
blank         | 
text          | for an arbitrary constant ζ > 0. Inspired from the above error indicator, the residual-based trust
              | region constraint is defined as
blank         | 
text          |                       ϑk (µ) =α1 (E1 (Φk , Ψk , Ik , µk ) + E1 (Φk , Ψk , Ik , µ)) +
              |                                                                                                       (6.45)
              |                                   α2 (E2 (Φk , Ψk , Ik , µk ) + E2 (Φk , Ψk , Ik , µ))
blank         | 
text          | for user-defined parameters α1 , α2 > 0 that balance the contribution of the reduced-order model
              | error and truncation error. However, unlike the approach taken in Chapter 5, the classical trust
              | region is primarily used in this section
blank         | 
text          |                                              ϑk (µ) = kµ − µk k .                                     (6.46)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 185
blank         | 
              | 
              | 
text          | This choice is primarily due to the fact that the objective error bound required for global convergence
              | of the trust region method (3.14)
blank         | 
text          |                              |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)
blank         | 
text          | for some constant ζ > 0, cannot be guaranteed with the residual-based choice of ϑk (µ) in (6.35)
              | due to approximate bound that results from approximating the truncation error using only the
              | collocation nodes corresponding to the neighbors of the sparse grid. From the discussion in Chapter 3,
              | the classical choice of ϑk (µ) in (6.46) guarantees the above error bound if the gradient conditions
              | (3.13),(3.15) are satisfied. Another reason for the choice of the classical trust region is that (6.45)
              | can significantly increase the cost of an iteration of the trust region subproblem since ϑk (µ) requires
              | an expectation computation over I ∪ N (I) at two points µk and µ, which may have substantially
              | more nodes that I alone (used for the evaluation of mk (µ)). Finally, the definition of ϑk (µ) is
              | not differentiable for all µ ∈ RNµ due to the presence of the norm in E1 and absolute value in E2 ,
              | which may cause convergence issues in the interior-point trust region subproblem solver discussed
              | in Section 3.1.2.
              |    From the choice of mk (µ), the gradient is ∇mk (µ) = ∇Fr (µ; Φk , Ψk , Ik ), which suggests the
              | following gradient error bound based on the approximate bound in (6.42)
blank         | 
text          |          ϕk (µ) = β1 E1 (Φk , Ψk , Ik , µ) + β2 E3 (Φk , Ψk , Ik , µ) + β3 E4 (Φk , Ψk , Ik , µ)   (6.47)
blank         | 
text          | This choice of ϕk (µ) does not guarantee the bound required by the global convergence theory in
              | Chapter 3, i.e.,
              |                                    k∇F (µk ) − ∇mk (µk )k ≤ ξϕk (µk ),                             (6.48)
blank         | 
text          | for a constant ξ > 0, due to the approximation of the truncation error on the neighbors of the
              | sparse grid. Therefore global convergence is not strictly guaranteed; however, the numerical results
              | in Sections 6.4 suggest this choice does lead to global convergence for these problems.
              |    With these choices of ϑk (µ) and ϕk (µ), the sparse grid Ik and reduced-order model Φk , Ψk
              | must be constructed to satisfy the error conditions in (3.14), (3.15), i.e.,
blank         | 
text          |                                     ϑk (µk ) ≤ κϑ ∆k
              |                                     ϕk (µk ) ≤ κϕ min{∇mk (µk ), ∆k }.
blank         | 
text          | The construction of these quantities such that the above error bounds are satisfied is somewhat
              | delicate since the error terms E1 and E3 behave differently than E2 and E4 when Φk , Ψk and Ik
              | are refined. For a fixed basis Φk , refinement of the sparse grid Ik decreases the truncation error
              | terms E2 and E4 . However, refinement of Ik may cause the model reduction error terms E1 and
              | E3 to increase since the pointwise error is integrated over an expanded set of collocation nodes.
              | A dimension-adaptive greedy algorithm that accounts for this interplay between the various error
              | terms in response to refinement of the sparse grid and reduced-order basis will be introduced in the
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 186
blank         | 
              | 
              | 
text          | next section.
              |    At this point, the basic version of the multifidelity trust region method in Algorithm 1 is fully
              | defined using the proposed two-level approximation and corresponding error indicators. An immedi-
              | ate issue with using these definitions in Algorithm 1 of Chapter 3 pertains to the evaluation of F (µ)
              | in the computation of ρk in (3.9). From its definition, evaluation of F (µ) requires an infinite sum
              | to evaluate the true expectation. The true expectation can be approximated on a “fine” quadra-
              | ture rule (possibly based on a refined sparse grid) to evaluate F (µ) to high precision. While this
              | option is simple and effective, it requires a large number of collocation nodes and the computation
              | will constitute a bottleneck in the trust region algorithm since it must be performed at each major
              | iteration. Instead, we opt to use the flexibility afforded by the trust region method in Chapter 3 for
              | inexact objective evaluations in the computation of the actual-to-predicted ratio. This follows the
              | work in [109] that uses dimension-adaptive sparse grids (without reduced-order models) for inexact
              | objective evaluations. For this purpose, a separate sparse grid Ik0 and reduced-order model Φ0k , Ψ0k
              | are introduced and, following the notation in Chapter 3, the inexact objective function, ψk (µ), is
              | employed with corresponding error indicator θk (µ) defined as
blank         | 
text          |                        ψk (µ) = Fr (µ; Φ0k , Ψ0k , Ik0 )
              |                        θk (µ) = α1 (E1 (Φ0k , Ψ0k , Ik0 , µk ) + E1 (Φ0k , Ψ0k , Ik0 , µ))+     (6.49)
              |                                  α2 (E2 (Φ0k , Ψ0k , Ik0 , µk ) + E2 (Φ0k , Ψ0k , Ik0 , µ)).
blank         | 
text          | These choices are identical to mk (µ) and the residual-based definition of ϑk (µ), based on a (possibly
              | refined) sparse grid Ik0 and reduced-order model Φ0k , Ψ0k . They do not necessarily guarantee the
              | bound required for global convergence (3.21), again due to the approximation of the truncation
              | errors on the neighbors of the sparse grid in (6.35). With these definitions, the actual-to-predicted
              | ratio is computed as
              |                                                 ψk (µk ) − ψk (µ̂k )
              |                                          ρk =                                                   (6.50)
              |                                                 mk (µk ) − mk (µ̂k )
              | where µ̂k is the solution of the trust region subproblem, i.e.,
blank         | 
text          |                           µ̂k = arg min mk (µ)             subject to ϑk (µ) ≤ ∆k .             (6.51)
              |                                    µ∈RNµ
blank         | 
              | 
text          | The sparse grid Ik0 and reduced-order model Φ0k , Ψ0k are constructed to guarantee
blank         | 
text          |                                   θkω ≤ η min{mk (µk ) − mk (µ̂k ), rk },                       (6.52)
blank         | 
text          | where ω ∈ (0, 1), η < min{η1 , 1 − η2 }, and {rk }∞
              |                                                   k=1 is a sequence such that rk → 0, using the two-
              | level dimension-adaptive greedy algorithm to be introduced in the next section. Once this training
              | algorithm is completely specified, the complete trust region algorithm will be fully prescribed and
              | is summarized in Section 6.3.3 and Algorithm 15.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 187
blank         | 
              | 
              | 
title         | 6.3.2     Greedy Construction of Sparse Grid and Reduced Basis
text          | The quality of the two-level approximation of risk-averse measures of PDE quantities of interest
              | introduced in the previous section depends critically on the sparse grid I and reduced-order basis
              | Φ used in (6.43). This section develops dimension-adaptive greedy methods for the simultaneous
              | construction of the sparse grid Ik (Ik0 ) and reduced-order basis Φk (Φ0k ) that targets each term in the
              | error indicators ϕk (µ) and θk (µ) such that the error conditions (required for global convergence) in
              | (3.14), (3.15), (3.22) are satisfied. Since the classical trust region constraint is used to define ϑk (µ),
              | the objective decrease condition (3.14) will be automatically satisfied if the gradient bound (3.13)
              | and condition (3.15) are satisfied (Chapter 3). Thus, the task reduces to construction of Ik , Φk such
              | that the gradient condition (3.15) is satisfied and Ik0 , Φ0k such that the inexact objective condition
              | (3.22) is satisfied. We begin with the gradient condition.
              |     Recall from (6.47), the gradient error indicator is a weighted sum of three terms: the primal
              | error E1 , the sensitivity/adjoint error E3 , and the gradient truncation error E4
blank         | 
text          |            ϕk (µ) = β1 E1 (Φk , Ψk , Ik , µ) + β2 E3 (Φk , Ψk , Ik , µ) + β3 E4 (Φk , Ψk , Ik , µ).
blank         | 
text          | From Chapter 3, global convergence of the multifidelity trust region method is predicated on the
              | satisfaction of the gradient condition: ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k }. A sufficient condition
              | for this gradient condition to hold is that each term satisfies an appropriate fraction of the condition,
              | i.e.,
              |                                                     κϕ
              |                           E1 (Φk , Ψk , Ik , µk ) ≤     min{k∇mk (µk )k , ∆k }
              |                                                     3β1
              |                                                     κϕ
              |                           E3 (Φk , Ψk , Ik , µk ) ≤     min{k∇mk (µk )k , ∆k }                        (6.53)
              |                                                     3β2
              |                                                     κϕ
              |                           E4 (Φk , Ψk , Ik , µk ) ≤     min{k∇mk (µk )k , ∆k }.
              |                                                     3β3
              | The purpose of the positive weights β1 , β2 , β3 , introduced in the previous section, is to balance
              | or scale the individual contributions of the error terms such the uniform split above is justified.
              | This decomposition has reduced the monolithic task of constructing a sparse grid and reduced-order
              | model such that the gradient condition in (3.15) holds to the individual tasks in (6.53). While
              | the interplay between the three error terms in (6.53) and refinement of the reduced-order model
              | and sparse grid is highly coupled and fairly complex, the following observations suggest an effective
              | training strategy: (1) for a fixed sparse grid, E1 and E3 decrease (possibly non-monotonically) as the
              | reduced-order model is hierarchically refined and (2) for a fixed reduced-order model, E4 decreases
              | (possibly non-monotonically) as the sparse grid is refined. Therefore, the construction of the reduced-
              | order model, for a fixed sparse grid, will proceed according to a variant of the classical greedy method
              | [149, 173], to target the error terms E1 and E3 . For a fixed reduced-order model, the sparse grid
              | will be adapted using the anisotropic dimension-adaptive approach [67] to target E4 . These steps
              | will be performed iteratively until the conditions in (6.53) are met. Before discussing the combined
              | algorithm in detail, the individual components, namely dimension-adaptive construction of a sparse
              | grid and greedy construction of a reduced-order model, are introduced.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 188
blank         | 
              | 
              | 
text          |    The construction of I will mimic the dimension-adaptive algorithm introduced in the seminal
              | work by Gerstner and Griebel [67] for constructing a goal-oriented, anisotropic sparse grid. In
              | this approach, the truncation error associated with the sparse grid I is approximated solely on the
              | neighbors N (I), exactly as discussed in Section 6.1.3. If this truncation error approximation is
              | larger than a specified tolerance, the multi-index in the set of neighbors that contributes most to
              | the error is added to the index set, that is, I ← I ∪ {i∗ } where
blank         | 
text          |                                         i∗ = arg min   ∆i [g] .                                 (6.54)
              |                                              i∈N (I)
blank         | 
              | 
text          | for the integrand g : RNy → R. In the context of the proposed two-level approximation, the
              | dimension-adaptive algorithm is applied to the integrand
blank         | 
text          |                                          k∇Fr (µ; · , Φ, Ψ)k
blank         | 
text          | for a fixed reduced-order model Φ, Ψ and given µ. With this integrand, the dimension-adaptive
              | algorithm decreases the error terms E4 (Φ, Ψ, · , µ). While the convergence is not necessarily mono-
              |                                                                   N
              | tonic, this term approaches zero in the limiting case as I → N+y . In fact, since E4 (Φ, Ψ, I, µ) is
              | exactly the truncation error approximation, it is used for the convergence criteria in the algorithm.
              |    The construction of the reduced-order basis follows the well-studied greedy algorithm [149, 173].
              | The original greedy algorithm improves the parametric robustness (usually over µ-space) of a
              | reduced-order basis Φ by adding snapshots of the high-dimensional model at the point where the
              | reduced-order model error is largest. Regions of high error are found by evaluating the reduced-
              | order model and an inexpensive error indicator at a (possibly large) set of candidate points (in the
              | space where the ROM is being trained) and performs a direct search for maximum value of the error
              | indicator over the candidate set. A weighted variant of the greedy algorithm was developed [42] for
              | stochastic problems with non-uniform probability distributions to train a reduced-order model over
              | the stochastic space Ξ. This weighted greedy method uses the probability density ρ(y) to weight
              | the error indicator at a particular realization y ∈ Ξ since regions with significant mass will amplify
              | errors during the expectation computation. In the same work, the weighted greedy algorithm was
              | coupled with sparse grids by using the sparse grid nodes as the candidate set; since the reduced-
              | order model is only queried on the nodes of the sparse grid, it only needs to be trained at these
              | points. In the present work, a similar weighted greedy algorithm is applied to train the reduced-order
              | model over the stochastic collocation nodes (and neighbors) ΞI∪N (I) for a fixed µ and sparse grid
              | I. Since the gradient condition is only required to hold at the trust region center, the training is
              | performed solely in stochastic space (with ΞI∪N (I) as the candidate set) for µ = µk fixed. Unlike
              | the traditional greedy methods, the proposed method builds a reduced-order model that accurately
              | represents primal and sensitivity or adjoint states over the training space. This is required since the
              | greedy algorithm will be responsible for reducing the primal E1 and sensitivity/adjoint E3 error terms
              | as both terms arise in the gradient error indicator ϕk (µ). This is achieved by adding sensitivity or
              | adjoint snapshots to the reduced-order basis, in addition to the standard primal snapshots. From
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 189
blank         | 
              | 
              | 
text          | the form of the gradient error indicator ϕk (µ) in (6.47), the primal error indicator is taken as the
              | weighted primal residual norm, i.e., the integrand in E1 ,
blank         | 
text          |                                   ρ(y) kr(Φur (µ; y, Φ, Ψ), µ, y)k                               (6.55)
blank         | 
text          | and the dual error indicator is taken as the weighted dual solution, i.e., the integrand in E3
blank         | 
text          |                                                       ∂ur
              |                       ρ(y) r ∂ (Φur (µ; y, Φ, Ψ), Φ       (µ; y, Φ, Ψ), µ, y)
              |                                                       ∂µ                                         (6.56)
              |                       ρ(y) r λ (Φur (µ; y, Φ, Ψ), Ψλr (µ; y, Φ, Ψ), µ, y) .
blank         | 
text          | Since the error terms E1 and E3 are integrated over I ∪ N (I), ΞI∪N (I) is used as the candidate
              | set. For a fixed point in parameter space µ and sparse grid I, the greedy algorithm builds up the
              | reduced-order basis using primal and sensitivity/adjoint snapshots in the described manner, un-
              | til E1 (Φ, Ψ, I, µ) and E3 (Φ, Ψ, I, µ) drop below user-defined tolerances. If a minimum-residual
              | reduced-order model is employed, the algorithm is guaranteed to terminate due to the monotonic-
              | ity property (Proposition 4.1). In the limiting case where snapshots have been added for each
              | y ∈ ΞI∪N (I) , the primal reduced-order model will be exact for each y ∈ ΞI∪N (I) and thus E1 is
              | identically zero. If the reduced-order model is exact at these sparse grid nodes, the reduced sensi-
              | tivity and adjoint method possess the minimum-residual property, which (Propositions 4.2 and 4.4)
              | guarantees the reduced sensitivity/adjoint exactly reconstruct the corresponding high-dimensional
              | quantity. Therefore E3 is identically zero. If a minimum-residual reduced-order model is employed,
              | E1 (in the appropriate norm) will actually decrease monotonically since adding snapshots to the
              | reduced-order basis can only improve the approximation quality (in terms of the residual norm in
              | a particular metric). A similar argument cannot be made for E3 , even if minimum-residual sensi-
              | tivity/adjoint reduced-order models are used, since modification of Φ alters the linearization point
              | defining the sensitivity/adjoint residual and the objective function in successive minimum-residual
              | optimization problems cannot be compared.
              |    The final training algorithm combines the dimension-adaptive sparse grid construction with
              | greedy sampling to build a reduced-order basis. For a fixed sparse grid I, the primal-sensitivity/adjoint
              | weighted greedy algorithm is used to build a reduced-order basis Φ such that E1 (Φ, Ψ, I, µ) and
              | E3 (Φ, Ψ, I, µ) satisfy (6.53). This reduced-order basis is fixed and a single step of the dimension-
              | adaptive sparse grid is applied to updated I according to I ← I ∪ {i∗ } where i∗ is defined in (6.54).
              | Then the weighted greedy algorithm is applied with the new sparse grid. The algorithm proceeds
              | in this manner until E4 (Φ, Ψ, I, µ) satisfies (6.53). Therefore the combined algorithm consists of
              | an outer loop that refines the sparse grid (to reduced truncation error, E4 ) and an inner loop that
              | builds an accurate reduced-order basis for a given sparse grid (to decrease the reduced-order model
              | error, E1 and E3 ).
              |    Algorithm 13 summarizes the combined dimension-adaptive greedy algorithm that proceeds ac-
              | cording to the above two-level iteration to improve a given sparse grid and reduced-order basis such
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 190
blank         | 
              | 
              | 
text          | that the gradient condition in (6.53) is satisfied. As stated, the algorithm implicitly requires ini-
              | tialization of each quantity. At iteration 0, the sparse grid is initialized as the uniform level-one
              | sparse grid, I = {(1, . . . , 1)}, which consists of a single node ΞI = {0}. The reduced-order basis is
              | constructed from the primal and sensitivity/adjoint snapshot at this single sparse grid node at the
              |                                                ∂u
              | first trust region center, i.e., u(µ0 , 0) and    (µ , 0) or λ(µ0 , 0). That is, the sparse grid I0 and
              |                                                ∂µ 0
              | reduced-order model Φ0 , Ψ0 are constructed as
blank         | 
text          |                        Φ0 , Ψ0 , I0 = two-level-refine-grad(Φ−1 , Ψ−1 , I−1 , µ0 )                           (6.57)
blank         | 
text          | where I−1 = {(1, . . . , 1)}, Φ−1 , Ψ−1 is the reduced-order model constructed with the aforemen-
              | tioned snapshots, and two-level-refine-grad is defined in Algorithm 13. For all subsequent
              | iterations, the sparse grid and reduced-basis basis are initialized from the previous iteration, i.e.,
              | the construction of Ik , Φk is initialized with Ik−1 , Φk−1
blank         | 
text          |                      Φk , Ψk , Ik = two-level-refine-grad(Φk−1 , Ψk−1 , Ik−1 , µk ).                         (6.58)
blank         | 
text          | Apart from being a natural way to initialize the dimension-adaptive greedy algorithm, it has the
              | added benefit of only refining Ik−1 and Φk−1 if the choice Ik = Ik−1 , Φk = Φk−1 are not sufficient
              | to guarantee convergence, i.e., the gradient condition in (6.53) does not hold.
              |    This completes the discussion of the training algorithm to build Ik and Φk , Ψk to ensure the
              | gradient condition holds and attention is shifted to construction of Ik0 , Φ0k , Ψ0k such that the inexact
              | objective condition (3.22) holds in order to properly assess the trust region step without requiring
              | queries to F (µ). The error indicator for the objective decrease is a weighted sum of two terms: the
              | primal error E1 and QoI truncation error E2
blank         | 
text          |                            θk (µ) =α1 (E1 (Φ0k , Ψ0k , Ik0 , µk ) + E1 (Φ0k , Ψ0k , Ik0 , µ))+
              |                                                                                                              (6.59)
              |                                       α2 (E2 (Φ0k , Ψ0k , Ik0 , µk ) + E2 (Φ0k , Ψ0k , Ik0 , µ)),
blank         | 
text          | which involves error terms evaluated at µk and µ̂k since the pointwise version of the objective
              | decrease bound is used. From Chapter 3, the error condition (3.22), i.e., θk (µ̂k )ω ≤ η min{mk (µk ) −
              | mk (µ̂k ), rk }, is required to preserve global convergence of the trust region method when ψk (µ) is
              | used in place of F (µ) in the computation of ρk . A sufficient condition for the objective condition to
              | hold is that each term satisfies an appropriate fraction of the condition, i.e.,
blank         | 
text          |                                                                  1                                     1/ω
              |      E1 (Φ0k , Ψ0k , Ik0 , µk ) + E1 (Φ0k , Ψ0k , Ik0 , µ̂k ) ≤     (η min{mk (µk ) − mk (µ̂k ), rk })
              |                                                                 2α1
              |                                                                                                              (6.60)
              |                                                                  1                                     1/ω
              |      E2 (Φ0k , Ψ0k , Ik0 , µk ) + E2 (Φ0k , Ψ0k , Ik0 , µ̂k ) ≤     (η min{mk (µk ) − mk (µ̂k ), rk })
              |                                                                 2α2
blank         | 
text          | where the positive weights α1 , α2 balance the contributions of E1 and E2 to justify this uniform split.
              | Therefore the monolithic task of satisfying the objective condition has been broken into the modular
              | tasks in (6.60). Similar to the approach taken to construct Ik and Φk , a weighted greedy algorithm
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 191
blank         | 
              | 
              | 
              | 
text          | Algorithm 13 Refine reduced-order basis and sparse grid for gradient condition
blank         | 
text          |                             Φ, Ψ, I = two-level-refine-grad(Φ, Ψ, I, µ, δ)
              |  1:   Initialization: Given
              |                                 Φ, Ψ, I, µ, δ, β1 > 0, β2 > 0, β3 > 0, κϕ > 0
              |                             κϕ
              |  2:   while E4 (Φ, Ψ, I, µ) >   min {kEI [∇Fr (µ; · , Φ, Ψ)]k , δ} do
              |                             3β3
              |  3:     Refine index set: Add index set with largest contribution to truncation error
blank         | 
text          |                     I ← I ∪ {i∗ }      where     i∗ = arg max ∆i [k∇Fr (µ; · , Φ, Ψ)k]
              |                                                        i∈N (I)
blank         | 
text          |                               κϕ
              |  4:     while E1 (Φ, Ψ, I, µ) >  min {kEI [∇Fr (µ; · , Φ, Ψ)]k , δ} do
              |                              3β1
              |  5:       Evaluate primal error indicator: Greedily select y ∈ Ξi∗ with largest error
blank         | 
text          |                               y ∗ = arg max ρ(y) kr(Φur (µ; y, Φ, Ψ); µ, y)k
              |                                      y∈Ξi∗
blank         | 
text          |  6:       Reduced-order model construction: Update reduced basis with new snapshots
blank         |                                             
text          |                                  ∂u
              |                Φ = Φ u(µ; y ∗ )     (µ; y ∗ )          Φ u(µ; y ∗ ) λ(µ; y ∗ )
blank         |                                                                               
text          |                                                or
              |                                  ∂µ
blank         | 
text          |  7:     end while
              |                               κϕ
              |  8:     while E2 (Φ, Ψ, I, µ) >   min {kEI [∇Fr (µ; · , Φ, Ψ)]k , δ} do
              |                               3β2
              |  9:       Evaluate dual error indicator: Greedily select y ∈ Ξi∗ with largest error
blank         |                                                                                
text          |               ∗                 ∂                        ∂ur
              |              y = arg max ρ(y) r Φur (µ; y, Φ, Ψ); Φ          (µ; y, Φ, Ψ), µ, y          or
              |                    y∈Ξi∗                                 ∂µ
blank         | 
text          |                  = arg max ρ(y) r λ (Φur (µ; y, Φ, Ψ); Ψλr (µ; y, Φ, Ψ), µ, y)
              |                     y∈Ξi∗
blank         | 
text          | 10:       Reduced-order model construction: Update reduced basis with new snapshots
blank         |                                             
text          |                                  ∂u
              |                Φ = Φ u(µ; y ∗ )     (µ; y ∗ )          Φ u(µ; y ∗ ) λ(µ; y ∗ )
blank         |                                                                               
text          |                                                or
              |                                  ∂µ
blank         | 
text          | 11:     end while
              | 12:   end while
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 192
blank         | 
              | 
              | 
text          | will be used to enforce the conditions on E1 and the dimension-adaptive sparse grid construction to
              | satisfy the conditions on E2 .
              |    While the dimension-adaptive greedy algorithm to construct Ik0 , Φ0k , Ψ0k for the objective decrease
              | condition will be very similar that used to construct Ik , Φk , Ψk , there will be two critical differences.
              | First, the error terms in (6.60) involve two points in parameters space: the trust region center µk and
              | the candidate step µ̂k . In contrast, the gradient condition only imposed requirements at the trust
              | region center. This has implications for both the dimension-adaptive sparse grid construction and
              | greedy method. Second, the conditions in (6.60) only impose requirements on the primal reduced-
              | order model accuracy and truncation error, whereas the gradient condition also placed requirements
              | on the sensitivity/adjoint accuracy. This implies only primal snapshots are required during the
              | greedy construction of the reduced-order model.
              |    For a given I, Φ, Ψ, if the truncation error conditions in (6.60), i.e., requirements on E2 , are not
              | satisfied, the sparse grid is updated according to I ← I ∪ {i∗ }, where
blank         | 
text          |                   i∗ = arg max max          ∆i [Fr (µk ; Φ, Ψ)] , ∆i [Fr (µ̂k ; Φ, Ψ)]
blank         |                                                                                         
text          |                                                                                              .       (6.61)
              |                          i∈N (I)
blank         | 
              | 
text          | The integrands in each term is precisely the integrand of E2 at the two parameters of interest: µk
              | and µ̂k . Therefore this refinement process can be repeated iteratively until the conditions on E2 in
              | (6.60) are satisfied. Following the combined dimension-adaptive greedy method introduced for the
              | gradient condition, the sparse grid refinement steps are interwoven with greedy construction of the
              | reduced-order model. For a fixed I, Φ, Ψ, define µ∗ ∈ {µk , µ̂k } and y ∗ ∈ ΞI as the quantities that
              | maximize the weighted residual-based error indicator
blank         | 
text          |                         µ∗ , y ∗ = arg max ρ(y) kr(Φur (µ; y, Φ, Ψ), µ, y)k .                        (6.62)
              |                                    µ∈{µk , µ̂k },
              |                                      y∈ΞI
blank         | 
text          |                                                          h                i
              | The reduced-order basis Φ is updated according to Φ ← Φ u(µ∗ , y ∗ ) ; an optional orthogonaliza-
              | tion step is usually used to ensure the the reduced basis is full rank and the resulting reduced-order
              | model is well-conditioned. As discussed, only primal snapshot are used since the conditions in (6.60)
              | only places requirements on the primal accuracy of the reduced-order model. The argument of
              | the maximization problem in (6.62) is exactly the integrand of E1 . Assuming a minimum-residual
              | reduced-order model is used, E1 (Φ, Ψ, I, µk ) and E1 (Φ, Ψ, I, µ̂k ) will monotonically decrease with
              | each iteration of the greedy method and the iterations proceed until the conditions in (6.60) are satis-
              | fied. The combined training algorithm alternates between sparse grid and reduced basis construction
              | exactly as that in Algorithm 14, namely, for a fixed sparse grid, the greedy method is applied to
              | ensure the conditions on E1 in (6.60) hold, then the reduced-order model is fixed and the sparse grid
              | is refined according to (6.62). The combined algorithm terminates when all conditions in (6.60) are
              | satisfied.
              |    Algorithm 14 summarizes the combined dimension-adaptive greedy algorithm that constructs a
              | sparse grid and reduced-order model such that the objective decrease condition (6.60) holds. Similar
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 193
blank         | 
              | 
              | 
text          | to Algorithm 13, this algorithm refines a given sparse grid and reduced basis and implicitly requires
              | initialization of each quantity. At any iteration k, the sparse grid Ik and reduced-order model Φk ,
              | Ψk are used to initialize Algorithm 14, i.e.,
blank         | 
text          |                     Φ0k , Ψ0k , Ik0 = two-level-refine-obj(Φk , Ψk , Ik , µk , µ̂k , rk )              (6.63)
blank         | 
text          | since Ik , Φk , Ψk have been constructed to satisfy the error condition in (3.15) at µk . If that
              | requirements turns out to be more restrictive than that in (3.22), the algorithm will not modify the
              | sparse grid or reduced-order basis, i.e., Ik0 = Ik and Φ0k = Φk , and the actual-to-predicted ratio is
              | unity and acceptance of the step is guaranteed.
blank         | 
text          | Algorithm 14 Refine reduced-order basis and sparse grid for objective decrease condition
blank         | 
text          |                            Φ, Ψ, I = two-level-refine-obj(Φ, Ψ, I, µ1 , µ2 , s)
              |  1:   Initialization: Given
blank         | 
text          |                                        Φ, Ψ, I, µ1 , µ2 , s > 0, ω ∈ (0, 1)
              |  2:   while
              |                 E2 (Φ, Ψ, I, µ1 ) + E2 (Φ, Ψ, I, µ2 ) >
              |                               1                                                                  1/ω
              |                                   (η min {EI [Fr (µ1 ; · , Φ, Ψ)] − EI [Fr (µ2 ; · , Φ, Ψ)] , s})
              |                              2α2
              |       do
              |  3:    Refine index set: Add index set with largest contribution to truncation error
blank         | 
text          |       I ← I ∪ {i∗ }            i∗ = arg max max ∆i Fr (µ1 ; · , Φ, Ψ) , ∆i Fr (µ2 ; · , Φ, Ψ)
blank         |                                                                                              
text          |                      where
              |                                             i∈N (I)
blank         | 
text          |  4:     while
              |                 E1 (Φ, Ψ, I, µ1 ) + E1 (Φ, Ψ, I, µ2 ) >
              |                               1                                                                  1/ω
              |                                   (η min {EI [Fr (µ1 ; · , Φ, Ψ)] − EI [Fr (µ2 ; · , Φ, Ψ)] , s})
              |                              2α1
              |       do
              |  5:        Evaluate error indicator: Greedily select µ ∈ {µ1 , µ2 }, y ∈ Ξi∗ with the largest error
blank         | 
text          |                             µ∗ , y ∗ = arg max ρ(y) kr(Φur (µ; y, Φ, Ψ); µ, y)k
              |                                        µ∈{µ1 , µ2 }
              |                                          y∈Ξi∗
blank         | 
text          |  6:        Reduced-order model construction: Update reduced basis with new snapshot
blank         | 
text          |                                      Φ = Φ u(µ∗ ; y ∗ )
blank         |                                                        
              | 
              | 
meta          |  7:     end while
              |  8:   end while
title         | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 194
blank         | 
              | 
              | 
title         | 6.3.3     Summary
text          | The proposed multifidelity trust region method for efficient stochastic PDE-constrained optimization
              | leverages the trust region framework of Chapter 3 and the two-level approximation of risk-averse
              | measures of PDE quantities of interest using anisotropic sparse grids and projection-based model
              | reduction. The ingredients required for the trust region algorithm in the present context were
              | introduced in Section 6.3.1 and summarized below
blank         | 
text          |                        mk (µ) = EIk [Fr (µ; · , Φk , Ψk )]
              |                         ϑk (µ) = kµ − µk k
              |                         ϕk (µ) = β1 E1 (Φk , Ψk , Ik , µ) + β2 E3 (Φk , Ψk , Ik , µ)+
              |                                  β3 E4 (Φk , Ψk , Ik , µ)                                            (6.64)
              |                         ψk (µ) = EIk0 Fr (µ; · , Φ0k , Ψ0k )
blank         |                                                            
              | 
text          |                         θk (µ) = α1 (E1 (Φ0k , Ψ0k , Ik0 , µk ) + E1 (Φ0k , Ψ0k , Ik0 , µ))+
              |                                   α2 (E2 (Φ0k , Ψ0k , Ik0 , µk ) + E2 (Φ0k , Ψ0k , Ik0 , µ)).
blank         | 
text          | The sparse grid Ik and reduced-order model Φk , Ψk are constructed using the dimension-adaptive
              | greedy algorithm (Algorithm 13) to ensure the gradient condition (6.53) is satisfied. The algorithm
              | is initialized from the sparse grid and reduced-order model from the previous iteration
blank         | 
text          |                    Φk , Ψk , Ik = two-level-refine-grad(Φk−1 , Ψk−1 , Ik−1 , µk )                    (6.65)
blank         | 
text          | in the event the quantities satisfy the gradient condition without refinement, e.g., if a small step is
              | taken, which would save queries to the high-dimensional model. Once the dimension-adaptive greedy
              | algorithm has been applied to satisfy the gradient condition (3.15), the objective decrease condition
              | in (3.14) holds trivially since ϑk (µ) is taken as the classical trust region constraint (Section 3.1.1).
              | The sparse grid Ik0 and reduced-order model Φ0k , Ψ0k defining the inexact objective decrease used
              | in the computation of ρk are constructed using a similar dimension-adaptive greedy algorithm (Al-
              | gorithm 13). In this case, primal high-dimensional model snapshots are used to reduce the error
              | terms E1 and E2 at two parameters—the trust region center µk and candidate step µ̂k —to satisfy
              | the objective error condition (6.60)
blank         | 
text          |                    Φ0k , Ψ0k , Ik0 = two-level-refine-obj(Φk , Ψk , Ik , µk , µ̂k , rk ).            (6.66)
blank         | 
text          | This initialization of the dimension-adaptive algorithm will take Ik0 = Ik , Φ0k = Φk , Ψ0k = Ψk if
              | they satisfy the objective error condition, which may save substantial computational resources as
              | it will eliminate (possibly many) queries to realizations of the high-dimensional model. With these
              | choices, it is clear from (6.43) and (6.49) that mk (µk ) = ψk (µk ) and mk (µ̂k ) = ψk (µ̂k ). This implies
              | ρk is unity and the step can be accepted with no additional work.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 195
blank         | 
              | 
              | 
text          |    The complete multifidelity trust region algorithm, including inexact evaluation of the actual-
              | to-predicted ratio using ψk (µ), is presented in Algorithm 16. Global convergence is not strictly
              | guaranteed since the error indicators ϕk (µ) and θk (µ) do not necessarily lead to bounds of the form
              | (3.13) and (3.21) due to the approximation EI c ≈ EN (I) in (6.35) and (6.36)-(6.37). However, even
              | though the bounds cannot be rigorously established in the general case, the fact that the bounds are
              | only required up to an arbitrary constant leaves hope they will hold in specific situations of interest.
              | The numerical results in the next section provide evidence that this is the case since convergence is
              | observed.
              |    Future work will consider the incorporation of partially converged solutions as snapshots in the
              | construction of Φk and Φ0k in Algorithms 15 and 16 to further improve the efficiency of the proposed
              | multifidelity trust region method. This will build on the idea introduced in Chapter 5; however, the
              | implications on global convergence of the trust region framework will be more complicated to analyze
              | since another layer of complexity is present, i.e., risk-averse measures of quantities of interest.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 196
blank         | 
              | 
              | 
              | 
text          | Algorithm 15 Trust region method based on reduced-order models and sparse grids
              |  1:   Initialization: Given
              |                              µ0 , ∆0 , 0 < γ < 1, ∆max > 0, 0 < η1 < η2 < 1, κϕ > 0,
blank         |                                                                       
text          |                                            I−1 = {0}, Φ−1 = u(µ0 ; 0)
blank         | 
text          |  2:   Model and constraint update: If the previous model is sufficient for convergence
blank         | 
text          |                                        ϕk−1 (µk ) ≤ κϕ min{k∇mk−1 (µk )k , ∆k },
blank         | 
text          |       re-use for the current iteration: mk (µ) := mk−1 (µ) and ϑk (µ) = kµ − µk k. Otherwise, refine
              |       the reduced-order model and sparse grid using two-level dimension adaptive greedy method
blank         | 
text          |                      Φk , Ψk , Ik = two-level-refine-grad(Φk−1 , Ψk−1 , Ik−1 , µk , ∆k )
blank         | 
text          |       and define model and constraint as
              |                                     mk (µ) = EIk [f (Φk ur (µ; · , Φk , Ψk ), µ, · )]
              |                                        ϑk (µ) = kµ − µk k
blank         | 
text          |  3:   Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                        min mk (µ)       subject to      ϑk (µ) ≤ ∆k
              |                                     µ∈RN
blank         | 
text          |     for a candidate, µ̂k , to satisfy the fraction of Cauchy decrease
              |  4: Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio
blank         | 
text          |                                                         F (µk ) − F (µ̂k )
              |                                                 ρk =
              |                                                        mk (µk ) − mk (µ̂k )
blank         | 
text          |  5:   Step acceptance:
blank         | 
text          |                 if        ρk ≥ η1        then       µk+1 = µ̂k       else     µk+1 = µk    end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                      if     ρk ≤ η 1              then        ∆k+1 ∈ (0, γϑk (µ̂k )]      end if
              |                      if     ρk ∈ (η1 , η2 )       then        ∆k+1 ∈ [γϑk (µ̂k ), ∆k ]    end if
              |                      if     ρk ≥ η 2              then        ∆k+1 ∈ [∆k , ∆max ]         end if
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 197
blank         | 
              | 
              | 
              | 
text          | Algorithm 16 Trust region method based on reduced-order models and sparse grids with inexact
              | objective evaluations
              |  1:   Initialization: Given
              |                              µ0 , ∆0 , 0 < γ < 1, ∆max > 0, 0 < η1 < η2 < 1, κϕ > 0,
              |                             = {0}, Φ−1 = u(µ0 ; 0) , ω ∈ (0, 1), {rk }∞
blank         |                                                    
text          |                       I−1                                             k=1 such that rk → 0
blank         | 
text          |  2:   Model and constraint update: If the previous model is sufficient for convergence
blank         | 
text          |                                         ϕk−1 (µk ) ≤ κϕ min{k∇mk−1 (µk )k , ∆k },
blank         | 
text          |       re-use for the current iteration: mk (µ) := mk−1 (µ) and ϑk (µ) = kµ − µk k. Otherwise, refine
              |       the reduced-order model and sparse grid using two-level dimension adaptive greedy method
blank         | 
text          |                      Φk , Ψk , Ik = two-level-refine-grad(Φk−1 , Ψk−1 , Ik−1 , µk , ∆k )
blank         | 
text          |       and define model and constraint as
              |                                      mk (µ) = EIk [f (Φk ur (µ; · , Φk , Ψk ), µ, · )]
              |                                         ϑk (µ) = kµ − µk k
blank         | 
text          |  3:   Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                         min mk (µ)       subject to       ϑk (µ) ≤ ∆k
              |                                      µ∈RN
blank         | 
text          |     for a candidate, µ̂k , to satisfy the fraction of Cauchy decrease
              |  4: Computed-to-predicted reduction: Compute computed-to-predicted reduction ratio
              |                      
              |                      
              |                                   1            if ϑk (µ̂k )ω ≤ η min{mk (µk ) − mk (µ̂k ), rk }
              |                      
              |                      
              |                ρk =
              |                          ψ (µ ) − ψk (µ̂k )
              |                       k k
              |                      
              |                                                 otherwise
              |                      
              |                      
              |                         mk (µk ) − mk (µ̂k )
blank         | 
text          |       where
              |                                 ψk (µ) := EIk0 f (Φ0k ur (µ; · , Φ0k , Ψ0k ), µ, · )
blank         |                                                                                    
              | 
text          |                           Φ0k , Ψ0k , Ik0 = two-level-refine-obj(Φk , Ψk , Ik , µk , µ̂k , rk )
              |  5:   Step acceptance:
blank         | 
text          |                 if        ρk ≥ η 1        then       µk+1 = µ̂k       else       µk+1 = µk    end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                      if      ρk ≤ η 1              then        ∆k+1 ∈ (0, γϑk (µ̂k )]        end if
              |                      if      ρk ∈ (η1 , η2 )       then        ∆k+1 ∈ [γϑk (µ̂k ), ∆k ]      end if
              |                      if      ρk ≥ η 2              then        ∆k+1 ∈ [∆k , ∆max ]           end if
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 198
blank         | 
              | 
              | 
title         | 6.4     Numerical Experiment: Optimal Control of the Viscous
              |         Burgers’ Equation with Uncertain Coefficients
text          | This section studies the performance of the proposed algorithms (Algorithms 15 and 16) on a simple
              | stochastic PDE-constrained optimization problem: optimal control of the one-dimensional viscous
              | Burgers’ equation with uncertain coefficients. This is precisely the stochastic counterpart to the
              | problem in Section 5.5.2 used to study the deterministic trust region algorithm based on reduced-
              | order models in Chapter 5. The optimization problem takes the form
              |                                       Z   1                                        1              
              |                                                1                            α
              |                          Z                                                      Z
              |              minimize          ρ(y)              (u(µ, y, x) − ū(x))2 dx +             z(µ, x)2 dx dy   (6.67)
              |                  nµ
              |                µ∈R         Ξ           0       2                            2   0
blank         | 
              | 
text          | where u(µ, y, x) is the solution of the following parametrization of the one-dimensional viscous
              | Burgers’ equation
blank         | 
text          |         −ν(y)∂xx u(µ, y, x) + u(µ, y, x)∂x u(µ, y, x) = z(µ, x) x ∈ (0, 1),                      y∈Ξ
              |                                                                                                          (6.68)
              |                       u(µ, y, 0) = d0 (y)            u(µ, y, 1) = d1 (y).
blank         | 
text          | corresponding to the realization y ∈ Ξ . As in Section 5.5.2, the target state is ū(x) ≡ 1 and the
              | regularization parameter is α = 10−3 . This is the risk-neutral optimal control problem. A three-
              | dimensional stochastic space, Ξ = [−1, 1]3 , is chosen to introduce stochasticity into the viscosity
              | and boundary conditions
blank         | 
text          |                                                                  y2                      y3
              |                       ν(y) = 10y1 −2             d0 (y) = 1 +            d1 (y) =            .
              |                                                                 1000                    1000
blank         | 
text          | A uniform probability distribution, ρ(y)dy = 2−3 dy, is chosen for simplicity, although any distribu-
              | tion could be used. The source term, or control, z(µ, x) is defined by 50 cubic splines with clamped
              | boundary conditions, which leads to 53 optimization variables. This stochastic optimal control prob-
              | lem is nearly identical to the one studied in [108, 109], with two exceptions being that the authors
              | in [108, 109]: (1) considered one additional stochastic parameter governing a forcing term in (6.68)
              | and (2) used a larger optimization parameter space consisting of the nodes of the underlying finite
              | element shape functions. In all numerical experiments, the partial differential equation in (6.68) is
              | discretized with 500 linear finite elements for a state space of dimension Nu = 499, after application
              | of the essential boundary conditions.
              |    The initial guess for the optimal control problem taken in all numerical experiments is the
              | constant: z(µ0 , x) ≡ 1. Figure 6.5 contains several different controls and the corresponding solution
              | statistics of (6.68), including those corresponding to the optimal deterministic (y = 0) and stochastic
              | control. It is clear that the including stochasticity in the optimization formulation has a non-trivial
              | impact on the optimal solution obtained. Furthermore, the stochastic formulation allows statistics
              | of the solution and quantities of interest to be considered. The remainder of this section is devoted
              | to studying the methods proposed in Algorithms 15 and 16 and comparing its performance to three
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 199
blank         | 
              | 
              | 
text          |            4                                                            1.5
blank         | 
              | 
              | 
              | 
text          |                                                        E[u(µ, · , x)]
              | z(µ, x)                                                                  1
              |            2
              |                                                                         0.5
              |            0                                                             0
              |                                                                     −0.5
              |                0   0.2    0.4       0.6   0.8    1                            0   0.2   0.4       0.6   0.8   1
              |                                 x                                                             x
blank         | 
              | 
text          | Figure 6.5: Left: the control defining the initial guess for the optimization problem (       ), the
              | solution of the deterministic optimal control problem, i.e., with the stochastic variables fixed at
              | their mean value y = 0 (      ), and the solution of the stochastic optimal control problem (      ).
              | Right: the mean solution of the viscous Burgers’ equation in (6.68) at the initial control (       ),
              | optimal deterministic control (    ), and the optimal stochastic control. One (    ) and two (      )
              | standard deviations about the mean solution corresponding to the optimal stochastic control are
              | also included.
blank         | 
              | 
text          | baseline methods.
              |           The first method applied to the solve the stochastic optimization problem in (6.67) is Algo-
              | rithm 15. Since the true function evaluations F (µ) are unavailable (the expectation cannot be
              | computed exactly), it is approximated using a level 5 isotropic sparse grid. This amounts to a trust
              | region method where inexactness is only used for the gradients, i.e., Algorithm 1 of Chapter 3.
              | The work in [108] considers an identical trust region method for stochastic optimization, except
              | the authors use an approximation model based solely on dimension-adaptive sparse grids, while the
              | proposed method also employs projection-based reduced-order models. The reduced-order models
              | considered in all numerical experiments use a Galerkin projection and, due to the large number
              | of optimization variables, the adjoint method is used to compute gradients of reduced quantities
              | of interest. To promote accuracy of the primal and adjoint solutions with respect to the HDM
              | counterparts, the trial basis is constructed from primal and adjoint snapshots according. Such a
              | reduced-order model does not guarantee the minimum-residual property (Definition 4.1) since the
              | Jacobians of (6.68) are not SPD. However, the numerical experiments suggest that it is important
              | for the reduced-order model gradients to possess discrete consistency to properly converge the trust
              | region subproblem and ensure global convergence, particularly when there are a large number of op-
              | timization variables. This will be referred to a method MI in the remainder. The second stochastic
              | optimization solver employed is exactly the method in Algorithm 16, including the approximation
              | of the actual-to-predicted reduction ratio. This will be referred to as method MII. These two meth-
              | ods are expected to converge similarly since they are built on the same approximation framework;
              | however, MI will require far more queries to the HDM since it employs a fine isotropic sparse grid
              | to evaluate ρk . It is possible that method MII will generate an inaccurate approximation of ρk
              | and will incorrectly accept or reject a step. It will be seen that does not occur in this numerical
              | experiment.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 200
blank         | 
              | 
              | 
              | 
text          | 10−2                                                     10−2
blank         | 
              | 
text          | 10−5                                                     10−5
blank         | 
              | 
text          | 10−8                                                     10−8
              |        0        1         2         3         4                 0       1         2         3         4
              |                     Major iteration                                         Major iteration
blank         | 
              | 
text          | Figure 6.6: Convergence history of the objective error quantities using MI (left) and MII (right):
              | |F (µk ) − F (µ∗ )| ( ), |F (µ̂k ) − F (µ∗ )| ( ), |mk (µk ) − F (µ∗ )| ( ), |mk (µ̂k ) − F (µ∗ )| ( ).
              | Rapid progress is made toward the optimal solution, despite poor agreement between the objective
              | and model at early iterations.
blank         | 
              | 
text          |    To assess the performance of these proposed stochastic optimization solvers, three baseline meth-
              | ods will be used for comparison. The first method is the naive approach of using a fixed level 5
              | isotropic sparse grid to integrate the quantity of interest and the optimization problem is solved with
              | the L-BFGS algorithm. This method will be denoted BI. The second, BII, and third, BIII, methods
              | are the dimension-adaptive sparse grid approaches of [108] and [109], respectively. The stochastic
              | optimization solvers MI, MII, BII, BIII are all trust region methods that use the Steihaug-Toint
              | CG [48] method to approximate the solution of the trust region subproblem and use the parameters
              | in (5.57).
              |    The convergence history of the proposed trust region methods MI and MII are shown in Fig-
              | ure 6.6 and Tables 6.1–6.2. Both methods are converging to a first-order critical point (k∇F (µk )k →
              | 0); after only 5 trust region iterations the first-order optimality condition has reduced 4 orders of
              | magnitude from the initial, sub-optimal control. At early iterations, the approximation model,
              | mk (µ), and true objective, F (µ), do not exhibit good agreement, even at trust region centers. In
              | fact, from the tables, they do not even agree in the first digit. Despite this lack of agreement, the
              | candidate step found by the approximation model leads to reasonable reduction in the true objective
              | function. As a local minima is approached, the bound in (3.15) places more stringent requirements
              | on the model error and, as a result, the approximation model shows excellent agreement with the
              | objective function. The behavior of methods MI and MII are nearly identical since they rely on
              | the same approximation model and error indicators in the trust region method. The only difference
              | is the computation of ρk and, even though MII uses the approximation in (3.20) to compute ρk , it
              | never falsely accepts or rejects a step; see Tables 6.1–6.2.
              |    In contrast to the values of the approximation model and objective function, the gradient norms
              | do show reasonable agreement, even at early iterations, which can be seen in Figure 6.7. This is
              | to be expected since the refinement method in Algorithm 13 targets the gradient error. Both the
              | values and gradients of the approximation model and objective do not show good agreement at the
              | candidate step µ̂k , which is also expected since, at iteration k, the sparse grid and reduced-order
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 201
blank         | 
              | 
              | 
              | 
text          |  10−2                                                                        10−2
blank         | 
              | 
text          |  10−4                                                                        10−4
blank         | 
              | 
text          |  10−6                                                                        10−6
              |                            0        1         2         3      4                                   0        1         2         3   4
              |                                         Major iteration                                                         Major iteration
blank         | 
              | 
text          | Figure 6.7: Convergence history of the gradient quantities using MI (left) and MII (right):
              | k∇F (µk )k (  ), k∇F (µ̂k )k (  ), k∇mk (µk )k (  ), k∇mk (µ̂k )k ( ).
blank         | 
text          |                          104                                                                      104
blank         | 
              | 
              | 
              | 
text          |                                                                         Adjoint HDM evaluations
              | Primal HDM evaluations
blank         | 
              | 
              | 
              | 
text          |                          103                                                                      103
blank         | 
text          |                          102                                                                      102
blank         | 
text          |                          101                                                                      101
blank         | 
text          |                          100                                                                      100
              |                                1          2            3           4                                    1         2            3        4
              |                                          Major iterations                                                        Major iterations
blank         | 
              | 
text          | Figure 6.8: Cumulative number of HDM primal and adjoint evaluations as the major iterations in
              | the various trust region algorithms progress: BII ( ), BIII ( ), MI (     ), MII (     ).
blank         | 
              | 
text          | model were only trained at the trust region center.
              |                          Figure 6.8 provide further insight to behavior of the two proposed methods (MI and MII) in
              | comparison to the trust region-based baseline methods (BII and BIII). All methods are based on the
              | trust region framework in Algorithms 1 and 2 of Chapter 3 that are adapted from the work in [108,
              | 109] and therefore possess the same concept of a major iteration. Figure 6.8 shows the cumulative
              | number of queries to the high-dimensional model as the major iterations progress. The methods
              | BII and BIII require more HDM queries (primal and adjoint) than their counterparts in MI and
              | MII since their trust region model problems rely solely on HDM queries on an anisotropic sparse
              | grid while MI, MII replace these with ROM queries. Another observation is that the BIII requires
              | fewer primal HDM queries than BII and the same number of adjoint queries. This is expected since
              | they both use the same approximation model in the trust region subproblem (implies same number
              | of adjoint queries), but BIII uses inexact objective evaluations to evaluate the actual-to-predicted
              | reduction ratio (implies fewer primal queries). A similar observation holds when comparing MI and
              | MII for the same reason.
              |                          The reduction in the number of HDM queries realized by the proposed methods MI and MII
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 202
blank         | 
              | 
              | 
text          |                                                                              400
              | Primal ROM evaluations
              |                                3,000                                         300
blank         | 
text          |                                2,000
              |                                                                              200
              |                                1,000
              |                                                                              100
              |                                         0
              |                                                                                0
              |                                             0   1          2        3    4         0    20       40       60
blank         | 
              | 
text          |                                                                              200
              |             Adjoint ROM evaluations
blank         | 
              | 
              | 
              | 
text          |                                       400
              |                                                                              150
blank         | 
text          |                                                                              100
              |                                       200
blank         | 
text          |                                                                               50
              |                                         0
              |                                                                                0
              |                                             0   1          2         3   4         0    20       40       60
              |                                                     Major iterations                     ROM size (ku )
blank         | 
              | 
text          | Figure 6.9: Left: Cumulative number of primal and adjoint ROM evaluations as the major iterations
              | in the various trust region algorithms progress. Right: Number of primal and adjoint ROM queries
              | organized according to the size of the reduced-order basis (ku ). Trust region methods considered:
              | MI (     ), MII (     ).
blank         | 
              | 
text          | comes at the price of a large number of ROM queries. This can be seen from Figure 6.9 that includes
              | the cumulative number of queries to the primal and adjoint ROM. Since the size of the reduced-order
              | model constantly changes as these algorithms progress, the number of queries to a reduced-order
              | model of a given size is also presented in Figure 6.9. Method MII requires nearly three times as
              | many primal reduced-order queries as MI, but nearly the same number of adjoint queries. This
              | comes from the fact that MII uses a (possibly) refined reduced-order model to approximation ρk ,
              | while MI uses high-dimensional model queries to compute it exactly. This also explains the fact
              | that larger reduced-order models are required for MII and nearly all of these large reduced-order
              | models are only called upon for a primal solve only, i.e., few adjoint solves for reduced-order models
              | of size > 40.
              |                           Since the proposed method MI and MII and the baseline methods BI–BIII have different
              | sources of cost, i.e., HDM and ROM evaluations versus only HDM evaluations, care must be taken
              | when assessing the performance of the methods. The ultimate cost metric of interest is wall time;
              | however, this one-dimensional model problem will not be representative of the speedups that can be
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 203
blank         | 
              | 
              | 
              | 
text          |                 10−2                                                10−2
              | |F (µ) − F (µ∗ )|
              |                                                                     10−3
blank         | 
              | 
              | 
              | 
text          |                                                              k∇F (µ)k
              |                 10−3
              |                                                                     10−4
              |                 10−4                                                10−5
              |                                                                     10−6
              |                      −5
              |                 10
              |                                                                     10−7 1
              |                       101   102   103      104       105               10     102   103     104        105
              |                                   Cost                                              Cost
blank         | 
              | 
text          | Figure 6.10: Convergence of the objective function (left) and gradient (right) as a function of the
              | cost metric in (6.69) for method MIII for several values of the speedup factor of the reduced-order
              | model: τ = 1 (      ), τ = 10 (     ), τ = 100 (  ), τ = ∞ (      ). The baseline methods used for
              | comparison: BI (       ) and BIII (      ).
blank         | 
              | 
text          | realized by methods MI and MII. Due to the small problem size and the fact that hyperreduction
              | has not been included to reduce the complexity associated with the nonlinear term, queries to the
              | reduced-order model are only marginally less expensive than the HDM queries. For larger problems
              | that include hyperreduction, the motivation for this work, ROM queries have been shown [198] to
              | be one to five orders of magnitude less expensive than HDM queries. To assess the speedups that
              | can be realized by this method, the following simple cost model is introduced
blank         | 
text          |                                      C = nhp + nha /2 + τ −1 (nrp + nra /2)                   (6.69)
blank         | 
text          | where C is the total cost associated with a particular method in the units of equivalent number
              | of primal HDM queries, nhp is the number of primal HDM queries, nha is the number of adjoint
              | HDM queries, nrp is the number of primal ROM queries, nra is the number of adjoint ROM queries,
              | and τ is the ratio of the cost of a primal HDM query to a primal ROM query. This cost model
              | assumes a primal solve is twice as expensive as an adjoint solve and a primal HDM solve is τ times
              | as expensive as a primal ROM solve. Figure 6.10 shows the evolution of the objective function
              | and gradient as a function of the cost metric in (6.69) for the baseline methods BI, BIII and the
              | proposed method MII for τ = 1, 10, 100, ∞. Even for slow reduced-order models (τ = 1), MII
              | exhibits faster convergence than the brute-force baseline method BI; however, it converges more
              | slowly than the state-of-the-art method BIII. For a modest ROM speedup of τ = 10, MII is more
              | than 5× less expensive than BIII, i.e., for a given value of the objective function or gradient, the
              | cost of MII is less than a fifth of BIII. For fast reduced-order models (τ = 100), MII is an order
              | of magnitude more efficient than BIII. An upper bound on the improvement attainable by MII
              | compared to BIII is slightly greater than an order of magnitude, which is seen from the limiting
              | case of free reduced-order model (τ = ∞) in Figure 6.10.
              | Table 6.1: Convergence history of Algorithm 15 applied to the optimal control of the stochastic Burgers’ equation in (6.67).
blank         | 
text          |           F (µk )      mk (µk )      F (µ̂k )    mk (µ̂k )    k∇F (µk )k       ρk            ∆k         Success?
              |         6.6506e-02    7.2694e-02   5.3655e-02   5.9922e-02    2.2959e-02   1.0062e+00   1.0000e+02     1.0000e+00
              |         5.3655e-02    5.9593e-02   5.0783e-02   5.7152e-02    2.3424e-03   1.1765e+00   2.0000e+02     1.0000e+00
              |         5.0783e-02    5.0670e-02   5.0412e-02   5.0292e-02    1.9724e-03   9.8344e-01   4.0000e+02     1.0000e+00
              |         5.0412e-02    5.0292e-02   5.0405e-02   5.0284e-02    9.2654e-05   8.7408e-01   8.0000e+02     1.0000e+00
              |         5.0405e-02    5.0404e-02   5.0403e-02   5.0401e-02    8.3139e-05   9.9873e-01   1.6000e+03     1.0000e+00
              |         5.0403e-02    5.0401e-02        -            -        2.2846e-06        -            -              -
blank         | 
              | 
              | 
              | 
text          | Table 6.2: Convergence history of Algorithm 16 applied to the optimal control of the stochastic Burgers’ equation in (6.67).
blank         | 
text          |            F (µk )     mk (µk )      F (µ̂k )     mk (µ̂k )   k∇F (µk )k       ρk            ∆k         Success?
              |          6.6506e-02   7.2694e-02   5.3655e-02   5.9922e-02    2.2959e-02   1.0257e+00    1.0000e+02    1.0000e+00
              |          5.3655e-02   5.9593e-02   5.0783e-02   5.7152e-02    2.3424e-03   9.7512e-01    2.0000e+02    1.0000e+00
              |          5.0783e-02   5.0670e-02   5.0412e-02   5.0292e-02    1.9724e-03   9.8351e-01    4.0000e+02    1.0000e+00
              |          5.0412e-02   5.0292e-02   5.0405e-02   5.0284e-02    9.2654e-05   8.7479e-01    8.0000e+02    1.0000e+00
              |          5.0405e-02   5.0404e-02   5.0403e-02   5.0401e-02    8.3139e-05   9.9946e-01    1.6000e+03    1.0000e+00
              |          5.0403e-02   5.0401e-02        -            -        2.2846e-06        -             -             -
meta          |                                                                                                                                CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 204
title         | Chapter 7
blank         | 
title         | Conclusions
blank         | 
title         | 7.1     Summary and Conclusions
text          | The primary contributions of this thesis are two-fold: (1) the development of an efficient solver
              | for deterministic PDE-constrained optimization problems that leverages projection-based reduced-
              | order models and partially converged PDE solutions and (2) the development of an efficient solver
              | for stochastic PDE-constrained optimization problems that leverages projection-based reduced-order
              | models and anisotropic sparse grids. These primary contributions were built on two independent
              | auxiliary contributions that have applications that extend well beyond the scope of this thesis:
              | (1) the introduction of a globally convergent, highly flexible generalized trust region method for
              | managing efficient approximation models and (2) the generalization and extension of minimum-
              | residual projection-based reduced-order models [115, 28, 31, 89] to sensitivity and adjoint PDEs.
              |    The multifidelity trust region method introduced in Chapter 3 extends traditional trust region
              | methods by allowing generalized trust region constraints to be used, provided the relationship in
              | (3.12) between the approximation model decrease error and the trust region constraint can be
              | established. This method is said to be a “generalized” trust region since the traditional trust
              | region constraint, i.e., a ball in RNµ , satisfies the required relationships and is therefore a valid
              | constraint in the proposed method. The trust region method is closely based on the methods in
              | [108, 109] that does not require first-order consistency with the objective function and allows an
              | approximation model to be used in the computation of the actual-to-predicted reduction. This
              | flexibility is significant since the resulting method in Algorithm 2 does not explicitly depend on the
              | expensive objective function F (µ); however, construction of the approximation models mk (µ) and
              | ψk (µ) will likely require (inexact) evaluations of F (µ). Furthermore, the inexactness conditions
              | adopted from [93, 108, 109] allow for asymptotic error bounds between the true and approximated
              | quantities, which provides considerable flexibility in the approximation models that can be used.
              | Even though the trust region framework was developed in the unconstrained setting, it can be
              | embedded in an augmented Lagrangian framework to handle nonlinear equality constraints. This
blank         | 
              | 
              | 
meta          |                                                  205
              | CHAPTER 7. CONCLUSIONS                                                                            206
blank         | 
              | 
              | 
text          | multifidelity trust region method, or trust region model management framework, constitutes one of
              | the pillars of this thesis from which the primary contributions regarding deterministic and stochastic
              | PDE-constrained optimization in Chapters 5 and 6 follow. The second pillar is the primary PDE
              | approximation technology employed in this work: projection-based model reduction.
              |    While the concept of minimum-residual projection-based reduced-order models is not new [115,
              | 28, 31, 89], this work contributes to the understanding of this technology and extends it to apply
              | to sensitivity and adjoint PDEs. The primary factors that motivate the use of minimum-residual
              | reduced-order models—optimality, monotonicity, and interpolation—are stated and proved in Propo-
              | sition 4.1, 4.2, 4.4 for the primal, sensitivity, and adjoint PDEs. For the primal PDE, these concepts
              | are well-known from previous work [31], but have only been sparingly explored [210] in the sensi-
              | tivity/adjoint settings. These properties are crucial when reduced-order models are combined with
              | the generalized trust region method of Chapter 3 as the trust region convergence theory places
              | specific requirements on the accuracy of the approximation model at trust region centers, which
              | is closely linked to these minimum-residual properties and the construction of the reduced-order
              | bases. Propositions 4.3 and 4.5 are particularly important contributions of this thesis to the model
              | reduction literature as they state conditions under which minimum-residual sensitivities/adjoints co-
              | incide with the true sensitivities/adjoints of the reduced-order model. These results provide insight
              | into the construction of the reduced-order basis and ensures the true reduced-order model sensitivi-
              | ties/adjoints possess the minimum-residual properties (optimality, monotonicity, and interpolation).
              | This is particularly important in the context of optimization since it guarantees the minimum-
              | residual sensitivities/adjoints will lead to consistent gradients of QoIs based on the reduced-order
              | model, which is extensively leveraged in the deterministic and stochastic PDE-constrained optimiza-
              | tion methods of Chapters 5 and 6. Finally, the minimum-residual sensitivities/adjoints are much
              | easier to implement and compute than their exact counterparts when a non-constant test basis is
              | used since they do not require second derivatives of the governing equations. These results sur-
              | rounding minimum-residual reduced-order models were extended to the case of collocation-based
              | hyperreduction where the residual minimization occurs only over the subset of the degrees of free-
              | dom in the mask. Weaker versions of the crucial propositions mentioned above were established in
              | this setting (Propositions 4.6 – 4.8).
              |    These two technologies—the generalized trust region method and minimum-residual projection-
              | based reduced-order models—serve as pillars for the primary contributions of the thesis: efficient
              | optimization methods for deterministic and stochastic PDE-constrained optimization. The proposed
              | method for deterministic PDE-constrained optimization uses projection-based reduced-order mod-
              | els as the approximation model in the generalized trust region method and residual-based error
              | indicators (Appendix B justifies the use of residual-based error indicators as error bounds). The
              | minimum-residual properties of the reduced-order models, as well as the compression algorithms in
              | Section 4.3, are used to build a ROM that exactly satisfies the error conditions in (3.14), (3.15),
              | which guarantees global convergence. The flexibility of the trust region framework is leveraged to
              | use partially converged PDE solutions as snapshots for the reduced-order model and to approximate
meta          | CHAPTER 7. CONCLUSIONS                                                                              207
blank         | 
              | 
              | 
text          | the actual-to-predicted reduction. The proposed method is applied to a number of PDE-constrained
              | optimization problems in fluid mechanics. The large-scale industrial example of aerodynamic shape
              | optimization of the Common Research Model demonstrated the potential of the proposed method
              | to be 1.6× faster than a state-of-the-art PDE-constrained optimization solver.
              |    The multifidelity trust region method proposed as an efficient solver for stochastic PDE-constrained
              | optimization problems in Chapter 6 requires a second level of inexactness to efficiently integrate quan-
              | tities of interest over the stochastic space to form risk measures (Section 2.2.1). This lead to the
              | development of the two-level approximation of risk measures of PDE quantities of interest that uses
              | dimension-adaptive anisotropic sparse grids to perform efficient integration in the stochastic space
              | and model reduction for efficient PDE queries at each collocation node. This two-level approximation
              | was used to define the approximation model in the multifidelity trust region method and suitable
              | error indicators were derived that take both the model reduction error and integral truncation er-
              | ror into account. Global convergence is established by employing a two-level dimension-adaptive
              | greedy algorithm to simultaneously construct the sparse grid and reduced-order basis to satisfy the
              | error conditions (3.14), (3.15). The proposed method directly extends the work in [108, 109] that
              | only defines the approximation model using dimension-adaptive sparse grids with PDE queries at
              | collocation nodes performed using the HDM. It is also similar to [44, 42, 43] that employs the same
              | two-level approximation, but embeds it in an offline-online framework and claims regarding conver-
              | gence only apply to simple PDEs. The numerical experiment in Chapter 6 demonstrate the promise
              | of this method as a 500-fold reduction in the cost metric (6.69), compared to using a fine isotropic
              | sparse grid without reduced-order models to perform the stochastic optimization, was realized. Even
              | compared to the method in [109] that is considered state-of-the-art, a 10-fold reduction in the cost
              | metric was realized.
blank         | 
              | 
title         | 7.2     Prospective Future Work
text          | This thesis leaves a variety of research issues and spin-off projects that constitute promising avenues
              | of future research. These research direction include:
blank         | 
text          |    • Possible improvements to the proposed methods. A number of possible improvements to the
              |       various methods proposed in this thesis are apparent. The first is a theoretical matter to
              |       extend the liminf statement on global convergence in Appendix A to the stronger lim conver-
              |       gence. Another independent issue that should be addressed is the complete formulation of the
              |       minimum-residual adjoint equations for the collocation-based hyperreduced models. As was
              |       pointed out in Section 4.2.6, this is delicate due to the differences between the mask and sample
              |       mesh that become a factor when considering the transpose of the Jacobian (as required by the
              |       adjoint residual). Another possible enhancement that would have a positive and widespread
              |       impact across the methods proposed in this thesis is the use of improved, faster, and possibly
              |       probabilistic, [15] error indicators. In the trust region framework, these can either be used as
meta          | CHAPTER 7. CONCLUSIONS                                                                          208
blank         | 
              | 
              | 
text          |     the trust region constraint or as the gradient error indicator. Finally, in the context of opti-
              |     mization under uncertainty, the use of sparse grids implicitly assumes the risk measures are
              |     sufficiently smooth, which eliminates many of the most interesting and relevant risk measures
              |     in Section 2.2.1. To enable the use of these alternate risk measures, future work should focus
              |     on an alternative construction of collocation nodes that explicitly deals with non-smoothness.
blank         | 
text          |   • Extension to problems with large-scale parameter spaces, Nµ = O(Nu ). All of the methods
              |     developed in this document assume there are few parameters compared to the dimension
              |     of the state vector, i.e., Nµ  Nu , since reduction was only applied to the state vector.
              |     To handle the more complicated case where Nµ = O(Nu ) that arises in applications such
              |     as topology optimization or inverse problems, reduction of some form must be applied to
              |     the parameter space as well. Evaluation of the reduced-order model will require at least
              |     O(Nµ ) operations, particularly if the parameters define coefficient of the underlying PDE,
              |     e.g., material properties, and this will constitute a major bottleneck when there are O(Nu )
              |     parameters. One possible method that exploits low-dimensional search spaces employed by
              |     individual iterations of linesearch and subspace methods is outlined in Appendix C.
blank         | 
text          |   • Extension to time-dependent problems, possibly with periodicity constraints. In this thesis, all
              |     problems considered in Chapters 5 and 6 were static. However, optimization problems governed
              |     by time-dependent PDEs (Appendix D) would benefit most from a multifidelity approach such
              |     as the ones proposed in this thesis due to their extreme computational cost and the plethora of
              |     training information generated, even after a single query to the HDM. Extension of this work
              |     to time-dependent problems will require the development of inexpensive error indicators for
              |     the primal and sensitivity/dual; however, the optimization methods themselves do not need
              |     to be modified since they are agnostic to the form of the underlying PDE (only work with
              |     quantities of interest and their gradients).
meta          | Appendix A
blank         | 
title         | Global Convergence Proof:
              | Error-Aware Trust Region Method
blank         | 
text          | This section provides the global convergence theory for the error-aware, multifidelity trust region
              | method in Algorithm 2 for the solution of the unconstrained optimization problem
blank         | 
text          |                                           minimize F (µ).
              |                                             µ∈RNµ
blank         | 
              | 
text          | It largely parallels the convergence theory in [133, 108, 109] with required changes to handle the
              | error-aware trust regions. At iteration k, define the approximation model mk : RNµ → R and the
              | error indicators ϑk : RNµ → R+ and ϕk : RNµ → R+ such that
blank         | 
text          |                     |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)ν      µ ∈ Rk                (A.1)
              |                                  k∇F (µk ) − ∇mk (µk )k ≤ ξϕk (µk )                             (A.2)
blank         | 
text          | where ζ, ξ > 0 are arbitrary constants, ν > 1, {µk } is the sequences of iterates produced by the
              | Algorithm 2, and Rk = {µ ∈ RNµ | ϑk (µ) ≤ ∆k } are the sublevel sets of the error indicator ϑk (µ).
              | Furthermore, require the approximation is refined such that the error indicators satisfy the following
              | conditions at trust region centers
blank         | 
text          |                                 ϑk (µk ) ≤ κϑ ∆k                                                (A.3)
              |                                 ϕk (µk ) ≤ κϕ min{k∇mk (µk )k , ∆k },                           (A.4)
blank         | 
text          | where κϑ ∈ (0, 1) and κϕ > 0 are algorithmic constants. Additionally, define an approximation
              | model for the objective function ψk : RNµ → R and corresponding error indicator θk : RNµ → R+
              | that satisfy
              |                             |F (µk ) − F (µ) + ψk (µ) − ψk (µk )| ≤ σθk (µ)                     (A.5)
blank         | 
              | 
              | 
meta          |                                                    209
              |   APPENDIX A. GLOBAL CONVERGENCE PROOF                                                              210
blank         | 
              | 
              | 
text          |   where σ > 0 is an arbitrary constant. Finally, require the objective approximation is refined such
              |   that the error indicators satisfies
blank         | 
text          |                                  θk (µ̂k )ω ≤ η min{mk (µk ) − mk (µ̂k ), rk }                    (A.6)
blank         | 
text          |   where η < min{η1 , 1 − η2 }, ω ∈ (0, 1) and 0 < η1 < η2 < 1 are algorithmic constants, {rk }∞
              |                                                                                               k=1 such
              |   that rk → 0 is a forcing sequence, and µ̂k is the solution of the trust region subproblem at iteration
              |   k
              |                                           minimize         mk (µ)
              |                                            µ∈RNµ
blank         | 
text          |                                           subject to ϑk (µ) ≤ ∆k .
blank         | 
text          |   Before proceeding the main content of this section, the global convergence proof of Algorithm 2,
              |   additional assumptions are introduced on the regularity and boundedness of the objective function
              |   F (µ) and approximation model mk (µ) in Assumptions A.1 and A.2.
blank         | 
text          |   Assumption A.1 (Objective function assumptions).
blank         | 
text          | (AF1) F : RNµ → R is twice-continuously differentiable on RNµ
blank         | 
text          | (AF2) F (µ) is bounded below on RNµ , i.e., there exists κlbf > 0 such that, for all µ ∈ RNµ ,
blank         | 
text          |                                                      F (µ) ≥ κlbf
blank         | 
              | 
text          |   Assumption A.2 (Approximation model assumptions).
blank         | 
text          | (AM1) mk : RNµ → R is twice-continuously differentiable on RNµ
blank         | 
text          | (AM2) The Hessian of the model remains bounded within the trust region, i.e.,
blank         | 
text          |                                         βk := 1 + sup       ∇2 mk (µ) ≤ κumh
              |                                                     µ∈Rk
blank         | 
              | 
text          |         where κumh ≥ 1
blank         | 
text          | (AM3) ϑk : RNµ → R is continuously differentiable on RNµ
blank         | 
text          | (AM4) The directional derivative of the constraint in any direction pk is bounded in the trust region,
              |         i.e.,
              |                                                      ∂ϑk
              |                                               sup        (µ)pk ≤ κ∇ϑ
              |                                              µ∈Rk    ∂µ
              |         where κ∇ϑ > 0
blank         | 
text          | (AM5) The trust region subproblem
              |                                              minimize        mk (µ)
              |                                                µ∈RNµ
blank         | 
text          |                                              subject to ϑk (µ) ≤ ∆k .
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                             211
blank         | 
              | 
              | 
text          |       has a local solution, which is guaranteed if mk (µ) has a local minima in the interior of Rk or
              |       Rk is compact and a local solution lies on the boundary of the trust region ∂Rk := {µ ∈ RNµ |
              |       ϑk (µ) = ∆k }.
blank         | 
text          | Lemma A.1 (Circumscribe ball with radius proportional to ∆k inside Rk ). Assume (AM3)–(AM4)
              | hold. Then,
              |                         Dk := {µ ∈ RNµ | kµ − µk k2 ≤ (1 − κϑ )κ−1
              |                                                                 ∇ϑ ∆k } ⊆ Rk .                  (A.7)
blank         | 
text          | Proof. Let pk be an arbitrary unit vector and take µ ∈ Dk such that µ = µk + αpk . From the
              | definition of Dk in (A.7), α ≤ (1 − κϑ )κ−1
              |                                          ∇ϑ ∆k . The mean value theorem, bound on ϑk (µk ), and
              | bound on the directional derivatives of ϑk (µ) lead to
blank         | 
text          |                                                             ∂ϑk
              |                    ϑk (µ) = ϑk (µk + αpk ) = ϑk (µk ) + α       (ζ)pk ≤ κϑ ∆k + ακ∇ϑ            (A.8)
              |                                                             ∂µ
blank         | 
text          | where ζ = µk + τ αpk for some τ ∈ [0, 1]. The bound on α that results from µ ∈ Dk , along with the
              | relation in (A.8) leads to
              |                                              ϑk (µ) ≤ ∆k
blank         | 
text          | and therefore µ ∈ Rk . Thus, Dk ⊆ Rk .
blank         | 
text          | Lemma A.2 (Fraction of Cauchy Decrease). Assume (AM1) and (AM3)–(AM4) hold. Then there
              | exists µ ∈ Rk such that
blank         | 
text          |                                                             k∇mk (µk )k
blank         |                                                                                            
text          |               mk (µk ) − mk (µ) ≥ κs k∇mk (µk )k min                    , (1 − κϑ )κ−1
              |                                                                                     ∇ϑ ∆k       (A.9)
              |                                                                βk
blank         | 
text          | for κs ∈ (0, 1).
blank         | 
text          | Proof. From Theorem 6.3.3 of [48], there exists µ ∈ Dk such that holds
blank         | 
text          |                                                                     k∇mk (µk )k
blank         |                                                                                  
text          |                        mk (µk ) − mk (µ) ≥ κs k∇mk (µk )k min                   ,δ ,
              |                                                                        βk
blank         | 
text          | where κs ∈ (0, 1) and δ = (1 − κϑ )κ−1
              |                                     ∇ϑ ∆k is the radius of Dk . From Lemma A.1, Dk ⊆ Rk .
              | Therefore there exists µ ∈ Rk such that (A.9) holds.
blank         | 
text          | Lemma A.3. Assume (AM1), (AM3)–(AM4), (AM5) hold. Then the solution of the optimization
              | problem
              |                                         minimize   mk (µ)
              |                                          µ∈RNµ
blank         | 
text          |                                         subject to ϑk (µ) ≤ ∆k
blank         | 
text          | satisfies
              |                                                             k∇mk (µk )k
blank         |                                                                                            
text          |               mk (µk ) − mk (µ) ≥ κs k∇mk (µk )k min                    , (1 − κϑ )κ−1
              |                                                                                     ∇ϑ ∆k
              |                                                                βk
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                                          212
blank         | 
              | 
              | 
text          | Proof. From Assumption (AM5), a solution of the optimization problem exists. By Lemma A.2,
              | there exists a point in the feasible set of the optimization problem, i.e., Rk , that satisfies (A.9).
              | The (global) solution of the optimization problem must realize (at least) the same reduction in the
              | objective function, which leads to the desired result.
blank         | 
text          | Lemma A.4. If the objective approximation error bound (A.5) and accuracy condition (A.6) hold,
              | then for k sufficiently large
blank         | 
text          |                 |F (µk ) − F (µ̂k ) + ψk (µ̂k ) − ψk (µk )| ≤ η min{mk (µk ) − mk (µ̂k ), rk }
blank         | 
text          | Proof. The forcing sequence, {rk }, in the bound on θk implies θk → 0. Therefore, for sufficiently
              | large k, θk ≤ σ −1/(1−ω) . Then, (A.5), θk ≤ σ −1/(1−ω) , and (A.6) lead to the desired result
blank         | 
text          |                                                                (1−ω)
              |   |F (µk ) − F (µ̂k ) + ψk (µ̂k ) − ψk (µk )| ≤ σθk = σθkω θk          ≤ θkω ≤ η min{mk (µk ) − mk (µ̂k ), rk }.
blank         | 
              | 
              | 
              | 
text          | Lemma A.5. If the objective approximation error bound (A.5) and accuracy condition (A.6) hold,
              | then for k sufficiently large
blank         | 
text          |                                           F (µk ) − F (µ̂k )
              |                                 ρ∗k :=                        ∈ [ρk − η, ρk + η],
              |                                          mk (µk ) − mk (µ̂k )
blank         | 
text          | where
              |                                                    ψk (µk ) − ψk (µ̂k )
              |                                             ρk =                        .
              |                                                    mk (µk ) − mk (µ̂k )
              | Proof. For sufficiently large k,
blank         | 
text          |                                                       F (µk ) − F (µ̂k ) + ψk (µ̂k ) − ψk (µk )
              |                    ρ∗k = ρk + (ρ∗k − ρk ) = ρk +                                                .
              |                                                                mk (µk ) − mk (µ̂k )
blank         | 
text          | Then, Lemma A.4 leads to the desired result
blank         | 
text          |                                          |F (µk ) − F (µ̂k ) + ψk (µ̂k ) − ψk (µk )|
              |                          |ρ∗k − ρk | ≤                                               ≤ η.
              |                                                    mk (µk ) − mk (µ̂k )
blank         | 
              | 
              | 
text          | Lemma A.6. Assume (AF2) and (AM1)–(AM4) hold and suppose there exists  > 0 such that
              | k∇mk (µk )k ≥  for k sufficiently large. Then the sequence of trust region radii {∆k } produced by
              | Algorithm 2 satisfies
              |                                                    ∞
              |                                                    X
              |                                                          ∆k < ∞.
              |                                                    k=1
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                                                   213
blank         | 
              | 
              | 
text          | Proof. If there are only a finite number of successful iterations, there exists K > 0 such that all
              | iterations k > K are unsuccessful. Then,
blank         | 
text          |                                                    ∞
              |                                                    X            K
              |                                                                 X             ∞
              |                                                                               X
              |                                                          ∆k =         ∆k +           ∆k
              |                                                    k=1          k=1          k=K+1
              |                                                                        ∞
              |                                                                        X
              |                                                            =C+                ∆k
              |                                                                       k=K+1
blank         | 
              | 
text          |                  K
              |                  X                                                                                                ∞
              |                                                                                                                   X
              | where C =              ∆k < ∞. Since iterations k > K are unsuccessful, ∆k+1 ≤ γ∆k and                                ∆k is
              |                  k=1                                                                                          k=K+1
              | bounded above by a geometric series, implying the infinite sum is finite. Therefore, the result holds
              | if there are a finite number of successful iterations. If there is an infinite sequence of successful
              | iterations {ki }, then for sufficiently large i
blank         | 
text          |              F (µki ) − F (µ̂ki ) ≥ ψki (µki ) − ψki (µ̂ki ) − η(mki (µki ) − mki (µ̂ki ))
              |                                    ≥ (η1 − η)(mki (µki ) − mki (µ̂ki ))
              |                                                                     (                                             )
              |                                                                       ∇mki (µki )
              |                                    ≥ (η1 − η)κs ∇mki (µki ) min                   , (1 − κϑ )κ−1
              |                                                                                               ∇ϑ ∆ki
              |                                                                           βk
blank         |                                                                               
text          |                                                                         −1
              |                                    ≥ (η1 − η)κs  min        , (1 − κϑ )κ∇ϑ ∆ki .
              |                                                         κumh
blank         | 
text          | The first inequality follows from Lemma A.5, the second from the step acceptance condition in Al-
              | gorithm 2, the third from the fraction of Cauchy decrease (A.9), and the fourth from the assumption
              | that k∇mk (µk )k ≥ . Summing over all i sufficiently large
blank         |                                                                              
text          |                                 X                  
              |                  (η1 − η)κs          min              , (1 −   κϑ )κ−1
              |                                                                      ∇ϑ ∆ki       ≤ F (µkI ) − lim F (µki ) < ∞
              |                                                 κumh                                             i→∞
              |                                 i≥I
blank         | 
              | 
text          | where the finiteness of the limit follows from F being bounded below. Since /κumh is bounded
              |                                                  P∞
              | away from zero, the inequality above implies that i=1 ∆ki < ∞.
              |       Let S ⊂ N be the ordered set of indicies of successful iterations. For every k ∈
              |                                                                                      / S, ∆k ≤
              |     k−j(k)
              | γ            ∆j(k) where j(k) ∈ S is the largest index such that j(k) < k, i.e. j(k) represents the last
              | successful iteration before the unsuccessful iteration k. Summing over all k ∈
              |                                                                              / S,
              |                                          ∞                                                  ∞              ∞
              |  X              X                        X             X                               1 X              1 X
              |        ∆k ≤           γ k−j(k) ∆j(k) =                           γ k−j(i) ∆j(i) ≤              ∆j(i) =       ∆k < ∞.
              |                                             i=1 j(i)<k<j(i+1)
              |                                                                                      1 − γ i=1         1−γ
              |  k∈S
              |   /             k∈S
              |                  /                                                                                          k∈S
blank         | 
              | 
text          | Then,
              |                              ∞               ∞                                        ∞
              |                                                                                       X
              |                              X               X            X                    1
              |                                    ∆k =            ∆k +         ∆k ≤    1+                      ∆k < ∞.
              |                                                                               1−γ
              |                              k=1             k∈S          k∈S
              |                                                            /                              k∈S
blank         | 
text          | This proves the desired result.
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                                  214
blank         | 
              | 
              | 
text          | Lemma A.7. Assume (AF2) and (AM1)–(AM4) hold and suppose there exists  > 0 such that
              | k∇mk (µk )k ≥  for k sufficiently large. Then the ratios, {ρk }, produced by Algorithm 2, converge
              | to one.
blank         | 
text          | Proof. From the asymptotic error bound on the approximation model in (A.1) and the fact that the
              | candidate step lies within the trust region, i.e., µ̂k ∈ Rk , it follows that
blank         | 
text          |                       |F (µk ) − F (µ̂k ) + mk (µ̂k ) − mk (µk )| ≤ ζϑk (µ̂k )ν ≤ ζ∆νk .           (A.10)
blank         | 
text          | From the Lemma A.2 and the convergence criteria on the trust region subproblem in Algorithm 2,
              | we have
              |                                                               k∇mk (µk )k
blank         |                                                                                             
text          |               mk (µk ) − mk (µ̂k ) ≥ κs k∇mk (µk )k min                   , (1 − κϑ )κ−1
              |                                                                                       ∇ϑ ∆ k   .
              |                                                                  βk
              | Then, for sufficiently large k,
blank         | 
text          |                                   mk (µk ) − mk (µ̂k ) ≥ (1 − κϑ )κ−1
              |                                                                    ∇ϑ κs ∆k
blank         | 
              | 
text          | due to Lemma A.6 and the assumption that k∇mk (µ)k ≥ . Combining these above inequalities
              | leads to
blank         | 
text          |               F (µk ) − F (µ̂k ) + mk (µ̂k ) − mk (µk )          ζ∆νk                  ζ
              |  |ρk − 1| =                                             ≤           −1        =                   ∆kν−1 .
              |                         mk (µk ) − mk (µ̂k )              (1 − κϑ )κ∇ϑ κs ∆k   (1 − κϑ )κ−1
              |                                                                                           ∇ϑ κs 
blank         | 
              | 
text          | Therefore, ρk → 1 since ∆k → 0 (Lemma A.6) and ν > 1.
blank         | 
text          | Theorem A.1. Assume (AF1)–(AF2), (AM1)–(AM4) hold. Let {µk } be the sequence of iterates
              | produced by Algorithm 2 and {mk } the corresponding models. Then
blank         | 
text          |                               lim inf k∇mk (µk )k = lim inf k∇F (µk )k = 0.
              |                                k→∞                     k→∞
blank         | 
              | 
text          | Proof. For contradiction, suppose there exists  > 0 such that kmk (µk )k ≥ . By Lemma A.7, there
              | exists K > 0 such that for all k > K, ρk is sufficiently close to 1 and the corresponding step is
              | successful. From Algorithm 2, this implies ∆K ≤ ∆k ≤ ∆max . This result contradicts Lemma A.6
              | and we must have
              |                                           lim inf k∇mk (µk )k = 0.
              |                                            k→∞
blank         | 
text          | From the triangle inequality and (A.2),
blank         | 
text          |               k∇F (µk )k ≤ k∇mk (µk )k + k∇mk (µk ) − ∇F (µk )k ≤ (1 + ξ) k∇mk (µk )k
blank         | 
text          | which implies
              |                                            lim inf k∇F (µk )k = 0.
              |                                             k→∞
meta          | Appendix B
blank         | 
title         | Residual-Based Error Bounds
blank         | 
text          | In this section, computable error bounds on quantities of interest and their gradients are sought that
              | will enable reduced-order models to be used in the multifidelity trust region framework introduced in
              | Chapter 3. Global convergence (Appendix A) of the trust region method in Algorithms 1–2 requires
              | asymptotic error bounds of the form
blank         | 
text          |                    |F (µk ) − F (µ) + mk (µ) − mk (µk )| ≤ ζϑk (µ)ν           µ ∈ Rk
              |                                   k∇F (µ) − ∇mk (µ)k ≤ ξϕk (µ)                µ ∈ Nk ,
blank         | 
text          | where ζ, ξ > 0 are arbitrary constants, ν > 1, Rk = {µ ∈ RNµ | ϑk (µ) ≤ ∆k }, and Nk is any
              | open neighborhood of µk . The constants ζ, ξ do not need to be small or even computable since
              | they are never used in the trust region algorithm and global convergence is only predicated on their
              | existence. Two key points about these error bounds that substantially reduces the burden of deriving
              | error indicators ϑk (µ) and ϕk (µ) are: (1) they do not need to have high effectivity to ensure global
              | convergence and (2) they are not required to hold in the entire parameter space, only in the bounded
              | sets Rk (assumed) and Nk . Therefore, this section will consider general residual-based error bounds
              | that hold in bounded subsets of the parameter space since they are easily derived and computed,
              | even though they are known to have poor effectivity.
              |    To facilitate the derivation of the residual-based error bounds, recall the following definition
              | from (2.90) that uses an approximate primal solution u ∈ RNu and sensitivity w ∈ RNu ×Nµ to
              | reconstruct the gradient of the quantity of interest
blank         | 
text          |                                                  ∂f          ∂f
              |                               g ∂ (u, w, µ) :=      (u, µ) +    (u, µ)w.                        (B.1)
              |                                                  ∂µ          ∂u
blank         | 
text          | Similarly, from (2.102), an approximate adjoint solution z ∈ RNu can be used to approximate the
              | gradient of the QoI as
              |                                                  ∂f              ∂r
              |                              g λ (u, z, µ) :=       (u, µ) + z T    (u, µ).                     (B.2)
              |                                                  ∂µ              ∂µ
blank         | 
              | 
              | 
meta          |                                                     215
              |  APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                216
blank         | 
              | 
              | 
text          |  Given these definitions, it is clear that
blank         |                                                        
text          |                                                ∂u
              |                             ∇F (µ) = g ∂ u(µ),    (µ), µ = g λ (u(µ), λ(µ), µ).                       (B.3)
              |                                                ∂µ
blank         | 
text          |      Before proceeding to the derivation of the residual-based error bounds, an operator D is defined
              |  in Definition B.1 that represents the Jacobian of the nonlinear residual integrated between states
              |  u1 and u2 for a fixed parameter µ.
blank         | 
text          |  Definition B.1. Define D : RNu × RNu × RNµ → RNu × RNu as
blank         | 
text          |                                                       1
              |                                                           ∂r
              |                                                   Z
              |                                D(u1 , u2 , µ) =              (u2 + t(u1 − u2 ), µ) dt.                (B.4)
              |                                                   0       ∂u
blank         | 
text          |  Remark. In the special case where r(u, µ) is linear in its first argument, i.e., r(u, µ) = A(µ)u+b,
              |  then D(u1 , u2 , µ) = A(µ).
blank         | 
text          |      The following assumptions are introduced on the nonlinear operator defining the system of equa-
              |  tions and the quantity of interest.
blank         | 
text          |  Assumption B.1 (Nonlinear system assumptions). Let U ⊂ RNu and V ⊂ RNµ be bounded subsets
blank         | 
text          | (AR1) r : U × V → RNu is continuously differentiable
              |                      ∂r
              | (AR2) The Jacobian,     (u, µ), is invertible for all u ∈ U and µ ∈ V
              |                      ∂u
              |                                     ∂r
              | (AR3) The inverse of the Jacobian,     (u, µ)−1 is bounded for all u ∈ U and µ ∈ V
              |                                     ∂u
              | (AR4) The matrix D(u1 , u2 , µ) defined in (B.4) is invertible for all u1 , u2 ∈ U and µ ∈ V
blank         | 
text          | (AR5) The matrix D(u1 , u2 , µ)−1 is bounded for all u1 , u2 ∈ U and µ ∈ V
              |                                      ∂r
              | (AR6) The parameter Jacobian,           (u, µ), is bounded for all u ∈ U and µ ∈ V
              |                                      ∂µ
              |                                            ∂r
              | (AR7) The Jacobian and its transpose,          (u, µ), are Lipschitz continuous in its first argument over
              |                                            ∂u
              |        U , i.e., there exists a constant c∂u r > 0 such that
blank         | 
text          |                                      ∂r            ∂r
              |                                         (u1 , µ) −    (u2 , µ) ≤ c∂u r ku1 − u2 k
              |                                      ∂u            ∂u
blank         | 
text          |        for all u1 , u2 ∈ U and µ ∈ V
              |                                     ∂r
              | (AR8) The parameter Jacobain,           (u, µ), is Lipschitz continuous in its first argument over U , i.e.,
              |                                     ∂µ
              |        there exists c∂µ r   > 0 such that
blank         | 
text          |                                      ∂r            ∂r
              |                                         (u1 , µ) −    (u2 , µ) ≤ c∂µ r ku1 − u2 k
              |                                      ∂µ            ∂µ
blank         | 
text          |        for all u1 , u2 ∈ U and µ ∈ V
meta          |  APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                  217
blank         | 
              | 
              | 
text          |  Assumption B.2 (Quantity of interest assumptions).              Let U ⊂ RNu and V ⊂ RNµ be bounded
              |  subsets
blank         | 
text          | (AQ1) f : U × V → R is continuously differentiable
blank         | 
text          | (AQ2) f : U × V → R is Lipschitz continuous with respect to its first argument, i.e., there exists a
              |         constant cf > 0 such that
blank         | 
text          |                                       |f (u1 , µ) − f (u2 , µ)| ≤ cf ku1 − u2 k                         (B.5)
blank         | 
text          |         ∂f
              | (AQ3)       : U × V → RNu is bounded and Lipschitz continuous with respect to its first argument, i.e.,
              |         ∂u
              |         there exists a constant c∂u f > 0 such that
blank         | 
text          |                                     ∂f            ∂f
              |                                        (u1 , µ) −    (u2 , µ) ≤ c∂u f ku1 − u2 k                        (B.6)
              |                                     ∂u            ∂u
blank         | 
text          |         ∂f
              | (AQ4)       : U × V → RNµ is Lipschitz continuous with respect to its first argument, i.e., there exists
              |         ∂µ
              |         a constant c∂µ f > 0 such that
blank         | 
text          |                                     ∂f            ∂f
              |                                        (u1 , µ) −    (u2 , µ) ≤ c∂µ f ku1 − u2 k                        (B.7)
              |                                     ∂µ            ∂µ
blank         | 
text          |      Finally, the sets U ∗ , W ∗ , Z ∗ are introduced as the set of primal, sensitivity, and adjoint solutions
              |  of the governing equations over a bounded set V of the parameter space.
blank         | 
text          |  Definition B.2. Let V ⊆ RNµ be a bounded set and define
blank         | 
text          |                                            U ∗ = {u(µ) | µ ∈ V }.
blank         | 
text          |  Furthermore, it is assumed that U ∗ is bounded.
blank         | 
text          |  Definition B.3. Let V ⊆ RNµ be a bounded set and define
blank         |                                                                     
text          |                                             ∗       ∂u
              |                                          W =           (µ) | µ ∈ V       .
              |                                                     ∂µ
blank         | 
text          |  Boundedness of W ∗ is established in Lemma B.1.
blank         | 
text          |  Definition B.4. Let V ⊆ RNµ be a bounded set and define
blank         | 
text          |                                            Z ∗ = {λ(µ) | µ ∈ V }.
blank         | 
text          |  Boundedness of Z ∗ is established in Lemma B.2.
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                218
blank         | 
              | 
              | 
text          | Lemma B.1. Assume (AR1)–(AR3), (AR6) hold and V ⊆ RNµ is a bounded subset. Then there
              | exists a constant κ > 0 such that
              |                                                   ∂u
              |                                            sup       (µ) ≤ κ                                          (B.8)
              |                                            µ∈V    ∂µ
              |         ∂u
              | where      (µ) is the solution of r ∂ (u(µ), · , µ) = 0 and u(µ) is the solution of r( · , µ) = 0.
              |         ∂µ
              |                               ∂u
              | Proof. From the definition of    (µ) in (2.87), boundedness of U ∗ , and assumptions (AR2), (AR3),
              |                               ∂µ
              | (AR6), there exists a constant κ > 0 such that
blank         | 
text          |                            ∂u       ∂r                      ∂r
              |                               (µ) ≤    (u(µ), µ)−1             (u(µ), µ) ≤ κ.
              |                            ∂µ       ∂u                      ∂µ
blank         | 
              | 
              | 
text          | Lemma B.2. Assume (AR1)–(AR3), (AQ3) hold and V ⊆ RNµ is a bounded subset. Then there
              | exists a constant κ > 0 such that
              |                                              sup kλ(µ)k ≤ κ                                           (B.9)
              |                                             µ∈V
blank         | 
text          | where λ(µ) is the solution of r λ (u(µ), · , µ) = 0 and u(µ) is the solution of r( · , µ) = 0.
blank         | 
text          | Proof. From the definition of λ(µ) in (2.92), boundedness of U ∗ , and assumptions (AR2), (AR3),
              | (AQ3), there exists a constant κ > 0 such that
blank         | 
text          |                                      ∂r                  ∂f
              |                            λ(µ) ≤       (u(µ), µ)−T         (u(µ), µ)T ≤ κ.
              |                                      ∂u                  ∂u
blank         | 
              | 
              | 
text          | Lemma B.3. Assume (AR1), (AR4), (AR5) hold and U ⊆ RNu , V ⊆ RNµ are bounded subsets.
              | Then there exists a constant κ > 0 such that
blank         | 
text          |                                  ku(µ) − uk ≤ κ kr(u, µ)k         µ∈V                                (B.10)
blank         | 
text          | for any u ∈ U , where u(µ) is the solution of r( · , µ) = 0.
blank         | 
text          | Proof. Consider any u ∈ U . A variant of the mean value theorem gives
blank         | 
text          |                           r(u(µ), µ) − r(u, µ) = D(u(µ), u, µ)(u(µ) − u).
blank         | 
text          | The boundedness of U ∗ and assumptions (AR4)–(AR5) imply the existence of a constant κ > 0 such
              | that
              |                                       ku(µ) − uk ≤ κ kr(u, µ)k .
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                               219
blank         | 
              | 
              | 
text          | Lemma B.4. Assume (AR1), (AR4), (AR5), (AQ2) hold and U ⊆ RNu , V ⊆ RNµ are bounded
              | subsets. Then there exists a constant κ > 0 such that
blank         | 
text          |                              |f (u(µ), µ) − f (u, µ)| ≤ κ kr(u, µ)k      µ∈V                       (B.11)
blank         | 
text          | for any u ∈ U , where u(µ) is the solution of r( · , µ) = 0.
blank         | 
text          | Proof. Consider any u ∈ U . Lipschitz continuity of f (AQ2) gives
blank         | 
text          |                                 |f (u(µ), µ) − f (u, µ)| ≤ cf ku(µ) − uk .                         (B.12)
blank         | 
text          | The bound in Lemma B.3 leads to the desired result.
blank         | 
text          | Lemma B.5. Assume (AR1)–(AR8) hold and U ⊆ RNu , V ⊆ RNµ , W ⊆ RNu ×Nµ are bounded
              | subsets. Then there exists constants κ, τ > 0 such that
blank         | 
text          |                        ∂u
              |                           (µ) − w ≤ κ kr(u, µ)k + τ r ∂ (u, w, µ)             µ∈V                  (B.13)
              |                        ∂µ
blank         | 
text          |                                          ∂u
              | for any u ∈ U and w ∈ W , where             (µ) is the solution of r ∂ (u(µ), · , µ) = 0 and u(µ) is the
              |                                          ∂µ
              | solution of r( · , µ) = 0.
              |                                   ∂u
              | Proof. From the definition of        (µ) in (2.87) and r ∂ in (2.89), the following relation holds for any
              |                                   ∂µ
              | w ∈ RNu ×Nµ
blank         |                                                                                
text          |                   ∂u             ∂r          −1 ∂r                ∂r
              |                      (µ) − w = −    (u(µ), µ)         (u(µ), µ) +    (u(µ), µ)w
              |                   ∂µ             ∂u                ∂µ             ∂u
              |                                                                                                    (B.14)
              |                                  ∂r
              |                              =−     (u(µ), µ)−1 r ∂ (u(µ), w, µ).
              |                                  ∂u
blank         | 
text          | Existence and boundedness of the Jacobian inverse over U ∗ leads to the bound
blank         | 
text          |                                     ∂u
              |                                        (µ) − w ≤ κ1 r ∂ (u(µ), w, µ)                               (B.15)
              |                                     ∂µ
blank         | 
text          | for any w ∈ RNu ×Nµ , where κ1 > 0 is a constant. The desired bound will be obtained by bounding
              | the sensitivity residual evaluated at the exact primal solution, r ∂ (u(µ), w, µ), by a combination of
              | the primal and sensitivity residuals at an approximate primal and sensitivity solution. For w ∈ W
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                              220
blank         | 
              | 
              | 
text          | there exists a constant κ2 > 0 such that
blank         | 
text          |  r ∂ (u(µ), w, µ) ≤ r ∂ (u, w, µ) + r ∂ (u(µ), w, µ) − r ∂ (u, w, µ)
              |                                            ∂r             ∂r              ∂r             ∂r
              |                     ≤ r ∂ (u, w, µ) +         (u(µ), µ) −    (u, µ) kwk +    (u(µ), µ) −    (u, µ)
              |                                            ∂u             ∂u              ∂µ             ∂µ
              |                     ≤ r ∂ (u, w, µ) + (c∂µ r + c∂u r kwk) ku(µ) − uk
              |                     ≤ r ∂ (u, w, µ) + κ2 kr(u, µ)k .
              |                                                                                                   (B.16)
              | The first two inequalities follow from the triangle inequality and definition of the sensitivity residual
              | r ∂ in (2.89). The third inequality follows from Lipschitz continuity of the partial derivatives of r
              | (AR7)–(AR8). The final inequality follows from the boundedness of the subset W and Lemma B.3.
              | Combining (B.15) and (B.16), the desired result follows.
blank         | 
text          | Lemma B.6. Assume (AR1)–(AR7), (AQ1), (AQ3) hold and U ⊆ RNu , V ⊆ RNµ , Z ⊆ RNu .
              | Then there exists constants κ, τ > 0 such that
blank         | 
text          |                       kλ(µ) − zk ≤ κ kr(u, µ)k + τ r λ (u, z, µ)           µ∈V                    (B.17)
blank         | 
text          | for any z ∈ Z, where λ(µ) is the solution of r λ (u(µ), · , µ) = 0 and u(µ) is the solution of
              | r( · , µ) = 0.
blank         | 
text          | Proof. From the definition of λ(µ) in (2.92) and r λ in (2.101), the following relation holds for any
              | z ∈ RNu                                                                      
              |                             ∂r          −T      ∂f           T ∂r          T
              |                  λ(µ) − z =    (u(µ), µ)     −     (u(µ), µ) +    (u(µ), µ) z
              |                             ∂u                  ∂u             ∂u
              |                                                                                                   (B.18)
              |                             ∂r
              |                           =    (u(µ), µ)−T r λ (u(µ), z, µ).
              |                             ∂u
              | Existence and boundedness of the Jacobian inverse over U ∗ leads to the bound
blank         | 
text          |                                   kλ(µ) − zk ≤ κ1 r λ (u(µ), z, µ)                                (B.19)
blank         | 
text          | for any z ∈ RNu , where κ1 > 0 is a constant. The desired bound will be obtained by bounding
              | the adjoint residual evaluated at the exact primal solution, r λ (u(µ), z, µ), by a combination of the
              | primal and adjoint residuals at an approximate primal and adjoint solution. Then for z ∈ Z, there
              | exists a constant κ2 > 0 such that
blank         | 
text          |  r λ (u(µ), z, µ) ≤ r λ (u, z, µ) + r λ (u(µ), z, µ) − r λ (u, z, µ)
              |                                           ∂r              ∂r               ∂f             ∂f
              |                    ≤ r λ (u, z, µ) +         (u(µ), µ)T −    (u, µ)T kzk +    (u(µ), µ) −    (u, µ)
              |                                           ∂u              ∂u               ∂u             ∂u
              |                    ≤ r λ (u, z, µ) + (c∂u f + c∂u r kzk) ku(µ) − uk
              |                    ≤ r λ (u, z, µ) + κ2 kr(u, µ)k .
              |                                                                                                   (B.20)
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                               221
blank         | 
              | 
              | 
text          | The first two inequalities follow from the triangle inequality and definition of the adjoint residual r λ
              | in (2.101). The third inequality follows from Lipschitz continuity of the Jacobian of r (AR7) and
              | f (AQ3). The final inequality follows from the boundedness of the set Z. Combining (B.18) and
              | (B.20), the desired result follows.
blank         | 
text          | Lemma B.7. Assume (AR1)–(AR8), (AQ1)–(AQ4) hold and U ⊆ RNu , V ⊆ RNµ , W ⊆ RNu ×Nµ
              | are bounded subsets. Then there exists constant κ, τ > 0 such that
blank         |                        
text          |                ∂u
              |      g ∂ u(µ),    (µ), µ − g ∂ (u, w, µ) ≤ κ kr(u, µ)k + τ r ∂ (u, w, µ)                 µ∈V       (B.21)
              |                ∂µ
blank         | 
text          |                                                                                  ∂u
              | for any u ∈ U , w ∈ W , where u(µ) is the solution of r( · , µ) = 0 and             (µ) is the solution of
              |                                                                                  ∂µ
              | r ∂ (u(µ), · , µ) = 0.
blank         | 
text          | Proof. From the definition of g ∂ in (2.90) and the triangle inequality
blank         |                                   
text          |             ∂             ∂u                          ∂f              ∂f
              |         g           u(µ),    (µ), µ − g ∂ (u, w, µ) ≤    (u(µ), µ) −     (u, µ) +
              |                           ∂µ                          ∂µ             ∂µ
              |                                                                                                    (B.22)
              |                                                       ∂f           ∂u        ∂f
              |                                                          (u(µ), µ)    (µ) −     (u, µ)w .
              |                                                       ∂u           ∂µ        ∂u
blank         | 
text          |                                                            ∂f
              | for any u ∈ U and w ∈ W . Lipschitz continuity of             ( ·, µ) leads to
              |                                                            ∂µ
blank         |                        
text          |                ∂u
              |      g ∂ u(µ),    (µ), µ − g ∂ (u, w, µ) ≤ c∂µ f ku(µ) − uk +
              |                ∂µ
              |                                                                                                    (B.23)
              |                                                   ∂f           ∂u       ∂f
              |                                                      (u(µ), µ)    (µ) −    (u, µ)w .
              |                                                   ∂u           ∂µ       ∂u
blank         | 
text          |                                ∂f        ∂u
              | Adding and subtracting            (u, µ)    (µ) leads to
              |                                ∂u        ∂µ
blank         |                                  
text          |            ∂             ∂u
              |        g           u(µ),    (µ), µ − g ∂ (u, w, µ) ≤ c∂µ f ku(µ) − uk +
              |                          ∂µ
blank         |                                                                                      
text          |                                                               ∂f             ∂f         ∂u
              |                                                                  (u(µ), µ) −    (u, µ)     (µ) +
              |                                                               ∂u             ∂u         ∂µ
blank         |                                                                                  
text          |                                                             ∂f          ∂u
              |                                                                (u, µ)      (µ) − w .
              |                                                             ∂u          ∂µ
blank         | 
text          |                                   ∂f
              |                                      ( ·, µ) gives
              | Lipschitz continuity and boundedness of
              |                                   ∂u
blank         |                                                       
text          |   ∂      ∂u           ∂                            ∂u                     ∂u
              |  g u(µ),    (µ), µ − g (u, w, µ) ≤ c∂µ f + c∂u f      (µ) ku(µ) − uk + τ1    (µ) − v ,
              |          ∂µ                                        ∂µ                     ∂µ
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                 222
blank         | 
              | 
              | 
text          | where τ1 > 0 is a constant. The boundedness of W ∗ leads to
blank         |                                           
text          |                     ∂             ∂u                                             ∂u
              |                 g           u(µ),    (µ), µ − g ∂ (u, w, µ) ≤ κ1 ku(µ) − uk + τ1    (µ) − w
              |                                   ∂µ                                             ∂µ
blank         | 
text          | where κ1 > 0 is a constant. Combining the above bound with Lemmas B.3 and B.5 gives the desired
              | result.
blank         | 
text          | Lemma B.8. Assume (AR1)–(AR8), (AQ1)–(AQ4) hold and U ⊆ RNu , V ⊆ RNµ , Z ⊆ RNu are
              | bounded subsets. Then there exists constant κ, τ > 0 such that
blank         | 
text          |           g λ (u(µ), λ(µ), µ) − g λ (u, z, µ) ≤ κ kr(u, µ)k + τ r λ (u, z, µ)               µ∈V       (B.24)
blank         | 
text          | for any u ∈ U , z ∈ Z, where u(µ) is the solution of r( · , µ) = 0 and λ(µ) is the solution of
              | r λ (u(µ), · , µ) = 0.
blank         | 
text          | Proof. From the definition of g λ in (2.102) and the triangle inequality
blank         | 
text          |                                                          ∂f             ∂f
              |             g λ (u(µ), λ(µ), µ) − g λ (u, z, µ) ≤           (u(µ), µ) −    (u, µ) +
              |                                                          ∂µ             ∂µ
              |                                                                                                       (B.25)
              |                                                                ∂r                 ∂r
              |                                                          λ(µ)T    (u(µ), µ) − z T    (u, µ) .
              |                                                                ∂µ                 ∂µ
blank         | 
text          |                                                              ∂f
              | for any u ∈ U and z ∈ Z. Lipschitz continuity of                ( ·, µ) over U leads to
              |                                                              ∂µ
blank         | 
text          |           g λ (u(µ), λ(µ), µ) − g λ (u, z, µ) ≤ c∂µ f ku(µ) − uk +
              |                                                                      ∂r                 ∂r            (B.26)
              |                                                              λ(µ)T      (u(µ), µ) − z T    (u, µ) .
              |                                                                      ∂µ                 ∂µ
blank         | 
text          |                                         ∂r
              | Adding and subtracting λ(µ)T               (u, µ) leads to
              |                                         ∂µ
blank         | 
text          |             g ∂ (u(µ), λ(µ), µ) − g ∂ (u, z, µ) ≤ c∂µ f ku(µ) − uk +
blank         |                                                                                          
text          |                                                              T   ∂r              ∂r
              |                                                          λ(µ)       (u(µ), µ) −     (u, µ) +
              |                                                                  ∂µ             ∂µ
              |                                                                    T ∂r
              |                                                          (λ(µ) − z)     (u, µ) .
              |                                                                      ∂µ
blank         | 
text          |                                                   ∂r
              | Lipschitz continuity and boundedness of              ( ·, µ) gives
              |                                                   ∂µ
blank         | 
text          |           g ∂ (u(µ), λ(µ), µ) − g ∂ (u, z, µ) ≤ c∂µ f + c∂µ r kλ(µ)k ku(µ) − uk + τ1 kλ(µ) − zk ,
blank         |                                                                     
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                    223
blank         | 
              | 
              | 
text          | where τ1 > 0 is a constant. The boundedness of Z ∗ leads to
blank         | 
text          |                 g ∂ (u(µ), λ(µ), µ) − g ∂ (u, z, µ) ≤ κ1 ku(µ) − uk + τ1 kλ(µ) − zk
blank         | 
text          | where κ1 > 0 is a constant. Combining the above bound with Lemmas B.3 and B.6 gives the desired
              | result.
meta          | Appendix C
blank         | 
title         | Adaptive State and Parameter
              | Space Reduction for Large-Scale
              | Optimization
blank         | 
text          | All of the optimization methods introduced in Chapters 5–6 were developed under the assump-
              | tion that the number of optimization parameters is small compared to the size of the state vector
              | (Nµ  Nu ) and therefore the dominant cost is attributed to the PDE solves. However, there are
              | a large a number of relevant optimization problems, including topological optimization and inverse
              | problems, where this is not the case. In these problems, the number of parameters is of the same
              | order of magnitude as the number of degrees of freedom in the PDE, i.e., Nµ = O(Nu ). In such
              | settings, the cost of the optimization problem cannot be notably decreased if the state vector alone
              | is reduced, e.g., with projection-based reduced-order models. This can be attributed to two main
              | sources of computational cost. The first comes from the fact that the linear algebra involved in
              | the optimization solver is non-negligible due to the large number of parameters. Therefore, even
              | the reduced-space approach to PDE-constrained optimization (Section 2.3.2) will yield a large-scale
              | optimization problem. Second, the evaluation of reduced-order model residual and Jacobian depend
              | on O(Nu ) parameters and will require at least O(Nu ) operations and can not be expected to enjoy
              | the dramatic reduction in computational resources that has been exploited in non-parametric or
              | few-parameter settings [17, 171, 114, 125, 52].
              |    The approach taken to eliminate the bottlenecks associated with large parameter spaces adap-
              | tively restricts the parameter space to a low-dimensional affine subspace of dimension kµ , where
              | kµ  Nµ . While similar approaches have been taken in the past [168, 167, 117, 120], the proposed
              | method focuses on establishing global convergence (not considered in [117, 120]) without requiring
              | first-order consistency, a requirement in [168, 167], for increased efficiency. The proposed restriction
              | converts the Nµ -parameter optimization problem to one in kµ parameters. The resulting opti-
              | mization problem with few parameters is solved using the globally convergent multifidelity trust
blank         | 
              | 
meta          |                                                   224
              | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                 225
blank         | 
              | 
              | 
text          | region method that leverages projection-based reduced-order models (Chapter 5). This results in
              | a two-level, nested reduction where, at the outermost level, the parameter space is restricted to a
              | low-dimensional affine subspace to yield an optimization problem in few variables and, at the in-
              | ner level, the projection-based model reduction reduces the dimensionality of the PDE itself. The
              | inexactness introduced at the innermost level through the use of projection-based reduced-order
              | models is managed using the multifidelity trust region of Chapter 5. Once the solution of the re-
              | stricted optimization problem is found, the low-dimensional parameter subspace is adapted at the
              | new point in µ-space. Such an approach to numerical optimization is called a subspace method
              | [54, 119, 137, 143, 207]; the popular linesearch methods [143] correspond to the special case with
              | kµ = 1. Convergence theory from the subspace/linesearch optimization literature will be recycled
              | to formulate a minimum requirement on the updated low-dimensional affine parameter subspace to
              | ensure a globally convergent method. The proposed subspace update will satisfy this minimum re-
              | quirement, thereby ensuring global convergence, while providing sufficient flexibility to incorporate
              | generic optimization-based vectors (such as the steepest descent direction, quasi-Newton directions,
              | and directions of negative curvature) as well as problem specific information. In applications such
              | as topology optimization and inverse problems, the parameter vector has a strong connection to the
              | geometry of the underlying PDE and its discretization, which can be exploited to yield a rapidly
              | converging algorithm.
              |    General subspace methods (kµ > 1) have not been widely adopted by the optimization com-
              | munity because of the inherent difficulty/expense required to search a kµ -dimensional subspace
              | compared to a one-dimensional subspace as in linesearch methods. This is one reason linesearch
              | methods have enjoyed considerable success. In contrast, trust region methods search the entire
              | Nµ -dimensional space at each optimization iteration; however, the expensive objective function is
              | usually replaced with a quadratic approximation that is inexpensive to query. The use of the more
              | expensive subspace methods are justified in this work for two reasons. First, an efficient method has
              | been developed in Chapter 5 to solve PDE-constrained optimization problems with few parameters
              | and it is desirable to use this method to do as much work as possible before adapting the param-
              | eter space. Additionally, restricting the parameter space to few parameters will ensure evaluation
              | of the reduced-order model does not involve operations that scale with Nµ = O(Nu ). To develop
              | the ideas of this section in a simple setting, only the deterministic case will be considered; future
              | work will consider stochastic optimization problems with large-dimensional parameter spaces and a
              | strategy that combines this approach with the method of Chapter 6, i.e., three-level approximation:
              | reduction of the state space via model reduction, reduction of the parameter space via subspace and
              | linesearch techniques, and approximation of integrals using dimension-adaptive sparse grids.
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                   226
blank         | 
              | 
              | 
title         | C.1      Two-Level Nested Reduction of Parametrized Partial
              |          Differential Equations
text          | This section proposes a two-level, nested reduction strategy for parametrized partial differential
              | equations. In the first level of reduction, the high-dimensional parameter space is restricted to
              | a low-dimensional affine subspace. In the context of optimization, this amounts to a restriction
              | of the search space to the chosen affine subspace; however, it does not introduce any error into
              | the pointwise evaluation of the PDE. The second level of reduction uses projection-based model
              | reduction (Chapter 4) to the reduce the number of degrees of freedom in the PDE, i.e., reduction
              | of the state space. Unlike the reduction of the parameter space, the state space restriction does
              | introduce error into the evaluation of the PDE, as seen previously in Chapter 4. These two types
              | of reduction will be nested to efficiently solve a PDE-constrained optimization problem as follows:
              | first, the parameter space restriction will be applied to reduce the optimization problem over Nµ
              | parameters to one over kµ  Nµ parameters and the trust region method of Chapter 5 that leverages
              | projection-based reduced-order models will be applied to solve the reduced optimization problem.
              | Adaptation of the parameter space, discussed in Section C.2.1, will be required to yield a globally
              | convergent method. The remainder of this section will consider each layer of reduction, in isolation,
              | which will be combined in Section C.2 to develop the nested optimization algorithm.
blank         | 
              | 
title         | C.1.1     Outer Layer of Reduction: Restriction of Parameter Space
text          | The PDE-constrained optimization problem that motivates this work takes the form (reduced-space
              | formulation)
              |                                    minimize F (µ) := f (u(µ), µ)                                  (C.1)
              |                                     µ∈RNµ
blank         | 
text          | where u(µ) is the unique (Assumption 2.2), continuously differentiable (Theorem 2.1) solution of
              | the fully discrete parametrized partial differential equation r( · , µ) = 0. Unlike previous chapters,
              | here it is assumed that Nµ is large, i.e., Nµ = O(Nu ). The gradient of the objective function can be
              | computed using either the sensitivity (Section 2.3.3) or adjoint (Section 2.3.4) method; however, due
              | to the large number of parameters Nµ = O(Nu ), the adjoint method is the only feasible approach.
              |    The reduction of the parameter space proceeds in an identical manner to the state reduction in
              | Chapter 4, i.e., with the ansatz that the parameter lies in a low-dimensional (affine) subspace
blank         | 
text          |                                             µ = µ̄ + Υη                                           (C.2)
blank         | 
text          | where µ̄ ∈ RNµ is the affine offset, Υ ∈ RNµ ×kµ a basis for the chosen low-dimensional subspace,
              | η ∈ Rkµ are the reduced coordinates of µ in the affine subspace A(µ̄, Υ) := {µ̄+Υη | η ∈ Rkµ }, and
              | kµ  Nµ . For the remainder of this section, µ̄ and Υ will be assumed given and fixed; Section C.2
              | will provide details pertaining to their construction and adaptation. Substitution of the ansatz in
              | (C.2) into the parametrized PDE r(u, µ) = 0 with Nµ parameters leads to a parametrized PDE in
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                     227
blank         | 
              | 
              | 
              | 
text          |                                     span(Υ)
blank         | 
text          |                                          µ̄
              |                                                                    µ∗
blank         | 
              | 
text          |                                                   µ̄ + Υη ∗
blank         | 
              | 
              | 
              | 
text          | Figure C.1: Schematic of restriction of parameter space RNµ to affine subspace A(µ̄, Υ) of dimension
              | kµ , in the special case where Nµ = 2 and kµ = 1. The optimal solution µ∗ in the parameter space,
              | as well as the optimal solution over A(µ̄, Υ) are also depicted.
blank         | 
              | 
text          | kµ parameters
              |                                           r(u, µ̄ + Υη) = 0.                                       (C.3)
blank         | 
text          | In the above setting, the affine offset µ̄ and basis Υ are fixed and the PDE parameter is varied
              | through variations in the reduced coordinates η. For the remainder of this document, let u(η; µ̄, Υ)
              | be the solution of the restricted PDE in (C.3), i.e., r( · , µ̄ + Υη) = 0. Uniqueness and continuous
              | differentiability of u(η; µ̄, Υ) follow immediately from the corresponding properties of u(µ) and
              | the affine relationship between µ and η. Following the discussion at the beginning of this section,
              | the approximation in (C.3) does not introduce error into the evaluation of the PDE since it is clear
              | that u(η; µ̄, Υ) = u(µ̄ + Υη). Rather, it limits the possible variations of the parameter that can
              | be realized, i.e., any µ ∈ RNµ such that µ 6∈ A(µ̄, Υ) cannot be considered by the restricted PDE
              | in (C.3). With the ansatz in (C.2), the PDE-constrained optimization problem in Nµ parameters
              | reduces to one in kµ parameters
blank         | 
text          |                           minimize F (η; µ̄, Υ) := f (u(η; µ̄, Υ), µ̄ + Υη)                        (C.4)
              |                             η∈Rkµ
blank         | 
              | 
text          | that amounts to a search for the optimal solution in the affine subspace A(µ̄, Υ), i.e., (C.4) is
              | equivalent to
              |                                               minimize F (µ).                                      (C.5)
              |                                               µ∈A(µ̄, Υ)
blank         | 
text          | This situation is illustrated in Figure C.1 for the case of kµ = 1. In general, a local minima of
              | (C.5), call it µ∗ , will not be lie in A(µ̄, Υ) for an a priori selection of µ̄ and Υ. This motivates the
              | adaptation strategy for µ̄ and Υ that will be introduced in Section C.2.1.
blank         | 
text          | Remark. As previously discussed, the idea of restricting the optimization problem to a low-dimensional
              | affine subspace generalizes linesearch methods that consider a one-dimensional affine search space.
              | Such methods are known as subspace methods. In linesearch methods, the subspace is defined by any
              | descent direction pk (possibly the steepest descent or a quasi-Newton direction) and offset to include
              | the current optimization iterate, µk , i.e., the search space is {µk + αpk | α > 0}. In the notation of
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                     228
blank         | 
              | 
              | 
text          | this section, linesearch methods amount to the selection µ̄ = µk and Υ = [pk ].
blank         | 
text          |    Since the number of parameters has been dramatically reduced, either the sensitivity or adjoint
              | method are feasible approaches to compute the gradient of F . Following the procedure outlined in
              | Section 2.3.3, the expression for ∇F (η; µ̄, Υ) based on the sensitivity method is
blank         |                                                                                     
text          |                                          ∂                    ∂u
              |                      ∇F (η; µ̄, Υ) = g           u(η; µ̄, Υ),    (η; µ̄, Υ), η; µ̄, Υ ,            (C.6)
              |                                                               ∂η
blank         | 
text          | where the definition of g ∂ varies slightly from that in (2.90)
blank         | 
text          |                                                  ∂f                 ∂f
              |                   g ∂ (u, wr , η; µ̄, Υ) :=         (u, µ̄ + Υη)Υ +    (u, µ̄ + Υη)wr .            (C.7)
              |                                                  ∂µ                 ∂u
blank         | 
text          |                                                      ∂u   ∂u
              | The sensitivity of u with respect to η, i.e.,           =    (η; µ̄, Υ) is defined as the solution of the
              |                                                      ∂η   ∂η
              | sensitivity equations
              |                                     r ∂ (u(η; µ̄, Υ), · , η; µ̄, Υ) = 0,                           (C.8)
blank         | 
text          | where the sensitivity residual is defined as
blank         | 
text          |                                               ∂r                 ∂r
              |                   r ∂ (u, wr , η; µ̄, Υ) :=      (u, µ̄ + Υη)Υ +    (u, µ̄ + Υη)wr .               (C.9)
              |                                               ∂µ                 ∂u
blank         | 
text          | Thus, the sensitivity computation requires the solution of kµ linear systems of equations defined by
              |                                                    ∂r
              | the Jacobian matrix with the kth right-hand side      Υek . For comparison, the sensitivity approach
              |                                                    ∂µ
              | to compute ∇F (µ) would require the solution of Nµ linear systems defined by the Jacobian matrix
              |                      ∂r
              | and right-hand side     ek . From (C.2), the following relationship between the sensitivity of u with
              |                      ∂µ
              | respect to µ and η holds
              |                                   ∂u               ∂u
              |                                      (η; µ̄, Υ) =     (µ̄ + Υur )Υ                             (C.10)
              |                                   ∂η               ∂µ
              |    Even though ku is much smaller than Nu , it may still be sufficiently large to prefer gradient
              | computations via the adjoint method. Following any of the three procedures outlined in Section 2.3.4,
              | the expression for ∇F (η; µ̄, Υ) based on the adjoint method is
blank         | 
text          |                         ∇F (η; µ̄, Υ) = g λ (u(η; µ̄, Υ), λ(η; µ̄, Υ), η; µ̄, Υ) ,                (C.11)
blank         | 
text          | where the definition of g λ varies slightly from that in (2.102)
blank         | 
text          |                                              ∂f                     ∂r
              |                   g λ (u, z, η; µ̄, Υ) :=       (u, µ̄ + Υη)Υ + z T    (u, µ̄ + Υη)Υ.             (C.12)
              |                                              ∂µ                     ∂µ
blank         | 
text          | The adjoint state, λ = λ(η; µ̄, Υ) is defined as the solution of the adjoint equations
blank         | 
text          |                                    r λ (u(η; µ̄, Υ), · , η; µ̄, Υ) = 0,                           (C.13)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                    229
blank         | 
              | 
              | 
text          | where the adjoint residual is defined as
blank         | 
text          |                                               ∂f                ∂r
              |                     r λ (u, z, η; µ̄, Υ) :=      (u, µ̄ + Υη) +    (u, µ̄ + Υη)T z.              (C.14)
              |                                               ∂u                ∂u
blank         | 
text          | Thus, the adjoint computation requires the solution of one linear system of equations defined by the
              | transpose of the Jacobian matrix, regardless of kµ .
blank         | 
              | 
text          | C.1.2     Inner Layer of Reduction: Projection-Based Model Reduction
              | While the first layer of reduction reduces the number of optimization variables, the large cost as-
              | sociated with solving the PDE for any µ ∈ A(µ̄, Υ) remains since the dimensionality of the state
              | space, i.e., number of equations and unknowns, is Nu  1. The second layer of reduction aims
              | to address this source computational expense through the application of projection-based model
              | reduction (Chapter 4).
              |    Let Φ and Ψ be a given trial and test basis defining a minimum-residual projection-based reduced-
              | order model. Introduction of the model reduction ansatz u = Φur into the discretized PDE defined
              | over the parameter space A(µ̄, Υ) and subsequent projection onto the columnspace of Ψ leads to
              | the projection-based reduced-order model
blank         | 
text          |                          rr (ur , η; µ̄, Υ, Φ, Ψ) := ΨT r(Φur , µ̄ + Υη) = 0.                    (C.15)
blank         | 
text          | Denote the unique, continuously differentiable solution of the fully reduced model in (C.15) as
              | ur (η; µ̄, Υ, Φ, Ψ). Substitution of the reconstructed primal reduced-order model solution into the
              | quantity of interest leads to its fully reduced form
blank         | 
text          |                       Fr (η; µ̄, Υ, Φ, Ψ) := f (Φur (η; µ̄, Υ, Φ, Ψ), µ̄ + Υη).                  (C.16)
blank         | 
text          | The gradient of the reduced quantity of interest is computed via the sensitivity (Section 2.3.3) or
              | adjoint (Section 2.3.4) method as
blank         |                                                                                
text          |                                                   ∂ur
              |                   ∇Fr (η; µ̄, Υ, Φ, Ψ) = g ∂ u, Φ     (η; µ̄, Υ, Φ, Ψ), µ̄ + Υη
              |                                                   ∂η                                             (C.17)
              |                                         = g λ (u, Ψλr (η; µ̄, Υ, Φ, Ψ), µ̄ + Υη)
blank         | 
text          |                                                                        ∂ur
              | where u = Φur (η; µ̄, Υ, Φ, Ψ) is the reconstructed primal solution,        (η; µ̄, Υ, Φ, Ψ) is the
              |                                                                         ∂η
              | solution of the reduced-order model sensitivity equations in (4.19), and λr (η; µ̄, Υ, Φ, Ψ) is the
              | solution of the reduced-order model adjoint equations in (4.45). While the adjoint equations are
              | identical to those in Section 4.1.3, the sensitivity equations require the following substitutions since
              | we seek sensitivities with respect to η instead of µ:
blank         | 
text          |                   ∂f          ∂f                            ∂r          ∂r
              |                      (u, µ) ←    (u, µ)Υ            and        (u, µ) ←    (u, µ)Υ               (C.18)
              |                   ∂µ          ∂µ                            ∂µ          ∂µ
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                  230
blank         | 
              | 
              | 
text          | for any u ∈ RNu and µ ∈ RNµ . If the test basis is non-constant, following the developments of
              | Sections 4.1.2 and 4.1.3, the minimum-residual approximation of the gradient ∇F
              |                                                                              dr (µ) can be used
              | to avoid computations involving second derivatives of r
              |                                                                                     !
              |                                              ∂∂u
              |                                               dr
              |          dr (η; µ̄, Υ, Φ, Ψ) = g ∂
              |          ∇F                              u, Φ    (η; µ̄, Υ, Φ∂ , Θ∂ , u), µ̄ + Υη
              |                                               ∂η                                               (C.19)
blank         |                                                                                    
text          |                                                  λ             λ    λ
              |                                 = g λ u( · ), Φ λ̂r (η; µ̄, Υ, Φ , Θ , u), µ̄ + Υη ,
blank         | 
              | 
text          |                                                                       ∂u
              |                                                                       dr
              | where u = Φur (η; µ̄, Υ, Φ, Ψ) is the reconstructed primal solution,      (η; µ̄, Υ, Φ∂ , Θ∂ , u) is
              |                                                                        ∂η
              | the solution of the minimum-residual sensitivity reduced-order model in (4.28), and
              | λ̂r (η; µ̄, Υ, Φλ , Θλ , u) is the solution of the minimum-residual adjoint reduced-order model in
              | (4.56). For the minimum-residual sensitivity ROM in (4.28), the substitutions in (C.18) are required
              | to directly compute sensitivities with respect to η.
              |    This section closes by stating the residual-based error bounds for the fully reduced quantity of
              | interest, its gradient, and minimum-residual gradient approximation. The error bounds are given
              | with respect to the first level of reduction as we are only concerned with the error for a fixed
              | µ ∈ A(µ̄, Υ). These will be used in the multifidelity trust region framework of Chapter 5 to
              | solve (C.4), i.e., the optimization problem after the first layer of reduction. From Lemma B.4, the
              | residual-based error bound on the quantity of interest takes the form
blank         | 
text          |          |F (η; µ̄, Υ) − Fr (η; µ̄, Υ, Φ, Ψ)| ≤ ζ kr(Φur (η; µ̄, Υ, Φ, Ψ), µ̄ + Υη)k           (C.20)
blank         | 
text          | for an arbitrary constant ζ > 0. The residual-based error indicator for the gradient ∇Fr (η; µ̄, Υ, Φ, Ψ)
              | computed with the sensitivity method is
blank         | 
text          |              k∇F (η; µ̄, Υ) − ∇Fr (η; µ̄, Υ, Φ, Ψ)k ≤ κ kr(u, µ)k + τ r ∂ (u, w, µ)            (C.21)
blank         | 
text          | where u = Φur (η; µ̄, Υ, Φ, ΨΥ) is the reconstructed primal solution,
              |        ∂ur
              | w=Φ        (η; µ̄, Υ, Φ, Ψ), is the reconstructed sensitivity solution, and κ, τ > 0 are arbitrary
              |        ∂η
              | constants. The corresponding bound for gradients computed with the adjoint method is
blank         | 
text          |              k∇F (η; µ̄, Υ) − ∇Fr (η; µ̄, Υ, Φ, Ψ)k ≤ κ kr(u, µ)k + τ r λ (u, z, µ)            (C.22)
blank         | 
text          | where u = Φur (η; µ̄, Υ, Φ, ΨΥ) is the reconstructed primal solution, z = Ψλr (η; µ̄, Υ, Φ, Ψ) is
              | the reconstructed adjoint solution, and κ, τ > 0 are arbitrary constants. The residual-based error
              | bounds for the minimum-residual approximation of the gradient of Fr are
blank         | 
text          |       ∇F (η; µ̄, Υ) − ∇Fr (η; µ̄, Υ, Φ, Ψ, Φ∂ , Θ∂ ) ≤ κ kr(u, µ)k + τ r ∂ (u, w, µ)
              |                                                                                                (C.23)
              |       ∇F (η; µ̄, Υ) − ∇Fr (η; µ̄, Υ, Φ, Ψ, Φλ , Θλ ) ≤ κ kr(u, µ)k + τ r λ (u, z, µ)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                  231
blank         | 
              | 
              | 
text          | where u = Φur (η; µ̄, Υ, Φ, ΨΥ) is the reconstructed primal solution,
              |        ∂u
              |         dr
              | w = Φ∂      (η; µ̄, Υ, Φ, Ψ, Φ∂ , Θ∂ ), is the reconstructed minimum-residual sensitivity solution,
              |         ∂η
              | z = Φλ λ̂r (η; µ̄, Υ, Φ, Ψ, Φλ , Θλ ) is the reconstructed minimum-residual adjoint solution, and
              | κ, τ > 0 are arbitrary constants.
blank         | 
text          | Remark. The I-norm used in the error bounds (5.18) can be replaced with the minimum-residual
              | metrics Θ, Θ∂ , Θλ as done in Chapter 5 for greater consistency with the minimum-residual inter-
              | pretation of the reduced-order model. This will be necessary if partially converged solutions are used
              | as snapshots in construction of the reduced-order basis, as discussed in Section 5.2. This will be
              | deferred to future work and the simpler (and less expensive) I-norm will be used.
blank         | 
              | 
title         | C.2      Globally Convergent Multifidelity Trust Region Method
text          | The two-level nested reduction of parametrized partial differential equations with a high-dimensional
              | state and parameter space will serve as a pillar for an efficient method to solve optimization prob-
              | lems constrained by such PDEs. The first layer of reduction restricts the parameter space to the
              | kµ -dimensional affine subspace A(µ̄, Υ) to yield an optimization problem in kµ variables. The
              | second layer of reduction uses projection-based reduced-order models, embedded in the globally
              | convergent multifidelity trust region framework of Chapter 3, i.e., the method developed in Chap-
              | ter 5, to efficiently solve the kµ -dimensional optimization problem. To ensure the method is globally
              | convergent, the restricted parameter space A(µ̄, Υ) is adapted using ideas from linesearch methods.
              | The proposed optimization algorithm based on this nested reduction strategy consists of two types of
              | iterations: (1) an inner iteration where the affine subspace for the parameter, A(µ̄, Υ) is fixed and
              | the multifidelity trust region method based on projection-based reduced-order models (Chapter 5)
              | is applied to solve the optimization problem in (C.4) and (2) an outer iteration that adapts the
              | parameter subspace A(µ̄, Υ) to ensure global convergence of the complete algorithm. The inner
              | iteration is guaranteed to converge to the solution of (C.4) since the multifidelity method introduced
              | in Chapter 5 is globally convergent. The parameter subspace adaptation in the outer iteration will
              | be constructed such that global convergence to the solution of (C.1) is guaranteed. The next two
              | sections detail both the inner and outer iterations.
blank         | 
              | 
text          | C.2.1     Outer Iteration: Globally Convergent Parameter Space Adapta-
              |           tion
              | It is not reasonable to expect an a-prior selection of the restricted parameter subspace A(µ̄, Υ) to
              | lead to a globally convergent algorithm since, in general, µ∗ ∈
              |                                                               / A(µ̄, Υ) where µ∗ is a local minima
              | of F (µ). Therefore, keeping with the theme of this document, this section develops an adaptation
              | strategy for the affine offset µ̄ and subspace Υ defining the restricted parameter space. That is, an
              |                                                                                       j
              | algorithm that constructs a sequence of affine subspaces {A(µ̄j , Υj )} of dimension kµ  Nµ such
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                          232
blank         | 
              | 
              | 
text          | that the iterates {µj }, computed as the solution of the restricted optimization problem
blank         | 
text          |                                         µj+1 :=     arg min        F (µ),                            (C.24)
              |                                                   µ∈A(µ̄j , Υj )
blank         | 
              | 
text          | converge to a stationary point of F (µ) over RNµ , i.e., lim ∇F (µj ) = 0. From the discussion in
              | Section C.1, the definition in (C.24) is equivalent to
blank         | 
text          |                      µj+1 = µ̄j + Υj η j+1             η j+1 = arg min F (η; µ̄j , Υj ),             (C.25)
              |                                                                         j
              |                                                                     η∈Rkµ
blank         | 
text          |                          j
              | i.e., the search in the kµ -dimensional subspace embedded in Nµ is equivalent to an optimization
              |             j
              | problem in kµ variables.
              |    Before launching into the details of the proposed adaptation strategy, recall two standard results
              | from optimization theory stated in Lemma C.1 and Theorem C.1. Theorem C.1 states that any
              | iteration of the form µj+1 = µj + αj pj , where pj is a descent direction at µj and αj > 0 satisfies
              | the Wolfe conditions (C.28), constitutes a globally convergent optimization method and Lemma C.1
              | establishes the existence of a point satisfying the Wolfe conditions for any descent direction. These
              | results are combined to arrive at the following conclusion: if µ̄j = µj and col(Υj ) contains a descent
              | direction of F at µj , the sequence {µj } produced by (C.24) will satisfy limj→∞ ∇F (µj )                = 0.
              | This claim is justified since µj+1 is the exact solution of the optimization problem restricted to
              | A(µ̄j , Υj ) (which contains µj and a descent direction of F (µj )) and, since a point exists that satisfies
              | the sufficient decrease conditions (Lemma C.1), µj+1 must also satisfy them and the iteration is
              | globally convergent (Theorem C.1). This argument is justified rigorously by showing µj+1 satisfies
              | the strong Wolfe conditions since Theorem C.1 guarantees global convergence if these conditions
              | hold. The choice µ̄j = µj implies the affine subspace A(µ̄j , Υj ) contains points of the form
              |                                                                                                      j
              | µ = µj + Υj η. Since col(Υj ) contains a descent direction of F at µj , there must exist η ∈ Rkµ such
              | that pj = (1/αj )Υj η is a descent direction of F at µj for any αj > 0. Thus, the affine subspace
              | A(µ̄j , Υj ) contains vectors of the form µ = µj + αj pj and Lemma C.1 guarantees the existence of
              | an interval of step sizes (αj ) that satisfies the strong Wolfe conditions. Let αj∗ be any such step size.
              | Then the following relations hold
blank         | 
text          |                         F (µj+1 ) ≤ F (µj + αj∗ pj ) ≤ F (µj ) + c1 αj∗ pTj ∇F (µj ),                (C.26)
blank         | 
text          | where the first inequality follows from µj+1 being the solution of the optimization problem in (C.24)
              | and the second holds since αj∗ satisfies the strong Wolfe conditions. This establishes the first strong
              | Wolfe condition in (C.28). For the remaining Wolfe condition, observe that pTj ∇F (µj+1 ) = 0. This
              | follows from the fact that pj = (1/αj )Υj η and the first-order optimality condition of (C.24), i.e.,
              | ΥTj ∇F (µj+1 ) = 0. Therefore, the following relationships hold
blank         | 
text          |                       pTj ∇F (µj+1 ) = 0 ≤ |pTj ∇F (µj + αj∗ pj )| ≤ c2 |pTj ∇F (µj )|,              (C.27)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                    233
blank         | 
              | 
              | 
text          | which establishes that µj+1 satisfies the second Wolfe condition. Therefore, by Theorem C.1, global
              | convergence of the sequence {µj } is guaranteed.
blank         | 
text          | Lemma C.1. Let {µj } be a sequence of iterations that satisfy the update formula µj+1 = µj +αj pj ,
              | where pj is any descent direction at µj . Suppose F (µ) is continuously differentiable and bounded
              | below along the ray {µj + αpj | α > 0}. Then, if 0 < c1 < c2 < 1, there exist intervals of step
              | lengths satisfying the strong Wolfe conditions
blank         | 
text          |                                   F (µj + αj pj ) ≤ F (µj ) + c1 αj pTj ∇F (µj )
              |                                                                                                  (C.28)
              |                            |pTj ∇F (µj + αj pj )| ≤ c2 |pTj ∇F (µj )|.
blank         | 
text          | Proof. Lemma 3.1 of [143].
blank         | 
text          | Theorem C.1. Let {µj } be a sequence of iterations that satisfies the update formula µj+1 =
              | µj + αj pj , where pj is any descent direction at µj and αj satisfies the strong Wolfe conditions
              | (C.28) with 0 < c1 < c2 < 1. Suppose the F is bounded below in RNµ and continuously differentiable
              | in an open set N containing the level set {µ ∈ RNµ | F (µ) ≤ F (µ0 )}. Assume also its gradient is
              | Lipschitz continuous on N . Then
              |                                           lim    ∇F (µj ) = 0.                                   (C.29)
              |                                           j→∞
blank         | 
text          | Proof. Theorem 3.2 of [143].
blank         | 
text          | Remark. In linesearch and subspace methods, it is usually considered difficult or expensive to solve
              | the low-dimensional optimization problem, e.g., (C.24), exactly. This lead to the introduction of the
              | Wolfe conditions (C.28) that define a criteria for sufficient decrease in the objective function that
              | will lead to global convergence (Theorem C.1). As a result, a slew of linesearch methods have been
              | developed to locate points that satisfy the Wolfe conditions [143]. The proposed method deviates from
              | this accepted strategy by solving the restricted optimization problem exactly to leverage the efficient
              | method developed in Chapter 5 for solving PDE-constrained optimization problems in few variables
              | using projection-based reduced-order models in the multifidelity trust region method of Chapter 3. To
              | align with standard practices, the inner iteration can be terminated once the strong Wolfe conditions
              | are satisfied without destroying global convergence.
blank         | 
text          |    Let pj be any descent direction to F at µj . From Theorem C.1, the following requirements on the
              | affine subspace A(µ̄j , Υj ) are sufficient to guarantee the iteration in (C.24) is globally convergent
blank         | 
text          |                                     µ̄j = µj           pj ∈ col(Υj ).                            (C.30)
blank         | 
text          | The simplest affine subspace that fulfills these requirements is defined by
              |                                                             h        i
              |                                µ̄j = µj          Υj = Υgj := ∇F (µj ) ,                          (C.31)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                     234
blank         | 
              | 
              | 
text          | which reduces the iteration in (C.24) to a steepest descent method with an exact linesearch. While
              | this choice will result in a globally convergent iteration, steepest descent methods are well-known
              | to suffer from slow convergence. The remainder of the section will construct a more sophisticated
              | affine subspace such that the iteration in (C.24) quickly converges to a local minima.
              |    From the requirements in (C.30), the affine offset will always be taken as the previous iterate
              | µ̄j = µj . While the requirement in (C.30) provides considerable flexibility in the definition of Υj , we
              | impose the stronger requirement that the affine subspace must contain the steepest descent space:
              | A(µj , Υgj ) ⊆ A(µ̄j , Υj ). This is accomplished by taking the first column of Υj to be ∇F (µj )
              | and guarantees global convergence regardless of the other basis vectors that comprise Υj . These
              | auxiliary basis vectors in Υj will serve to improve the convergence rate of the iteration in (C.24). We
              | will consider two types of auxiliary vectors: (1) optimization-based vectors that are defined for any
              | optimization problem and (2) problem-specific information that exploits any knowledge or structure
              | of the optimization variables µ.
              |    The optimization-based vectors will consist of any variety of descent directions, i.e., Newton or
              | quasi-Newton direction, or directions of negative curvature at the current iterate µj . Let Pj be a
              | matrix consisting of such all optimization-based vectors and define
              |                                             h           i
              |                                         Υj = ∇F (µj ) Pj .                                        (C.32)
blank         | 
text          | This construction is general since the aforementioned directions can be constructed for any opti-
              | mization problem.
              |    In many applications, particularly those related to PDEs, it may be advantageous to incorporate
              | problem-specific information in the affine subspace. This is particularly true for topology optimiza-
              | tion and inverse problems where the optimization vectors have a strong connection to the underlying
              | PDE mesh. The proposed framework is sufficiently flexible to incorporate such information without
              | destroying global convergence by building Υj according to
              |                                           h                        i
              |                                       Υj = ∇F (µj ) Pj        Qj                                  (C.33)
blank         | 
text          | where Qj is a matrix whose columns consist of problem-specific vectors. Future work will develop
              | problem-specific information for various in structural and acoustic inverse problems. Algorithm 17
              | provides the complete outer iteration algorithm.
blank         | 
              | 
title         | C.2.2     Inner Iteration: Multifidelity Optimization with Reduced-Order
              |           Models
text          | Each iteration of the affine parameter space adaptation requires the solution of the PDE-constrained
              | optimization problem (C.24), which can be written as an optimization problem in few variables
              | (kµ  Nµ ). Even though the optimization problem contains few variables, it is still expensive
              | to solve since each objective evaluation requires the solution of a potentially large-scale partial
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                         235
blank         | 
              | 
              | 
text          | Algorithm 17 Outer iteration: adaptive reduction of parameter space
              |  1:   Initialization: Given
              |                                                        µ̄0 , Υ0
              |  2:   Inner iteration: Solve restricted optimization problem (Algorithm 18)
blank         | 
text          |                                             minimize
              |                                                  j
              |                                                      F (µ̄j + Υj η)
              |                                               η∈Rkµ
blank         | 
text          |       for η ∗j , the optimal solution in the restricted parameter space and define µ∗j = µ̄j + Υj η ∗j
              |  3:   Update search space: Compute ∇F (µ∗j ), the optimization-based vectors P (µ∗j ), and the
              |       problem-specific vectors Q(µ∗j ) and update the restricted parameter space
blank         | 
text          |                            µ̄j+1 = µ∗j           Υj+1 = ∇F (µ∗j ) P (µ∗j ) Q(µ∗j )
blank         |                                                                                  
              | 
              | 
              | 
              | 
text          | differential equation, and the gradient requires a sensitivity or adjoint solution. The multifidelity
              | trust region method based on projection-based model reduction proposed in Chapter 5 has been
              | shown to be an efficient method to handle exactly these types of problems. This section will consider
              | a special case of the method proposed in Chapter 5 to solve each kµ -variable optimization problem
              | encountered in the iteration (C.24).
              |       Consider the optimization problem that arises at iteration j of (C.24)
blank         | 
text          |                                                minimize        F (µ)                                 (C.34)
              |                                               µ∈A(µ̄j , Υj )
blank         | 
              | 
text          |                                                                     j
              | which, from the definition of F and A(µ̄, Υ), is equivalent to the kµ -dimensional optimization
              | problem
              |                                           minimize
              |                                                j
              |                                                    F (η; µ̄j , Υj ).                                 (C.35)
              |                                             η∈Rkµ
blank         | 
text          | We propose to solve this PDE-constrained optimization problem in few parameters using the method
              | proposed in Chapter 5, i.e., the the multifidelity trust region method using reduced-order/hyperreduced
              | approximation models. For a fixed outer iteration j, the approximation model at iteration k of the
              | trust region method is defined as
blank         | 
text          |                                   mj, k (η) = Fr (η; µ̄j , Υj , Φj, k , Ψj, k ),                     (C.36)
blank         | 
text          | where Fr is defined in (C.16) and Φj, k , Ψj, k are presumed given and define a projection-based
              | reduced-order model that possesses the minimum-residual property. Details pertaining to the con-
              | struction of the trial basis Φj, k (and implicitly the test basis Ψj, k based on the minimum-residual
              | requirement (4.14)) are provided at the end of this section. The gradient of the approximation model
              | is computed exactly as
              |                                 ∇mj, k (η) = ∇Fr (η; µ̄j , Υj , Φj, k , Ψj, k )                      (C.37)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                               236
blank         | 
              | 
              | 
text          | according to the sensitivity or adjoint method as defined in Section 2.3. In situations where the test
              | basis is not constant, the exact gradient is cumbersome to compute and may be approximated with
blank         | 
text          |                           ∇m           d r (η; µ̄j , Υj , Φj, k , Ψj, k , Φ∂j, k , Θ∂j, k )
              |                           d j, k (η) = ∇F
              |                                                                                                             (C.38)
              |                                          d r (η; µ̄j , Υj , Φj, k , Ψj, k , Φλ
              |                                        = ∇F                                          λ
              |                                                                              j, k , Θj, k )
blank         | 
              | 
text          | using minimum-residual sensitivity or adjoint reduced-order models.
              |    A critical component of the multifidelity trust region method of Chapter 3 is the introduction
              | of an objective decrease error indicator ϑk (µ) and gradient error indicator ϕk (µ) that lead to the
              | error bounds
blank         | 
text          |                 |F (η j, k ; µ̄j , Υj ) − F (η; µ̄j , Υj ) + mj, k (η) − mj, k (η j, k )| ≤ ζϑj, k (η)
              |                                                                                                             (C.39)
              |                                                    ∇F (η; µ̄j , Υj ) − ∇mj, k (η) ≤ ξϕj, k (η),
blank         | 
text          | where ζ, ξ > 0 are arbitrary constants and η j, k is the trust region center in the reduced parameter
              | space. Two options are considered for the objective decrease error indicator: the classical trust region
              | constraint ϑj, k (η) = η − η j, k and the residual-based error indicator introduced in Section 5.1.1
blank         | 
text          |                    ϑj, k (η) = r(Φj, k ur (η j, k ; µ̄j , Υj , Φj, k , Ψj, k ), µ̄j + Υj η j, k ) +
              |                                                                                                             (C.40)
              |                                 r(Φj, k ur (η; µ̄j , Υj , Φj, k , Ψj, k ), µ̄j + Υj η) .
blank         | 
text          | From the discussion in Section 5.1.1 that refers to the proof in Appendix B, the residual-based error
              | indicator satisfies the bound in (3.12). The classical trust region satisfies this bound, provided the
              | gradient bound and condition hold (see Chapter 3 for a complete discussion). For simplicity, only
              | the classical trust region constraint will be considered in the remainder; see Chapter 5 for a complete
              | discussion regarding the use of the residual-based error indicator. From the bounds on the gradient
              | error derived in Lemmas B.7 and B.8, the gradient error indicator is taken as
blank         | 
text          |  ϕj, k (η) =α1 r(Φj, k ur (η; µ̄j , Υj , Φj, k , Ψj, k ), µ̄j + Υj η) +
blank         |                                                                                                             
text          |                 ∂                                                ∂ur
              |             α2 r Φj, k ur (η; µ̄j , Υj , Φj, k , Ψj, k ), Φj, k      (η; µ̄j , Υj , Φj, k , Ψj, k ), µ̂ + Υη
              |                                                                   ∂η
              |                                                                                                            (C.41)
              | if the sensitivity approach is used to compute ∇mj, k (η) and
blank         | 
text          |    ϕj, k (η) =α1 r(Φj, k ur (η; µ̄j , Υj , Φj, k , Ψj, k ), µ̄j + Υj η) +
              |                α2 r λ Φj, k ur (η; µ̄j , Υj , Φj, k , Ψj, k ), Ψj, k λr (η; µ̄j , Υj , Φj, k , Ψj, k ), µ̂ + Υη
blank         |                                                                                                                 
              | 
text          |                                                                                                               (C.42)
              | if the adjoint approach is used. These indicators can be modified accordingly if the minimum-
              | residual sensitivity or adjoint approach is used to compute the gradient approximation ∇m
              |                                                                                        d j, k (µ).
              | Finally, the trust region method of Chapter 3 provides the flexibility to introduce an inexpensive
              | approximation of the objective decrease ψj, k (η) and corresponding error indicator θj, k (η) to mitigate
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                          237
blank         | 
              | 
              | 
text          | the computational burden of computing the actual-to-predicted reduction at each trust region step.
              | Section 5.3 details an approach that defines ψj, k (η) based on partially converged PDE solutions and
              | θj, k (η) as the residual-based error indicator. This construction can be used in this context without
              | modification and does not need to be discussed further.
              |    With the definition of the necessary approximations and corresponding error indicators, the only
              | remaining conditions that are left to satisfy are the error conditions in (3.14) and (3.15), restated
              | here for convenience
              |                               ϑj, k (η j, k ) ≤ κϑ ∆j, k
              |                                                                                                       (C.43)
              |                               ϕj, k (η j, k ) ≤ κϕ min{ ∇mj, k (η j, k ) , ∆j, k }.
blank         | 
text          | Since the classical trust region constraint is used to define ϑj, k (η), the first condition is always satis-
              | fied since ϑj, k (η j, k ) = 0. The second condition, called the gradient condition, is not always satisfied
              | a priori and relies critically on the construction of the reduced-order model. The strategy taken con-
              | structs the reduced-order model such that the reconstructed primal and sensitivity/adjoint solutions
              | exactly match the corresponding high-dimensional model quantity. This will obviously guarantee
              | ϕj, k (η j, k ) = 0 and the gradient condition will be satisfied. Without repeating the details from
              | Section 5.1.2, the reduced-order model and its sensitivity/adjoint will be possess these interpola-
              | tion properties provided primal and sensitivity/adjoint minimum-residual reduced-order models are
              | used, the relationships between the reduced-order bases in (4.35) and (4.63) hold, and the trial basis
              | possesses the following properties
blank         | 
text          |                                                        u(η j, k ; µ̄j , Υj ) ∈ col(Φj, k )
              |                                                      ∂u
              |                                                         (η ; µ̄ , Υj ) ∈ col(Φj, k )                  (C.44)
              |                                                      ∂η j, k j
              |                                           ∂r
              |                            Θλ
              |                             j, k (u, µ)      (u, µ)T λ(η j, k ; µ̄j , Υj ) ∈ col(Φj, k )
              |                                           ∂u
blank         | 
text          | where µ = µ̄j + Υj η and u = u(µ). The conditions in (4.14), (4.35), and (4.63) completely specify
              | the test Ψj, k , sensitivity Φ∂j, k , and adjoint Φλ
              |                                                    j, k bases in terms of the trial basis Φj, k and optimality
              | metrics Θj, k , Θ∂j, k , Θλ
              |                           j, k . Therefore, the reduced-order model will possess the required interpolation
              | properties provided the trial basis is constructed such that (C.44) holds.
blank         | 
text          | Remark. The requirement that the reduced-order model is exact at the trust region center leads
              | to the stronger condition ϕj, k (η j, k ) = 0 than required by (3.15) and may result in wasted effort.
              | The weaker condition in (3.15) can be enforced directly using partially converged solutions in the
              | construction of Φj, k as detailed in Section 5.2; however, this is not considered in this section.
blank         | 
text          |    Before continuing with the construction of Φj, k , the following notation is introduced to allow
              | the sensitivity and adjoint method to be treated simultaneously and compactly:
blank         | 
text          |                                ∂u
              |                              
              |                              
              |                                  (η; µ̄, Υ)                   sensitivity method
              |                v̂(η; µ̄, Υ) = ∂η                                                                      (C.45)
              |                              Θλ (u, µ) ∂r (u, µ)T λ(η; µ̄, Υ) adjoint method
              |                              
              |                                 j, k
              |                                            ∂u
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                           238
blank         | 
              | 
              | 
text          | where µ = µ̄j + Υj η and u = u(µ). Since the sensitivity and adjoint method are rarely employed
              | simultaneously the condition in (C.44) is weakened to
blank         | 
text          |                                         u(η j, k ; µ̄j , Υj ) ∈ col(Φj, k )
              |                                                                                                          (C.46)
              |                                         v̂(η j, k ; µ̄j , Υj ) ∈ col(Φj, k ).
blank         | 
text          | Next, define primal and dual snapshot matrices according to the recursive relationships
              |                            h                                                                         i
              |                    Uj, k = Uj−1, nj−1        u(η j, 0 , µ̄j , Υj ) · · ·    u(η j, k−1 , µ̄j , Υj )
              |                            h                                                                         i   (C.47)
              |                    V̂j, k = V̂j−1, nj−1      v̂(η j, 0 , µ̄j , Υj ) · · ·   v̂(η j, k−1 , µ̄j , Υj )
blank         | 
text          | where U−1, k = ∅, V̂−1, k = ∅, and nj is the number of inner iterations corresponding to outer
              | iteration j. Then, the reduced-order basis is defined according to the heterogeneous, span-preserving
              | variant of POD (Algorithm 7) as
blank         | 
text          |                    Φj, k = PODHSP(u(η j, k , µ̄j , Υj ), Uj, k , v̂(η j, k , µ̄j , Υj ), V̂j, k ).       (C.48)
blank         | 
text          | By construction, the basis satisfies (C.44) and possesses additional information to improve the para-
              | metric robustness of the reduced-order model. The complete inner iteration algorithm is provided
              | in Algorithm 18.
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                                                      239
blank         | 
              | 
              | 
text          | Algorithm 18 Inner iteration: trust region method based on reduced-order models in reduced
              | parameter space
              |  1:   Initialization: Given
              |             µ̄j , Υj , η j, 0 , Uj−1, nj−1 , V̂j−1, nj−1 , ∆j, 0 , 0 < γ < 1, ∆max > 0, 0 < η1 < η2 < 1,
              |                              0 < κϑ < 1, 0 < κϕ , 0 < ω < 1, {rk }∞     k=0 such that rk → 0
blank         | 
text          |  2:   Model and constraint update: If previous model and constraint are sufficient for convergence
blank         | 
text          |              ϑj, k−1 (η j, k ) ≤ κϑ ∆j, k                 ϕj, k−1 (η j, k ) ≤ κϕ min{ ∇mj, k−1 (η j, k ) , ∆j, k },
blank         | 
text          |       re-use for the current iteration: mj, k (η) := mj, k−1 (η) and ϑj, k (η) := ϑj, k−1 (η). Otherwise,
              |       evaluate primal and sensitivity or adjoint solution of high-dimensional model
              |                                                 ∂u                                   ∂r
              |         uj, k := u(µj, k )          v̂j, k :=      (µ ) or Θλ
              |                                                             j, k (u(µj, k ), µj, k )    (u(µj, k ), µj, k )T λ(µj, k )
              |                                                 ∂µ j, k                              ∂u
              |       where µj, k = µ̄j + Υj η j, k and compute reduced-order basis via span-preserving variant of POD
              |       (Algorithm 7)
              |                                      Φj, k = PODHSP(uj, k , Uj, k , v̂j, k , V̂j, k ),
              |       define model and constraint as
              |                        mj, k (η) = f (Φj, k ur (µ̄j + Υj η; Φj, k , Ψj, k ), µ̄j + Υj η)
              |                        ϑj, k (η) = r(Φj, k ur (µ̄j + Υj η j, k ; Φj, k , Ψj, k ), µ̄j + Υj η j, k )            Θj, k
              |                                                                                                                        +
              |                                        r(Φj, k ur (µ̄j + Υj η; Φj, k , Ψj, k ), µ̄j + Υj η)           Θj, k
              |                                                                                                               ,
blank         | 
text          |       and update snapshot matrices
blank         |                                                                                                                               
text          |           Uj, k+1 ← Uj−1, nj−1 uj, 0                ···   uj, k           V̂j, k+1 ← V̂j−1, nj−1           v̂j, 0   ···    v̂j, k .
blank         | 
text          |  3:   Step computation: Solve (exactly) the trust region subproblem
blank         | 
text          |                                       min mj, k (η)           subject to           ϑj, k (η) ≤ ∆j, k
              |                                     η∈Rkµ
blank         | 
text          |     for a candidate, η̂ j, k , using interior-point method of Section 3.1.2.
              |  4: Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio
              |            
              |            
              |                        1                                if ϑj, k (η̂ j, k )ω ≤ η min{mj, k (η j, k ) − mj, k (η̂ j, k ), rk }
              |     ρj, k = F (µ̄j + Υj η j, k ) − F (µ̄j + Υj η̂ j, k )
              |                                                         otherwise
              |                    mj, k (η j, k ) − mj, k (η̂ j, k )
              |            
blank         | 
text          |     where η < min{η1 , 1 − η2 }
              |  5: Step acceptance:
blank         | 
text          |              if        ρj, k ≥ η1        then         η j, k+1 = η̂ j, k         else       η j, k+1 = η j, k          end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                   if      ρj, k ≤ η1                  then            ∆k+1 ∈ (0, γϑj, k (η̂ j, k )]                 end if
              |                   if      ρj, k ∈ (η1 , η2 )          then            ∆k+1 ∈ [γϑj, k (η̂ j, k ), ∆j, k ]            end if
              |                   if      ρj, k ≥ η2                  then            ∆k+1 ∈ [∆j, k , ∆max ]                        end if
meta          | Appendix D
blank         | 
title         | Time-Dependent PDE-Constrained
              | Optimization under Periodicity
              | Constraints
blank         | 
text          | This appendix summarizes the work in [211, 212].
blank         | 
              | 
title         | D.1       Governing Equations and Discretization
text          | This section is devoted to the treatment of conservation laws (2.9) on a parametrized, deforming
              | domain using an Arbitrary Lagrangian-Eulerian (ALE) description of the governing equations and
              | a brief discussion of a globally high-order numerical discretization of the ALE form of the system of
              | conservation laws that closely parallels that in Chapter 2. Subsequently, Section D.2 will develop
              | the corresponding fully discrete adjoint equations and the adjoint method for constructing gradients
              | of quantities of interest.
              |    The methods introduced in this work are not necessarily limited to Partial Differential Equations
              | (PDE) that can be written as conservation laws (D.1). In Section D.1.2, the chosen spatial dis-
              | cretization (discontinuous Galerkin Arbitrary Lagrangian-Eulerian method) is applied to the PDE,
              | resulting in a system of first-order ODEs, which is the point of departure for all adjoint-related
              | derivations. Time-dependent PDEs that are not conservation laws can be written similarly at the
              | semi-discrete level after application of an appropriate spatial discretization, e.g., a continuous fi-
              | nite element method for parabolic PDEs. In this work, the scope is limited to first-order temporal
              | systems, or those which are recast as such.
blank         | 
              | 
              | 
              | 
meta          |                                                  240
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        241
blank         | 
              | 
              | 
text          |                                                                      nda
blank         | 
              | 
              | 
text          |                                    N dA             x=x(X)
              |                                                                            v
blank         | 
text          |                                                               x2
blank         | 
text          |                                           V
blank         | 
text          |                               X2                                       x1
blank         | 
              | 
              | 
text          |                                      X1
blank         | 
              | 
text          |           Figure D.1: Time-dependent mapping between reference and physical domains.
blank         | 
              | 
text          | D.1.1     System of Conservation Laws on Deforming Domain: Arbitrary
              |           Lagrangian-Eulerian Description
              | Consider a general system of conservation laws, defined on a parametrized, deforming domain,
              | v(µ, t), written at the continuous level as
blank         | 
text          |                                 ∂U
              |                                    + ∇ · F (U , ∇U ) = 0           in v(µ, t)                    (D.1)
              |                                 ∂t
blank         | 
text          | where the physical flux is decomposed into an inviscid and a viscous part F (U , ∇U ) = F inv (U ) +
              | F vis (U , ∇U ), U (x, µ, t) is the solution of the system of conservation laws, t ∈ (0, T ) represents
              | time, and µ ∈ RNµ is a vector of parameters. This work will focus on the case where the domain
              | is parametrized by µ, although extension to other types of parameters, e.g., constants defining the
              | conservation law, is straightforward. The conservation law on a deforming domain is transformed
              | into a conservation law on a fixed reference domain through the introduction of a time-dependent
              | mapping between the physical and reference domains, resulting in an Arbitrary Lagrangian-Eulerian
              | description of the governing equations.
              |    Denote the physical domain by v(µ, t) ⊂ Rnsd and the fixed, reference domain by V ⊂ Rnsd ,
              | where nsd is the number of spatial dimensions. At each time t, let G be a time-dependent diffeomor-
              | phism between the reference domain and physical domain: x(X, µ, t) = G(X, µ, t), where X ∈ V is
              | a point in the reference domain and x(X, µ, t) ∈ v(µ, t) is the corresponding point in the physical
              | domain at time t and parameter configuration µ.
              |    The transformed system of conservation laws from (D.1), under the mapping G, defined on the
              | reference domain takes the form
blank         | 
text          |                                ∂UX
              |                                               + ∇X · FX (UX , ∇X UX ) = 0                        (D.2)
              |                                 ∂t    X
blank         | 
text          | where ∇X denotes spatial derivatives with respect to the reference variables, X. The transformed
              | state vector, UX , and its corresponding spatial gradient with respect to the reference configuration
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        242
blank         | 
              | 
              | 
text          | take the form
              |                                                                   ∂g
              |                           UX = gU ,          ∇X UX = g −1 UX         + g∇U · G,                  (D.3)
              |                                                                   ∂X
              |                                         ∂x   ∂G
              | where G = ∇X G, g = det(G), vG =           =    , and the arguments have been dropped, for brevity.
              |                                         ∂t   ∂t
              | The transformed fluxes are
blank         | 
text          |                              inv          vis
              |          FX (UX , ∇X UX ) = FX   (UX ) + FX   (UX , ∇X UX ),
              |                 FX inv
              |                        (UX ) = gF inv (g −1 UX )G−T − UX ⊗ G−1 vG ,                              (D.4)
blank         |                                                                          
text          |                                                                     ∂g
              |          vis
              |         FX   (UX , ∇X UX ) = gF vis g −1 UX , g −1 ∇X UX − g −1 UX       G−1 G−T .
              |                                                                     ∂X
blank         | 
text          | For details regarding the derivation of the transformed equations, the reader is referred to [152].
              |    When integrated using inexact numerical schemes, this ALE formulation does not satisfy the
              | Geometric Conservation Law (GCL) [60, 152]. This is overcome by introducing an auxiliary variable
              | ḡ, defined as the solution of
              |                                         ∂ḡ
              |                                             − ∇X · gG−1 vG = 0.
blank         |                                                           
text          |                                                                                                  (D.5)
              |                                         ∂t
              | The auxiliary variable, ḡ is used to modify the transformed conservation law according to
blank         | 
text          |                                  ∂UX̄
              |                                             + ∇X · FX̄ (UX̄ , ∇X UX̄ ) = 0                       (D.6)
              |                                   ∂t    X
blank         | 
text          | where the GCL-transformed state variables are
blank         | 
text          |                                                                   ∂ḡ
              |                           UX̄ = ḡU ,        ∇X UX̄ = ḡ −1 UX̄       + ḡ∇U · G                 (D.7)
              |                                                                   ∂X
blank         | 
text          | and the corresponding fluxes
blank         | 
text          |                                 inv           vis
              |          FX̄ (UX̄ , ∇X UX̄ ) = FX̄  (UX̄ ) + FX̄  (UX̄ , ∇X UX̄ ),
              |                 FX̄ inv
              |                         (UX̄ ) = gF inv (ḡ −1 UX̄ )G−T − UX̄ ⊗ G−1 vG ,                         (D.8)
blank         |                                                                                  
text          |                                                                            ∂ḡ
              |          vis
              |         FX̄  (UX̄ , ∇X UX̄ ) = gF vis ḡ −1 UX̄ , ḡ −1 ∇X UX̄ − ḡ −1 UX̄       G−1 G−T .
              |                                                                            ∂X
blank         | 
text          | It was shown in [152] that the transformed equations (D.6) satisfy the GCL. In the next section, the
              | ALE description of the governing equations (D.2) and (D.6) will be converted to first-order form
              | and discretized via a high-order discontinuous Galerkin method.
blank         | 
              | 
title         | D.1.2      Arbitrary Lagrangian-Eulerian Discontinuous Galerkin Method
text          | The ALE description of the conservation law without GCL augmentation will be considered first. To
              | proceed, the second-order system of partial differential equations in (D.2) is converted to first-order
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          243
blank         | 
              | 
              | 
text          | form
              |                                   ∂UX
              |                                              + ∇X · FX (UX , QX ) = 0
              |                                    ∂t    X                                                         (D.9)
              |                                                       QX − ∇X UX = 0,
blank         | 
text          | where QX is introduced as an auxiliary variable to represent the spatial gradient of the UX . Equa-
              | tion (D.9) is discretized using a standard nodal discontinuous Galerkin finite element method [46],
              | which, after local elimination of the auxiliary variables QX , leads to the following system of ODEs
blank         | 
text          |                                              ∂uX
              |                                       MX         = ruX (uX , µ, t),                               (D.10)
              |                                               ∂t
blank         | 
text          | where MX is the block-diagonal, symmetric, fixed mass matrix, uX is the vectorization of UX at
              | all nodes in the high-order mesh, and ruX is the nonlinear function defining the DG discretization
              | of the inviscid and viscous fluxes.
              |    The GCL augmentation is treated identically, i.e., conversion to first-order form and subsequent
              | application of the discontinuous Galerkin finite element method, where UX̄ is taken as the state
              | variable. The result is a system of ODEs corresponding to a high-order ALE scheme that satisfies
              | the GCL
              |                                           ∂ ḡ
              |                                         Mḡ    = rḡ (µ, t)
              |                                           ∂t                                                      (D.11)
              |                                         ∂u
              |                                       MX X̄ = ruX̄ (uX̄ , ḡ, µ, t)
              |                                          ∂t
              | where each term is defined according to their counterparts in (D.10). From the conservation law
              | defining ḡ (D.5), the corresponding flux is continuous, implying the physical flux gG−1 vG can be
              | used as the numerical flux. This implies no information is required from neighboring elements and
              | (D.5) can be solved at the element level, i.e., statically condensed. Furthermore, the ḡ residual, rḡ ,
              | does not depend on ḡ itself since the physical flux gG−1 vG is independent of ḡ.
              |    Since the equation for ḡ does not depend on uX̄ , it can be solved independently of the equation
              | for uX̄ . This enables ḡ to be considered an implicit function of µ, i.e., ḡ = ḡ(µ, t), through
              | application of the implicit function theorem. Then, (D.11) reduces to
blank         | 
text          |                                        ∂uX̄
              |                                  MX         = ruX̄ (uX̄ , ḡ(µ, t), µ, t).                        (D.12)
              |                                         ∂t
blank         | 
text          | Equations (D.10) and (D.12) are abstracted into the following system of ODEs
blank         | 
text          |                                                  ∂u
              |                                              M      = r(u, µ, t),                                 (D.13)
              |                                                  ∂t
blank         | 
text          | for convenience in the derivation of the fully discrete adjoint equations. Evaluation of the residual,
              | r, in (D.13) at parameter µ and time t requires evaluation of the mapping, x(µ, t) and ẋ(µ, t),
              | and ḡ(µ, t), if GCL augmentation is employed. The implicit dependence of ḡ on µ requires special
              | treatment when computing derivatives with respect to µ, which will be required in the adjoint
              | method (Section D.2). Treatment of such terms will be deferred to Section D.2.4.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                        244
blank         | 
              | 
              | 
text          |    A convenient property of this DG-ALE scheme is that all computations are performed on the
              | reference domain which is independent of time and parameter. This implies that the mass matrix
              | of the ODE (D.13) is also time- and parameter-independent, which simplifies all adjoint compu-
              |                                                        ∂M        ∂M
              | tations introduced in Section D.2 as terms involving        and       are identically zero. This,
              |                                                         ∂u       ∂µ
              | in turn, simplifies the implementation of the adjoint method and translates to computational sav-
              | ings since contractions with these third-order tensor are not required; see [88] for a discretization
              | with parameter-dependent mass matrices and the corresponding adjoint derivation. In subsequent
              | sections, it will be assumed that the mass matrix is time- and parameter-independent.
              |    The DG-ALE scheme outlined in this section constitutes a spatial discretization, which yields a
              | system of ODEs when applied to the PDE in (D.1). The semi-discrete form of the conservation law
              | is the point of departure for the remainder of this document. The subsequent development applies to
              | any system of ODEs of the form (D.13) without relying on the specific spatial discretization scheme
              | employed. The DG-ALE scheme was chosen to provide a high-order, stable spatial discretization of
              | the conservation law (D.1).
              |    The diagonally implicit Runge-Kutta scheme introduced in Section 2.1.3 is applied to the system
              | of ODEs for a stable, high-order implicit discretization, repeated here for convenience
blank         | 
text          |                                         u(0) = u0 (µ)
              |                                                               s
              |                                                                         (n)
              |                                                               X
              |                                         u(n) = u(n−1) +             bi ki                                       (D.14)
              |                                                               i=1
blank         |                                                                            
text          |                                          (n)            (n)
              |                                   M ki         = ∆tn r ui , µ, tn−1 + ci ∆tn ,
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s, where Nt are the number of time steps in the temporal dis-
              | cretization and s is the number of stages in the DIRK scheme. The temporal domain, [0, T ] is
              | discretized into Nt segments with endpoints {t0 , t1 , . . . , tNt }, with the nth segment having length
              |                                                                               (n)
              | ∆tn = tn − tn−1 for n = 1, . . . , Nt . Additionally, in (D.14), ui                 is used to denote the approximation
              |     (n)
              | of u      at the ith stage of time step n
blank         | 
text          |                                                                                         i
              |                         (n)       (n)               (n)                                           (n)
              |                                                                                         X
              |                       ui      = ui (u(n−1) , k1 , . . . , ks(n) ) = u(n−1) +                  aij kj .          (D.15)
              |                                                                                         j=1
blank         | 
              | 
text          | From (D.14), a complete time step requires the solution of a sequence of s nonlinear systems of
              | equation of size Nu .
              |    Finally, a solver-consistent discretization (Section 2.1.4) is applied to discretize output quantities
              | of interest that take the form
              |                                                       Z tZ
              |                                      F(U , µ, t) =                f (U , µ, τ ) dS dτ                           (D.16)
              |                                                           0   Γ
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                245
blank         | 
              | 
              | 
text          | to yield the update equations in (2.45) and the fully discrete quantity of interest
blank         | 
text          |                                                                     (1)
              |                                        F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) )
blank         | 
text          | in (2.46). The generalization to other types of quantities of interest, such as volumetric integrals and
              | instantaneous or pointwise quantities of interest, is immediate as the specific form of the quantity
              | of interest will be abstracted away at the fully discrete level. The form in (D.16) will be used in the
              | physical setup of the applications in Sections D.2.5–D.2.6.
blank         | 
              | 
title         | D.2       Fully Discrete, Time-Dependent Adjoint Equations
text          | The purpose of this section is to derive an expression for the total derivative of the discrete quantity
              | of interest F in (2.46), which can be expanded as
blank         | 
text          |                                              N                            N    s                 (n)
              |                           dF   ∂F   Xt
              |                                        ∂F ∂u(n) X   t X
              |                                                            ∂F ∂ki
              |                              =    +      (n)
              |                                                 +            (n) ∂µ
              |                                                                     ,                                  (D.17)
              |                           dµ   ∂µ n=0 ∂u     ∂µ   n=1 i=1 ∂k                           i
blank         | 
text          |                                                                                                  (n)
              |                                                                ∂u(n)       ∂ki
              | that depends on the sensitivities of the state variables,            and        . Each of the Nµ state
              |                                                                 ∂µ          ∂µ
              | variable sensitivities is the solution of a linear evolution equation of the same dimension and number
              | of steps as the primal equation (D.14), rendering these quantities intractable to compute when Nµ is
              | large. Elimination of the state variable sensitivities from (D.17) is accomplished through introduction
              | of the adjoint equations corresponding to the functional F , and the corresponding dual variables.
              | From the derivation of the adjoint equation in Section D.4.1, an expression for the reconstruction of
              | the gradient of F , independent of the state variables sensitivities, follows naturally. At this point,
              | it is emphasized that F represents any quantity of interest whose gradient is desired, such as the
              | optimization objective function or a constraint. This section concludes with a discussion of the
              | advantages of the fully discrete framework in the setting of the high-order numerical scheme.
              |    Before proceeding to the derivation of the adjoint method, the following definitions are introduced
              | for the Runge-Kutta stage equations and state updates
blank         | 
text          |                                   r̃ (0) (u(0) , µ) = u(0) − u0 (µ) = 0
              |                                                                                s
              |                             (n)                                                            (i)
              |                                                                                X
              |     r̃ (n) (u(n−1) , u(n) , k1 , . . . , ks(n) , µ) = u(n) − u(n−1) −                bi ki = 0         (D.18)
              |                                                                                i=1
blank         |                                                                                                
text          |               (n)           (n)           (n)                (n)            (n)
              |             Ri (u(n−1) , k1 , . . . , ki , µ) = M ki               − ∆tn r ui , µ, tn−1 + ci ∆tn = 0
blank         | 
meta          | for n = 1, . . . , n and i = 1, . . . , s. Differentiation of these expressions with respect to µ gives rise to
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                      246
blank         | 
              | 
              | 
text          | the fully discrete sensitivity equations
blank         | 
text          |                                                                   ∂ r̃ (0)   ∂ r̃ (0) ∂u(0)
              |                                                                            +                =0
              |                                                                    ∂µ        ∂u(0) ∂µ
              |                                                                            s            (n)
              |                       ∂ r̃ (n)   ∂ r̃ (n) ∂u(n)    ∂ r̃ (n) ∂u(n−1) X ∂ r̃ (n) ∂kp
              |                                +                +                    +                      =0
              |                        ∂µ        ∂u(n) ∂µ         ∂u(n−1) ∂µ                     (n) ∂µ                        (D.19)
              |                                                                          p=1 ∂kp
              |                                           (n)           (n)           i     (n)           (n)
              |                                       ∂Ri            ∂Ri    ∂u(n−1) X ∂Ri ∂kj
              |                                                 +                  +               =0
              |                                        ∂µ           ∂u(n−1) ∂µ       j=1 ∂k
              |                                                                             (n) ∂µ
              |                                                                                   j
blank         | 
              | 
text          | where n = 1, . . . , Nt , i = 1, . . . , s, and arguments have been dropped.
blank         | 
              | 
title         | D.2.1       Derivation
text          | The derivation of the fully discrete adjoint equations corresponding to the quantity of interest, F ,
              | begins with the introduction of test variables
blank         | 
text          |                                                               (n)
              |                                              λ(0) , λ(n) , κi       ∈ RNu                                      (D.20)
blank         | 
text          |                                                                                                  dF
              | for n = 1, . . . , Nt and i = 1, . . . , s. To eliminate the state sensitivities from the expression for
              |                                                                                                      in
              |                                                                                                  dµ
              | (D.17), multiply the sensitivity equations (D.19) by the test variables, integrate (sum in the discrete
              | setting) over the time domain, and subtract from the expression for the gradient in (D.17) to obtain
blank         | 
              | 
text          |                 Nt                      Nt X   s           (n)
              |                     ∂F ∂u(n) X
              |                                                                              (0)
              |                                                                                         ∂ r̃ (0) ∂u(0)
blank         |                                                                                                          
text          |      dF   ∂F   X                                   ∂F ∂ki            (0) T ∂ r̃
              |         =    +                      +                           −λ                   +
              |      dµ   ∂µ n=0 ∂u(n) ∂µ              n=1 i=1 ∂ki
              |                                                      (n) ∂µ                   ∂µ        ∂u(0) ∂µ
              |                 Nt
              |                           "                                                           s
              |                                                                                                            #
              |                                  (n)          (n)    (n)         (n)       (n−1)                (n)    (n)
              |                X        T   ∂ r̃         ∂ r̃     ∂u        ∂ r̃      ∂u             X    ∂  r̃     ∂k p
              |              −     λ(n)               +                  +                        +                            (D.21)
              |                n=1
              |                               ∂µ         ∂u(n) ∂µ          ∂u(n−1) ∂µ                p=1 ∂kp
              |                                                                                                 (n) ∂µ
blank         | 
text          |                                                                                              
              |                 Nt Xs                  (n)          (n)                  i       (n)    (n)
              |                X         (n) T  ∂Ri              ∂Ri    ∂u(n−1) X ∂Ri ∂kj 
              |              −         κi                   +                        +                           .
              |                n=1 i=1
              |                                      ∂µ         ∂u(n−1) ∂µ             j=1 ∂k
              |                                                                                 (n) ∂µ
              |                                                                                    j
blank         | 
              | 
text          |                                                                             dF
              | The right side of the equality in (D.21) is an equivalent expression for       for any value of the test
              |                                                                             dµ
              | variables since the terms in the brackets are zero, i.e., the sensitivity equations. Re-arrangement of
              |                                                         dF
              | terms in (D.21) leads to the following expression for       , where the state variable sensitivities have
              |                                                         dµ
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                   247
blank         | 
              | 
              | 
text          | been isolated
blank         | 
text          |                                         (Nt )                  Nt                        Nt Xs            (n)
              |                                                   ∂u(Nt ) X                      (n)
blank         |                                              
text          |    dF   ∂F       ∂F        (Nt ) T ∂ r̃                               (n) T ∂ r̃
              |                                                                                         X
              |                                                                                                   (n) T ∂Rp
              |       =    +           − λ                                  −       λ                −           κp
              |    dµ   ∂µ     ∂u(Nt )             ∂u(Nt )           ∂µ       n=0
              |                                                                               ∂µ        n=1 p=1
              |                                                                                                          ∂µ
              |               Nt
              |                  "                                                                    s
              |                                                                                                          #
              |                                                    (n−1)                    (n)                      (n)
              |              X        ∂F           (n−1)  T   ∂ r̃             (n) T   ∂ r̃          X     (n) T ∂R
              |                                                                                                      i     ∂u(n−1)
              |            +                −   λ                         −  λ                    −       κi
              |              n=1
              |                    ∂u(n−1)                    ∂u(n−1)                ∂u(n−1) i=1                ∂u(n−1)       ∂µ
              |                                                                                
              |               Nt Xs                             (n)       s                (n)       (n)
              |                       ∂F − λ(n) ∂ r̃                          (n) T ∂Ri  ∂kp
              |              X                           T              X
              |            +              (n)                    (n)
              |                                                       −      κ i          (n)
              |                                                                                          .
              |              n=1 p=1 ∂kp                   ∂kp           i=p          ∂kp           ∂µ
              |                                                                                                                             (D.22)
              |                            (n)              (n)
              | The dual variables, λ            and       κi ,   which have remained arbitrary to this point, are chosen as the
              | solution to the following equations
blank         | 
text          |                                                                T
              |                                                   ∂ r̃ (Nt )           ∂F
              |                                                        (N  )
              |                                                              λ(Nt ) =
              |                                                   ∂u     t            ∂u(Nt )
              |                                  T                         T                                         s              (n) T
              |                    ∂ r̃ (n)   (n)   ∂ r̃ (n−1)   (n−1)     ∂F T X ∂Ri           (n)
              |                             λ     +            λ       =         −             κi                                           (D.23)
              |                   ∂u(n−1)           ∂u(n−1)              ∂u(n−1)   i=1
              |                                                                        ∂u(n−1)
blank         | 
text          |                                                    (n)             T                                     T
              |                                                s
              |                                                X ∂Rj                    (n)        ∂F         ∂ r̃ (n)
              |                                                           (n)
              |                                                                        κj     =     (n)
              |                                                                                           −       (n)
              |                                                                                                              λ(n)
              |                                                j=i   ∂ki                          ∂ki         ∂ki
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. These are the fully discrete adjoint equations corresponding to
              | the primal evolution equations in (D.18) and quantity of interest F . Defining the dual variables as
              |                                                                     dF
              | the solution of the adjoint equations in (D.23), the expression for    in (D.22) reduces to
              |                                                                     dµ
blank         | 
text          |                                                      N                              N     s                    (n)
              |                                                T ∂ r̃ (n)
              |                                         t                     t X
              |                              dF   ∂F   X                    X            T ∂Rp
              |                                 =    −    λ(n)            −         κ(n)
              |                                                                      p         ,                                            (D.24)
              |                              dµ   ∂µ n=0          ∂µ        n=1 p=1
              |                                                                             ∂µ
blank         | 
              | 
text          | which is independent of the state sensitivities. Finally, elimination of the auxiliary variables, r̃ (n)
              |       (n)
              | and Ri , in equations (D.23) and (D.24) through differentiation of their expressions in (D.18) gives
              | rise to the adjoint equations
blank         | 
text          |                            ∂F T
              |              λ(Nt ) =
              |                           ∂u(Nt )
              |                                                           s
              |                                            ∂F T X          ∂r  (n)                      T
              |                                                                                              (n)
              |             λ(n−1) = λ(n) +                      +     ∆tn     u i  , µ, t n−1 + ci ∆t n    κi                              (D.25)
              |                                          ∂u(n−1)   i=1
              |                                                            ∂u
              |                                      T                   s
              |                 (n)        ∂F                            X                    ∂r  (n)                 T
              |                                                                                                           (n)
              |          M T κi       =     (n)
              |                                          + bi λ(n) +           aji ∆tn            uj , µ, tn−1 + cj ∆tn κj
              |                           ∂ki                            j=i
              |                                                                               ∂u
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          248
blank         | 
              | 
              | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s and the expression for gradient reconstruction, independent of
              | state sensitivities,
blank         | 
text          |                                        Nt      s
              |                dF   ∂F        T ∂u0   X       X    (n) T ∂r   (n)
              |                   =    + λ(0)       +     ∆tn     κi        (ui , µ, tn−1 + ci ∆tn ),             (D.26)
              |                dµ   ∂µ          ∂µ    n=1     i=1
              |                                                          ∂µ
blank         | 
text          | specialized to the case of a DIRK temporal discretization. From inspection of (D.26), it is clear
              |                                        ∂u0                                                         dF
              | that the initial condition sensitivity      is the only sensitivity term required to reconstruct      .
              |                                        ∂µ                                                          dµ
              | The presence of this term does not destroy the efficiency of the adjoint method for two reasons:
              |                                         ∂u0 T
              | (a) only matrix-vector products with           are required and (b) the parametrization of the initial
              |                                         ∂µ
              | condition is either known analytically (uniform flow, zero freestream, independent of µ, etc) or is the
              | solution of some nonlinear system of equations (most likely the steady-state equations). In the first
              |            T ∂u0
              | case, λ(0)       can be computed analytically once λ(0) is known. The next section details efficient
              |              ∂µ
              |                     T ∂u0
              | computation of λ(0)        using the adjoint method of the steady-state problem.
              |                       ∂µ
blank         | 
title         | D.2.2      Parametrization of Initial Condition
text          | Suppose the initial condition u0 (µ) is defined as the solution of the nonlinear system of equations—
              | whose Jacobian is invertible at u0 (µ)—which is most likely the fully discrete steady-state form of
              | the governing equations
              |                                                R(u0 (µ), µ) = 0.                                  (D.27)
blank         | 
text          | Differentiating with respect to the parameter µ leads to the expansion
blank         | 
text          |                                        dR   ∂R   ∂R ∂u0
              |                                           =    +        = 0,                                      (D.28)
              |                                        dµ   ∂µ   ∂u0 ∂µ
blank         | 
text          | where arguments have been dropped for brevity. Assuming the Jacobian matrix is invertible, mul-
              | tiply the preceding equation by the λ(0) and rearrange to obtain
              |                                                     "           #T
              |                                       (0) T   ∂u0     ∂R −T (0)    ∂R
              |                                  −λ               =        λ          .                           (D.29)
              |                                               ∂µ      ∂u0          ∂µ
blank         | 
text          |                             ∂u0
              |                              T
              | This reveals the term λ(0)       can be computed at the cost of one linear system solve of the form
              |                             ∂µ
              |      T
              | ∂R                                       ∂R
              |        v = λ(0) and an inner product v T     . The only operation whose cost scales with the size
              | ∂u0                                      ∂µ
              |                           ∂R
              | of µ is the evaluation of       and subsequent inner product. Given this exposition on the fully
              |                            ∂µ
              |                                                                                              T ∂u0
              | discrete, time-dependent adjoint method and the discrete adjoint method for computing λ(0)         ,
              |                                                                                                ∂µ
              | a discussion is provided detailing the advantages of the fully discrete framework when computing
              | gradients of output quantities before discussing implementation details in Section D.2.4.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                   249
blank         | 
              | 
              | 
title         | D.2.3      Benefits of Fully Discrete Framework
text          | In the context of optimization, the fully discrete adjoint method is advantageous compared to the
              | continuous or semi-discrete version as it is guaranteed that the resulting derivatives will be consistent
              | with the quantity of interest, F . This emanates from the fact that in the fully discrete setting, the
              | discretization errors are also differentiated. This property is practically relevant as convergence
              | guarantees and convergence rates of many black-box optimizers are heavily dependent on consistent
              | gradients of optimization functionals.
              |    Additionally, when Runge-Kutta schemes are chosen for the temporal discretization, the fully
              | discrete framework is particularly advantageous since the stages are rarely invariant with respect to
              | the direction of time, that is to say,
blank         | 
text          |                        6 ∃i, j ∈ {1, . . . , s}   such that tn−1 + ci ∆tn = tn − cj ∆tn ,                 (D.30)
blank         | 
text          | where c is from the Butcher tableau. Temporal invariance of an Runge-Kutta scheme, as defined in
              | (D.30) is significant when computing adjoint variables. During the primal solve, u will be computed
              | at tn for n = 1, . . . , N and its stage values at tn−1 + ci ∆tn for n = 1, . . . , N and i = 1, . . . , s. If the
              | same RK scheme is applied to integrate the semi-discrete adjoint equations backward in time, the
              | primal solution will be required at tn − ci ∆tn for n = 1, . . . , N and i = 1, . . . , s. Due to condition
              | (D.30), the solution to the primal problem was not computed during the forward solve. Obtaining the
              | primal solution at this time requires interpolation, which complicates the implementation, degrades
              | the accuracy of the computed adjoint variables, and destroys discrete consistency of the computed
              | gradients. This issue does not arise in the fully discrete setting as only terms computed during the
              | primal solve appear in the adjoint equations, by construction.
              |    The next section is devoted to detailing an efficient and modular implementation of the fully
              | discrete adjoint method on deforming domains.
blank         | 
              | 
title         | D.2.4      Implementation
text          | Implementation of the fully discrete adjoint method introduced in Section D.2 relies on the compu-
              | tation of the following terms from the spatial discretization
blank         | 
text          |                                                   ∂r ∂r T ∂r        ∂fh ∂fh
              |                                        M , r,       ,    ,   , fh ,    ,    .                             (D.31)
              |                                                   ∂u ∂u ∂µ          ∂u ∂µ
blank         | 
text          | Here, M is the mass matrix of the semi-discrete conservation law, and r is the spatial residual
              |                          ∂r                    ∂r
              | vector with derivatives      (Jacobian) and       . As in the previous section, fh is the discretization
              |                          ∂u                    ∂µ
              |                                                                              ∂fh        ∂fh
              | of the spatial integral of the output quantity of interest with derivatives       and       . The mass
              |                                                                               ∂u        ∂µ
              | matrix, spatial flux, Jacobian of spatial flux, and output quantity are standard terms required by an
              | implicit solver and will not be considered further. The Jacobian transpose is explicitly mentioned as
              | additional implementational effort is required when performing parallel matrix transposition. The
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        250
blank         | 
              | 
              | 
text          | derivatives with respect to µ are rarely required outside adjoint method computations and will be
              | considered further in subsequent sections. As indicated in Section D.1.2, all relevant derivatives of
              | the mass matrix are zero since it is independent of time, parameter, and state variable, which is an
              | artifact of the transformation to a fixed reference domain.
              |       The parallel implementation of all semi-discrete quantities in (D.31) is performed using domain
              | decomposition, where each processor contains a subset of the elements in the mesh, including a halo
              | of elements to be communicated with neighbors [154]. Linear systems of the form
blank         | 
text          |                                           ∂r                       ∂r T
              |                                              x=b                        x=b
              |                                           ∂u                       ∂u
blank         | 
text          | are solved in parallel using a GMRES solver with a block Incomplete-LU (ILU) preconditioner.
              |       Given the availability of all terms in (D.31), the solution of the primal problem and integration
              | of the output quantity F is given in Algorithm 19. The solution of the corresponding fully discrete
              | adjoint equation, and reconstruction of the gradient of F , is given in Algorithm 20.
blank         | 
text          | Algorithm 19 Primal Solution: Functional Evaluation
              | Input: Initial condition, u(0) ; parameter configuration, µ
              |                                                     (N )                              (n)
              | Output: Integrated output quantity, F = Fh t , and primal state quantities, u(n) and ki for
              |     n = 1, . . . , Nt and i = 1, . . . , s
              |                     (0)
              |  1: Initialize: Fh = 0
              |  2: for n = 1, . . . , Nt do
              |  3:   for i = 1, . . . , s do
              |                               (n)
              |  4:      Solve (D.14) for ki
blank         |                                                                          
text          |                                              (n)         (n)
              |                                           M ki = ∆tr ui , µ, tn−1 + ci ∆t
blank         | 
text          |               (n)                Pi          (n)
              |       where ui      = u(n−1) +    j=1   aij kj
              |                     (n)
              |  5:       Write ki to disk
              |  6:     end for
              |  7:     Update u according to (D.14)
              |                                                                     s
              |                                                                                (n)
              |                                                                     X
              |                                                  u(n) = u(n−1) +           bi ki
              |                                                                      i=1
blank         | 
text          |  8:     Update Fh according to (2.45)
              |                                                         s                               
              |                                   (n)       (n−1)                   (n)
              |                                                         X
              |                                  Fh     = Fh        +         bi f ui , µ, tn−1 + ci ∆tn
              |                                                         i=1
blank         | 
title         |  9:     Write u(n) to disk
              | 10:   end for
blank         | 
              | 
text          |       A well-documented implementational issue corresponding to the unsteady adjoint method per-
              | tains to storage and I/O demands. The adjoint equations are solved backward in time and require
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                 251
blank         | 
              | 
              | 
              | 
text          | Algorithm 20 Dual Solution: Gradient Evaluation
              |                                                      (n)
              | Input: Primal state quantities, u(n) and ki for n = 1, . . . , Nt and i = 1, . . . , s; initial condition
              |                     ∂u(0)
              |     sensitivity,          ; parameter configuration, µ
              |                      ∂µ
              |                                                        dF                                          (n)
              | Output: Gradient of integrated output quantity,           , and dual state quantities, λ(n) and κi for
              |                                                        dµ
              |     n = 1, . . . , Nt and i = 1, . . . , s
              |  1: Read primal solution u(Nt ) from disk
              |       (N )        ∂F T
              |  2: λ t =
              |                ∂u(Nt )
              |  3: Initial gradient of F with partial derivative and initial condition sensitivity
blank         | 
text          |                                                  dF   ∂F        T ∂u0
              |                                                     =    + λ(0)
              |                                                  dµ   ∂µ          ∂µ
              |  4: for n = Nt , . . . , 1 do
              |  5:   Read primal solution u(n−1) from disk
              |  6:   for i = s, . . . , 1 do
              |                                   (n)
              |  7:     Read primal solution ki from disk
              |                               (n)
              |  8:     Solve (D.25) for κi
              |                                         T                 s
              |                       (n)        ∂F                       X               ∂r (n)                      (n)
              |                 M T κi      =               + bi λ(n) +         aji ∆tn     (u , µ, tn−1 + cj ∆tn )T κj
              |                                   (n)
              |                                 ∂ki                       j=i
              |                                                                           ∂u j
blank         | 
text          |                   dF
              |  9:      Update      according to (D.26)
              |                   dµ
              |                                 dF   dF        (n) T ∂r   (n)
              |                                    =    + ∆tn κi        (u , µ, tn−1 + ci ∆tn )
              |                                 dµ   dµ              ∂µ i
              | 10:    end for
              | 11:    Update λ according to (D.25)
              |                                                                 s
              |                                               ∂F T X          ∂r (n)                      (n)
              |                    λ(n−1) = λ(n) +                  +     ∆tn    (ui , µ, ti + ci ∆tn )T κi
              |                                             ∂u(n−1)   i=1
              |                                                               ∂u
blank         | 
meta          | 12:   end for
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          252
blank         | 
              | 
              | 
text          | the solution of the primal problem at each of the corresponding steps/stages. Therefore, the adjoint
              | computations cannot begin until all primal states have been computed. Additionally, this implies
              | all primal states must be stored since they will be required in reverse order during the adjoint
              | computation. For most problems, storing all primal states in memory will be infeasible, requiring
              | disk I/O, which must be performed in parallel to ensure parallel scaling is maintained. There have
              | been a number of strategies to minimize the required I/O operations, such as local-in-time adjoint
              | strategies [206] and checkpointing [40, 90, 95]. For the DG-ALE method in this work, the cost of I/O
              | was not significant compared to the cost of assembly and solving the linearized system of equations.
              |    In this work, the 3DG software [150] was used for the high-order DG-ALE scheme. The tem-
              | poral discretization and unsteady adjoint method were implemented in the Model Order Reduction
              | Testbed (MORTestbed) [209, 210] code-base, which was used to wrap 3DG such that all data struc-
              | tures, and thus all parallel capabilities, were inherited.
blank         | 
title         | Partial Derivatives of Residuals and Output Quantities
blank         | 
text          | This section details computation of partial derivatives of the residual, r, and the output quantity, fh ,
              | with respect to the parameter µ. The DG-ALE discretizations of Section D.1.2, with and without
              | GCL augmentation, are considered separately as the implicit dependence of ḡ on µ requires special
              | treatment.
blank         | 
title         | Without GCL Augmentation
text          | When the GCL augmentation is not considered, the dependence of r and fh on the parameter
              | µ is solely due to the domain parametrization. Therefore, the following expansion of the partial
              | derivatives with respect to µ is exploited
blank         | 
text          |                       ∂r   ∂r ∂x   ∂r ∂ ẋ              ∂fh   ∂fh ∂x ∂fh ∂ ẋ
              |                          =       +                          =       +                             (D.32)
              |                       ∂µ   ∂x ∂µ ∂ ẋ ∂µ                ∂µ    ∂x ∂µ   ∂ ẋ ∂µ
blank         | 
text          |         ∂x     ∂ ẋ
              | where      and      are determined solely from the domain parametrization and the terms
              |         ∂µ     ∂µ
blank         | 
text          |                                            ∂r ∂r ∂fh ∂fh
              |                                              ,    ,  ,                                            (D.33)
              |                                            ∂x ∂ ẋ ∂x ∂ ẋ
blank         | 
text          | are determined from the form of the governing equations and spatial discretization outlined in
              | Section D.1. From the expressions in (D.32), the terms in (D.33) are not explicitly required in
              |                                                 ∂x     ∂ ẋ
              | matrix form, rather matrix-vector products with    and      from Section D.2.4 are required.
              |                                                 ∂µ     ∂µ
blank         | 
text          | With GCL Augmentation
              | For the DG-ALE scheme with GCL augmentation, the dependence of r and f on the parameter
              | µ arises from two sources, the domain parametrization and the implicit dependence of ḡ on µ.
              | Therefore, the chain rule expansions in (D.32) must include an additional term to account for the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                           253
blank         | 
              | 
              | 
text          | dependence of ḡ on µ
blank         | 
text          |          ∂r   ∂r ∂x   ∂r ∂ ẋ   ∂r ∂ ḡ                        ∂f   ∂f ∂x   ∂f ∂ ẋ ∂f ∂ ḡ
              |             =       +         +                                   =       +        +        .      (D.34)
              |          ∂µ   ∂x ∂µ ∂ ẋ ∂µ ∂ ḡ ∂µ                            ∂µ   ∂x ∂µ ∂ ẋ ∂µ ∂ ḡ ∂µ
blank         | 
text          |                                                        ∂x     ∂ ẋ
              | Similar to the previous section, the terms                and      are determined solely from the domain
              |                                                        ∂µ     ∂µ
              | parametrization and
              |                                         ∂r ∂r ∂r ∂f ∂f ∂f
              |                                           ,    ,    ,  ,    ,                                      (D.35)
              |                                         ∂x ∂ ẋ ∂ ḡ ∂x ∂ ẋ ∂ ḡ
              | are determined from the form of the governing equations and spatial discretization in Section D.1.
              |                          ∂ ḡ
              | The only remaining term       is defined as the solution of the following ODE
              |                          ∂µ
blank         |                                                   
text          |                                    ∂        ∂ ḡ           ∂rḡ   ∂rḡ ∂ ḡ   ∂rḡ
              |                                Mḡ                     =        +           =      ,               (D.36)
              |                                    ∂t       ∂µ             ∂µ     ∂ ḡ ∂µ     ∂µ
blank         | 
text          | obtained by direct differentiation of (D.11). The last equality uses the fact that rḡ is independent of
              | ḡ, which can be deduced from examination of the governing equation for ḡ (D.5). Equation (D.36)
              | is discretized with the same DIRK scheme used for the temporal discretization of the state equation.
blank         | 
text          | Remark. The special treatment of ḡ detailed in this section, including integration of the sensitivity
              | equations (D.36), can be avoided by considering the ODEs in (D.11) directly without leveraging the
              | fact that the ḡ equation is independent of uX̄ . This implies the state vector will contain an additional
              | unknown for ḡ for each DG node. This increases the cost of a primal and dual solve, but simplifies
              | the adjoint derivation and implementation.
blank         | 
title         | Time-Dependent, Parametrized Domain Deformation
blank         | 
text          | A crucial component of the fully discrete adjoint method on deforming domains is a time-dependent
              | parametrization of the domain, amenable to parallel implementation. A parallel implementation
              | is required as domain deformation will involve operations on the entire computational mesh and
              | will be queried at every stage of each time step of both the primal and dual solves, according to
              | Algorithms 19 and 20. In this work, the domain parametrization is required to be sufficiently general
              | to handle shape deformation, as well as kinematic motion. Additionally, the domain deformation
              | must be sufficiently smooth to ensure sufficient regularity of the transformed solution, and the
              | spatial and temporal derivatives must be analytically available for fast, accurate computation of the
              | deformation gradient, G, and velocity, vX , of the mapping, G.
              |    The domain deformation will be defined by the superposition of a rigid body motion and a
              | spatially varying deformation. To avoid large mesh velocities at the far-field, which could arise from
              | rigid rotations of the body, the blending maps of [152] are used. First, define a spatial configuration
              | consisting of a rigid body motion (Q(µ, t), v(µ, t)) and deformation (ϕ(X, µ, t)) to the reference
              | domain
              |                                 X 0 = Q(µ, t)X + v(µ, t) + ϕ(X, µ, t),                             (D.37)
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                       254
blank         | 
              | 
              | 
text          | which completely defines the physical motion of the body. This physical configuration is blended
              | with the reference configuration according to
blank         | 
text          |                                  x = (1 − b(d(X)))X 0 + b(d(X))X                               (D.38)
blank         | 
text          | where d(X) = kX − X0 k − R0 is the signed distance from the origin X0 to the circle of radius R0
              | centered at X0 and                       
              |                                          
              |                                          
              |                                           0,      if s < 0
              |                                          
              |                                    b(s) = 1,       if s > R1                                   (D.39)
              |                                          
              |                                          
              |                                          
              |                                          r(s/R ), otherwise
              |                                                1
blank         | 
              | 
text          | where r(s) = 3s2 − 2x3 for a cubic blending and r(s) = 10s3 − 15s4 + 6s5 for a quintic blending.
              | Spatial blending of this form ensures the desired physical motion of the body, X 0 is exactly achieved
              | within a radius R0 of the origin. Further, there is no deformation outside a radius R0 + R1 of the
              | origin. In the annulus about the origin with inner radius R0 and outer radius R0 + R1 , the spatial
              | configuration is blended smoothly between these two spatial configurations.
              |    The specific form of Q(µ, t), v(µ, t), and ϕ(X, µ, t) is problem-specific and will be deferred to
              | Sections D.2.5, D.2.6, D.4.4, D.4.5. Assuming these terms are known analytically, the specific form
              |         ∂x               ∂x ∂x         ∂ ẋ
              | of G =      , vX = ẋ =     ,    , and      can be easily computed.
              |         ∂X               ∂t ∂µ         ∂µ
              |    In the next two sections, the high-order numerical discretization of a system of conservation laws
              | and corresponding adjoint method is applied to the isentropic compressible Navier-Stokes equa-
              | tions (2.24)-(2.25) to solve optimal control and shape optimization problems using gradient-based
              | optimization techniques. The DG-ALE scheme introduced in Section D.1 is used for the spatial dis-
              | cretization of the system of conservation laws with polynomial order p = 3 and a diagonally implicit
              | Runge-Kutta scheme for the temporal discretization. The DG-ALE scheme uses the Roe flux [169]
              | for the inviscid numerical flux and the Compact DG flux [150] for the viscous numerical flux. The
              | Butcher tableau for the three-stage, third-order DIRK scheme considered in this work is given in
              | Table D.1. The instantaneous quantities of interest for a body, defined by the surface Γ, take the
blank         | 
text          |            Table D.1: Butcher Tableau for 3-stage, 3rd order DIRK scheme [3]
              |                                            2                2
              |            α = 0.435866521508459, γ = − 6α −16α+1
              |                                               4    , ω = 6α −20α+5
              |                                                               4    .
blank         | 
text          |                                       α          α
              |                                        1+α    1+α
              |                                         2      2  −α     α
              |                                       1          γ       ω   α
              |                                                  γ       ω   α
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                           255
blank         | 
              | 
              | 
text          | following form
              |                          Z                                                        Z
              |    Fx (U , µ, t) =            f (U , µ, t) · e1 dS              Fy (U , µ, t) =        f (U , µ, t) · e2 dS
              |                          ZΓ                                                       ZΓ
              |     P(U , µ, t) =             f (U , µ, t) · ẋ dS              Px (U , µ, t) =   ẋf (U , µ, t) · e1 dS                           (D.40)
              |                          ZΓ                                                      Z Γ
blank         | 
text          |    Py (U , µ, t) =            ẏf (U , µ, t) · e2 dS            Pθ (U , µ, t) = − θ̇f (U , µ, t) × (x − x0 ) dS
              |                           Γ                                                            Γ
blank         | 
text          | where f ∈ Rnsd is the force imparted by the fluid on the body, ei is the ith canonical basis vector
              | in Rnsd , x and ẋ are the position and velocity of a point on the surface Γ, and x, y, θ, ẋ, ẏ, θ̇ define
              | the motion of the reference point, x0 (the 1/3-chord of the airfoil, in this case); see Figure D.2. The
              | Fx and Fy terms correspond to the total x- and y-directed forces on the body and P is the total
              | power exerted on the body by the fluid. The total power P is broken into its translational, Px and
              | Py , and rotational, Pθ , components. For a 2D rigid body motion, an additive relationship among
              | these terms holds
              |                                        P(U , µ, t) = Px (U , µ, t) + Py (U , µ, t) + Pθ (U , µ, t).                                (D.41)
blank         | 
text          | The negative sign is included in the definition of Pθ due to the clockwise definition of θ in Fig-
              | ure D.2. In the remainder of this document, a superscript h will be used to denote the high-order
              | DG approximation to these spatial integrals that constitute the instantaneous quantities of interest,
              | e.g., P h (u, µ, t) is the high-order approximation of P(U , µ, t), where u is the semi-discrete approxi-
              | mation of U . Temporal integration of the instantaneous quantities of interest leads to the integrated
              | quantities of interest
              |                 Z    T   Z                                                            Z    T   Z
              |  Jx (U , µ) =                    f (U , µ, t) · e1 dS dt            Jy (U , µ) =                       f (U , µ, t) · e2 dS dt
              |                  0           Γ                                                         0           Γ
              |                 Z    T   Z                                                            Z    T   Z
              |  W(U , µ) =                      f (U , µ, t) · ẋ dS dt           Wx (U , µ) =                        ẋf (U , µ, t) · e1 dS dt
              |                  0           Γ                                                         0            Γ
              |                 Z    T   Z                                                                Z        T Z
              | Wy (U , µ) =                     ẏf (U , µ, t) · e2 dS dt         Wθ (U , µ) = −                          θ̇f (U , µ, t) × (x − x0 ) dS dt
              |                  0           Γ                                                                 0       Γ
              |                                                                                                                                    (D.42)
              | which will be used as optimization functionals in subsequent sections. The terms Jx and Jy are
              | the x- and y-directed impulse the fluid exerts on the airfoil, respectively, W is the total work done
              | on the airfoil by the fluid, and Wx , Wy , and Wθ are the translational and rotational components
              | of the total work. The fully discrete, high-order approximation of the integrated quantities of
              | interest (DG in space, DIRK in time) will be denoted with the corresponding Roman symbol, e.g.,
              |                                  (n)         (n)
              | W (u(0) , . . . , u(Nt ) , k1 , . . . , ks , µ) is the fully discrete approximation of W(U , µ).
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          256
blank         | 
              | 
              | 
title         | D.2.5     Numerical Experiment: Energetically Optimal Trajectory of 2D
              |           Airfoil in Compressible, Viscous Flow
text          | In this section, the high-order, time-dependent PDE-constrained optimization framework introduced
              | in this document is applied to find the energetically optimal trajectory of a 2D NACA0012 airfoil with
              | chord length l = 1 and zero-thickness trailing edge. The governing equations are the 2D compressible,
              | isentropic Navier-Stokes equations. The mission of the airfoil is to move a distance of −1.5 units
blank         | 
text          |                                                            l
              |                                                     l/3
blank         | 
text          |                                      θ(t)
blank         | 
text          |                                             x(t)
              |                                                    y(t)
blank         | 
              | 
text          |                                      Figure D.2: Airfoil kinematics
blank         | 
text          | horizontally and 1.5 units vertically in T = 4 units of time, with the restriction that θ(0) = θ(T ) = 0,
              | i.e., the angle of attack at the initial and final time is zero. Additionally, to ensure smoothness of
              | the motion and avoid non-physical transients, ẋ(0) = ẋ(T ) = ẏ(0) = ẏ(T ) = θ̇(0) = θ̇(T ) = 0 are
              | enforced. The goal is to determine the trajectory x(t), y(t), θ(t) of the airfoil that minimizes the
              | total energy required to complete the mission, i.e.,
blank         | 
text          |                           minimize     W(U , µ)
              |                             U, µ
blank         | 
text          |                           subject to x(0) = ẋ(0) = ẋ(T ) = 0, x(T ) = −1.5
              |                                        y(0) = ẏ(0) = ẏ(T ) = 0, y(T ) = 1.5                     (D.43)
              |                                        θ(0) = θ(T ) = θ̇(0) = θ̇(T ) = 0
              |                                        ∂U
              |                                           + ∇ · F (U , ∇U ) = 0       in v(µ, t).
              |                                        ∂t
blank         | 
text          | The trajectory of the airfoil—x(t), y(t), and θ(t)—is discretized via clamped cubic splines with
              | mx + 1, my + 1, and mθ + 1 knots, respectively. The knots are uniformly spaced between 0 and
              | T in the t-dimension and the knot values are optimization parameters. Table D.2 summarizes
              | two parametrizations considered in this section: (PI) the translational degrees of freedom—x(t)
              | and y(t))—are frozen at their nominal value in Figure D.4 and the rotational degree of freedom—
              | θ(t)—is parametrized with a mθ + 1-knot clamped cubic spline and (PII) all rigid body modes are
              | parametrized with clamped cubic splines. The 7 IDs in Table D.2 correspond to levels of refinement
              | of the given parametrization with ID = 1 being the coarsest parametrization and ID = 7 the finest.
              | With this parametrization of the airfoil kinematics, spatial and temporal discretization with the
              | high-order scheme of Section D.1 leads to the fully discrete version of the optimization problem in
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                              257
blank         | 
              | 
              | 
text          | Table D.2: Summary of parametrizations considered in Section D.2.5. The number of clamped cubic
              | spline knots used to discretize x(t), y(t), and θ(t) are mx + 1, my + 1, and mθ , respectively. PI
              | freezes the rigid body translation (mx = my = 0) and optimizes over only the rotation (mθ 6= 0).
              | PII optimizes over all rigid body degrees of freedom (mx = my = mθ 6= 0).
blank         | 
text          |                                                 PI                             PII
              |                           ID    mx        my         mθ   Nµ     mx       my          mθ    Nµ
              |                           1      0         0      2        3       2       2           2     9
              |                           2      0         0      6        7       6       6           6     21
              |                           3      0         0     10       11      10      10          10     33
              |                           4      0         0     15       16      15      15          15     48
              |                           5      0         0     25       26      25      25          25     78
              |                           6      0         0     50       51      50      50          50    153
              |                           7      0         0     100      101    100      100         100   303
blank         | 
              | 
text          | (D.43)
              |                                                                              (1)
              |                           minimize              W (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ)
              |                     (0)        (Nt )
              |                    u , ..., u          ∈RNu ,
              |                     (1)
              |                    k1 , ..., ks(Nt ) ∈RNu ,
              |                           µ∈RNµ
blank         | 
text          |                    subject to                   x(0) = ẋ(0) = ẋ(T ) = 0, x(T ) = −1.5
              |                                                 y(0) = ẏ(0) = ẏ(T ) = 0, y(T ) = 1.5
              |                                                 θ(0) = θ(T ) = θ̇(0) = θ̇(T ) = 0                      (D.44)
blank         | 
text          |                                                 u(0) = u0
              |                                                                     s
              |                                                                                 (n)
              |                                                                     X
              |                                                 u(n) = u(n−1) +           bi ki
              |                                                                     i=1
blank         |                                                                                       
text          |                                                    (n)             (n)
              |                                                 M ki      = ∆tn r ui , µ, tn−1 + ci ∆tn .
blank         | 
text          |    Before considering the optimization problem (D.44), the proposed adjoint method for comput-
              | ing gradients of quantities of interest on the manifold of fully discrete, high-order solutions of the
              | conservation law (D.14) is verified against a fourth-order finite difference approximation. The finite
              | difference approximation to gradients on the aforementioned manifold requires finding the solution
              | of the fully-discretized governing equations at perturbations about the nominal parameter config-
              | uration in Figure D.4. To mitigate round-offs errors as much as possible in the finite difference
              | computation, the number of time steps was reduced to 10 and only half of a period was simulated.
              | Figure D.3 shows the relative error between the gradients computed via the adjoint method and this
              | finite difference approximation for a sweep of finite difference intervals, τ . A relative error on the
              | order of 10−10 is observed for a finite difference step of τ = 10−4 . As expected, the error starts to
              | increase after τ drops too small due to the trade-off between finite difference accuracy and roundoff
              | error.
              |    With this verification of the adjoint-based gradients, attention is turned to the optimization prob-
              | lem in (D.44). The optimization solver used in this section is L-BFGS-B [215], a bound-constrained,
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                    258
blank         | 
              | 
              | 
text          |                            10−5
blank         | 
              | 
              | 
              | 
text          |                   ∆W
              |                   ∆µ
              |                            10−7
blank         | 
              | 
              | 
              | 
text          |                   /
              |                   ∆W
              |                   ∆µ
              |                            10−9
blank         | 
text          |                   −
              |                   dW
              |                   dµ
              |                        10−11
              |                                      10−9           10−7              10−5           10−3           10−1
              |                                                                        τ
blank         | 
text          | Figure D.3: Verification of adjoint-based gradient with fourth-order centered finite difference approx-
              | imation, for a range of finite intervals, τ , for the total work W —the objective function in (D.44)—for
              | parametrization PII (Table D.2). The computed gradient match the finite difference approximation
              | to about 10 digits of accuracy before round-off errors degrade the accuracy.
blank         | 
              | 
text          | limited-memory BFGS algorithm. Figure D.4 contains the initial guess for the optimization prob-
              | lem in (D.44) as well as its solution under both parametrization, PI and PII, at the finest level of
              | refinement (ID = 7). The initial guess for the optimization problem is a pure translational motion
              | with θ(t) = 0. The solution under parametrization PI freezes the translational motion at its nominal
              | value and incorporates rotational motion. The solution under parametrization PII increases the am-
              | plitude of the rotation, flattens the trajectory of x(t), and incorporates an overshoot in y(t) before
              | settling to the required location, as compared to the optimal solution corresponding to PI.
blank         | 
text          |           0                                                            1.5
blank         | 
text          |        −0.5                                                             1
              | x(t)
blank         | 
              | 
              | 
              | 
text          |                                                                y(t)
blank         | 
              | 
              | 
              | 
text          |         −1                                                             0.5
blank         | 
text          |        −1.5                                                             0
              |               0        1         2          3         4                      0              1         2    3   4
              |                                time                                                                 time
blank         | 
text          |                                         0
              |                               θ(t)
blank         | 
              | 
              | 
              | 
text          |                                      −0.5
blank         | 
              | 
              | 
text          |                                                 0          1            2        3              4
              |                                                                       time
blank         | 
              | 
text          | Figure D.4: Trajectories of x(t), y(t), and θ(t) at initial guess (   ), solution of (D.44) under
              | parametrization PI (    ), and solution of (D.44) under parametrization PII (    ) for ID = 7.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          259
blank         | 
              | 
              | 
text          |    The instantaneous quantities of interest for the nominal trajectory and solution of (D.44) under
              | parametrizations PI and PII are included in Figure D.5. It is clear that the optimal solution under
              | both parametrizations result in a time history of the total power that is uniformly closer to 0 than
              | that at the nominal trajectory, which is expected since W is the objective function. With the
              | exception of the edges of the time interval, the total power time history for the optimal solution
              | under parametrization PII is uniformly closer to 0 than that of PI. The same observation holds for
              | the power due to the translational motion, Pxh and Pyh . Whereas the total power corresponding
              | to the nominal trajectory is due solely to the translational motion (since there is no rotation), the
              | optimal solutions exchange large amounts of translational power for a small amount of rotational
              | power. These observations can also be verified in Table D.3 which summarizes the optimal values
              | of the integrated quantities of interest.
blank         | 
text          |        0.1
              |                                                                    1
              |          0
              |  Fxh
blank         | 
              | 
              | 
              | 
text          |                                                             Fyh
              |                                                                    0
              |       −0.1
              |                                                                   −1
blank         | 
              | 
              | 
              | 
text          |          0                                                      0.05
              |  Ph
blank         | 
              | 
              | 
              | 
text          |                                                          Pxh
blank         | 
              | 
              | 
              | 
text          |       −0.2
              |                                                                    0
              |       −0.4
              |                                                                −0.05
blank         | 
              | 
              | 
text          |          0                                                      0.05
blank         | 
text          |       −0.2                                                         0
              | Pyh
blank         | 
              | 
              | 
              | 
text          |                                                          Pθh
blank         | 
              | 
              | 
              | 
text          |                                                                −0.05
              |       −0.4
              |                                                                −0.1
              |               0       1        2       3        4                      0       1        2       3           4
              |                              time                                                     time
blank         | 
              | 
text          | Figure D.5: Time history of instantaneous quantities of interest (x-directed force – Fxh (u, µ, t),
              | y-directed force – Fyh (u, µ, t), total power – P h (u, µ, t), x-translational power – Pxh (u, µ, t), y-
              | translational power – Pyh (u, µ, t), rotational power – Pθh (u, µ, t)) at initial guess ( ), solution of
              | (D.44) under parametrization PI (        ), and solution of (D.44) under parametrization PII (      ) for
              | ID = 7.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                       260
blank         | 
              | 
              | 
text          |     The convergence of the total work, i.e., the objective function of the optimization problem, with
              | iterations of the optimization solver is summarized in Figure D.6 (left). Both parametrizations are
              | included and iterations are agglomerated over all IDs. The first iteration corresponds to a steepest
              | descent step, which causes an adverse jump in the objective value. The following iterations make
              | rapid progress toward the optimal solution, which is slowed as convergence is approached. The solver
              | requires additional iterations to converge the solution corresponding to parametrization PII, which
              | is expected due to the larger parameter space.
              |     Next, convergence of the total work as the parameter space is refined is considered in Figure D.6
              | (right) and Table D.3. This implies the optimal trajectory among all twice continuously differentiable
              | functions is being approached. For both parametrizations, the optimal value of the total work agrees
              | to 3 digits between IDs 6 and 7 (roughly a factor of 2 difference in dimension of parameter spaces)
              | and 2 digits between IDs 3 and 7 (roughly a factor of 10 difference in dimension of parameter spaces).
blank         | 
text          |         0
blank         | 
text          |     −0.5
              |                                                            −0.15
              | W
blank         | 
              | 
              | 
              | 
text          |                                                        W
              |       −1
              |                                                            −0.2
              |     −1.5
              |             0       20       40      60       80                           101              102
              |                          iteration                                                 Nµ
blank         | 
              | 
text          | Figure D.6: Left: Convergence of total work W with optimization iteration for parametrization PI
              | (    ) and PII (   ) for ID = 7. Both optimization problems converge to a motion with significantly
              | lower required total work; PII finds a better motion than PI (in terms of total work) due to the
              | enlarged search space, at the cost of additional iterations. Each optimization iteration requires a
              | primal flow computation—to evaluate the quantities of interest—and its corresponding adjoint—
              | to evaluate the gradient of the quantity of interest. Right: Convergence of optimal value of total
              | work W as parameter space is refined for parametrization PI (       ) and PII (     ). This implies
              | convergence to an optimal, smooth trajectory that is not polluted by its discrete parametrization.
blank         | 
text          |     The motion of the airfoil and vorticity of the surrounding flow are shown in Figure D.7 (nominal
              | trajectory), Figure D.8 (optimal solution under parametrization PI), and Figure D.9 (optimal solu-
              | tion under parametrization PII). The flow corresponding to the nominal configuration experiences
              | flow separation and vortex shedding, which results in the relatively large amount of total energy to
              | complete the mission. Fixing the translational motion and optimizing over the rotation (PI) dra-
              | matically reduces the amount of shedding and consequently reduces the amount of work required.
              | Optimizing the entire rigid body motion (PII) further reduces the shedding and required work.
              | Table D.3: Table summarizing integrated quantities of interest at optimal solution of (D.44) for each parametrization (PI, PII) for each level
              | of refinement. The total work monotonically increases as Nµ increases for a given parametrization, which is expected due to the nested search
              | spaces. For a fixed ID, the optimal total work for parametrization PII is larger than that for PI since the search space for PI is a subset of
              | that of PII. The other integrated quantities are included for completeness, but do not exhibit trends (except for converging to a fixed value
              | as Nµ increases) since they were not included in the optimization problem.
blank         | 
text          |                                                                         PI
              |                    ID         1             2              3             4             5             6             7
              |                    W     -2.1951e-01   -1.5881e-01   -1.5358e-01    -1.5128e-01   -1.5026e-01   -1.4950e-01   -1.4924e-01
              |                    Wx    8.1329e-02     5.6090e-02   4.9543e-02     4.5924e-02    4.5085e-02    4.4712e-02    4.4707e-02
              |                    Wy    -2.3460e-01   -1.8153e-01   -1.7122e-01    -1.6544e-01   -1.6374e-01   -1.6298e-01   -1.6294e-01
              |                    Wθ    6.6234e-02     3.3370e-02   3.1906e-02     3.1768e-02    3.1604e-02    3.1223e-02    3.1010e-02
              |                    Fx    -1.9234e-01   -1.3123e-01   -1.1886e-01    -1.1136e-01   -1.0912e-01   -1.0810e-01   -1.0800e-01
              |                    Fy    -5.1539e-01   -3.1711e-01   -3.1816e-01    -3.1877e-01   -3.2551e-01   -3.2959e-01   -3.3063e-01
              |                                                                         PII
              |                    ID         1             2              3             4             5             6             7
              |                    W     -1.7357e-01   -1.2095e-01   -1.1733e-01    -1.1629e-01   -1.1603e-01   -1.1557e-01   -1.1502e-01
              |                    Wx    9.6487e-03    -1.4123e-02   -1.4328e-02    -1.4967e-02   -1.5021e-02   -1.5061e-02   -1.5027e-02
              |                    Wy    -1.1041e-01   -6.2238e-02   -6.1036e-02    -6.0425e-02   -6.0032e-02   -5.9489e-02   -5.9245e-02
              |                    Wθ    7.2807e-02     4.4585e-02   4.1963e-02     4.0895e-02    4.0980e-02    4.1023e-02    4.0749e-02
              |                    Fx    -4.1265e-02    2.8091e-02   2.7677e-02     2.9596e-02    2.9848e-02    3.0231e-02    3.0221e-02
              |                    Fy    -3.2231e-01    -1.064e-01   -1.0806e-01    -1.0890e-01   -1.1343e-01   -1.1626e-01   -1.1764e-01
meta          |                                                                                                                                                  APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION
              |                                                                                                                                                  261
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                     262
blank         | 
              | 
              | 
              | 
text          | Figure D.7: Flow vorticity around airfoil undergoing motion corresponding to initial guess for opti-
              | mization, i.e., pure heaving (  ). Flow separation off leading edge implies a large amount of work
              | required to complete mission. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0.
blank         | 
              | 
              | 
              | 
text          | Figure D.8: Flow vorticity around airfoil undergoing motion corresponding to optimal pitching
              | motion for fixed translational motion, i.e., solution of (D.44) under parametrization PI ( ). The
              | pitching motion greatly reduces the degree of flow separation and vortex shedding compared to
              | the initial guess, and requires less work to complete the mission. Snapshots taken at times t =
meta          | 0.0, 0.8, 1.6, 2.4, 3.2, 4.0.
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          263
blank         | 
              | 
              | 
              | 
text          | Figure D.9: Flow vorticity around airfoil undergoing motion corresponding to optimal rigid body mo-
              | tion, i.e., solution of (D.44) under parametrization PII (    ). This rigid body motion further reduces
              | the degree of flow separation and required work to complete the mission. This motion differs from
              | the solution of PI as it has a larger pitch amplitude and slightly overshoots the final vertical position
              | before settling to the required position. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0.
blank         | 
              | 
title         | D.2.6     Numerical Experiment: Energetically Optimal Shape and Flapping
text          |           Motion of 2D Airfoil at Constant Impulse
              | In this section, the high-order, time-dependent PDE-constrained optimization framework introduced
              | in this document is applied to find the energetically optimal flapping motion, under an impulse
              | constraint, of a 2D NACA0012 airfoil (Figure D.10) with chord length l = 1 and zero-thickness
              | trailing edge. The governing equations are the 2D compressible, isentropic Navier-Stokes equations.
blank         | 
              | 
              | 
text          |                                 l
              |                       l/3
blank         | 
text          |           θ(t)                                                              c(t)
blank         | 
              | 
text          |                      y(t)
blank         | 
              | 
text          |                             Figure D.10: Airfoil kinematics and deformation
blank         | 
              | 
text          |    The goal is to determine the flapping motion—y(t) and θ(t)—and shape—c(t)—of the airfoil
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                       264
blank         | 
              | 
              | 
text          | that minimizes the total energy such than a x-impulse of q is achieved, i.e.,
blank         | 
text          |                             minimize     W(U , µ)
              |                               U, µ
blank         | 
text          |                             subject to Jx (U , µ) = q                                                           (D.45)
              |                                           ∂U
              |                                              + ∇ · F (U , ∇U ) = 0                  in v(µ, t).
              |                                           ∂t
blank         | 
text          | The flapping frequency is fixed at 0.2, which corresponds to a period of T = 5. Proper initialization
              | of the flow is the initial condition that results in a time-periodic flow [212] to completely avoid non-
              | physical transients and simulate representative, in-flight conditions; this experiment uses a crude
              | approximation that initializes the flow from the steady-state condition, simulates 3 periods of the
              | flapping motion, and integrates the quantities of interest over the last period only. The deformation
              | of the domain is determined from the value of c(t) using the spatial blending map of Section D.2.4
              | with                                                 "                              #
              |                                                                   0
              |                                       ϕ(X, µ, t) =                              2                               (D.46)
              |                                                          2c(t)e−[(X−x0 )·e1 ]
blank         | 
text          |    The trajectory of the airfoil—y(t), and θ(t)—and its shape – c(t)—are discretized via cubic
              | splines with my + 1, mθ + 1, and mc + 1 knots, respectively, with boundary conditions that enforce
blank         | 
text          |          y(t) = −y(t + T /2)              θ(t) = −θ(t + T /2)                   c(t) = −c(t + T /2).            (D.47)
blank         | 
text          | These boundary conditions1 for y(t), θ(t), and c(t) correspond to a mirroring of the trajectory at
              | t = T /2 and implicitly enforces periodicity with period T . The knots are uniformly spaced between
              | 0 and T in the t-dimension and the knot values are optimization parameters. Since the unsteady
              | simulation is initialized from the steady-state flow, non-zero velocities of the airfoil at t = 0 will
              | result in non-physical transients. These transients are avoided by blending the periodic cubic spline
              | smoothly to the zero function at the beginning of the time interval [193]. Let sy (t; µ), sθ (t; µ), and
              | sc (t; µ) denote the periodic cubic spline approximations. Then, the flapping and shape trajectories
              | are defined as
blank         | 
text          |                y(t) = b(t)sy (t; µ)          θ(t) = b(t)sθ (t; µ)                       c(t) = b(t)sc (t; µ),   (D.48)
blank         | 
text          |                         2
              | where b(t) = 1.0 − e−t . Table D.4 summarizes two parametrizations considered in this section:
              | (FI) rigid body motion parametrized via cubic splines and shape fixed at nominal value and (FII) rigid
              | body motion and shape of airfoil parametrized via cubic splines. With this parametrization of the
              | airfoil kinematics and shape, spatial and temporal discretization with the high-order scheme of
              |   1 Periodic and mirrored cubic splines of this form with m + 1 knots only have m degrees of freedom since the
blank         | 
text          | boundary condition prescribes the value of the m + 1 knot from the values of the others m.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                 265
blank         | 
              | 
              | 
text          | Table D.4: Summary of parametrizations considered in Section D.2.6. The number of periodic cubic
              | spline knots used to discretize y(t), θ(t), and ¸(t) are my + 1, mθ + 1, and mc + 1, respectively.
              | FI freezes the airfoil shape and considers only rigid body motions (my = mθ 6= 0, mc = 0). FII
              | parametrizes both shape and kinematic motion (my = mθ = mc 6= 0).
blank         | 
text          |                                               FI                              FII
              |                                my      mθ          mc   Nµ     my     mθ            mc   Nµ
              |                                 4         4        0     6      4         4         4    9
blank         | 
              | 
text          | Section D.1 leads to the fully discrete version of the optimization problem in (D.45)
blank         | 
text          |                                                                               (1)
              |                         minimize              W (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ)
              |                   (0)        (Nt )   Nu
              |                  u , ..., u        ∈R   ,
              |                   (1)
              |                  k1 , ..., ks(Nt ) ∈RNu ,
              |                         µ∈RNµ
              |                                                                               (1)
              |                  subject to                   Jx (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ) = 0
              |                                               u(0) = u0                                                   (D.49)
              |                                                                     s
              |                                                                               (n)
              |                                                                     X
              |                                               u(n) = u(n−1) +             bi ki
              |                                                                     i=1
blank         |                                                                                     
text          |                                                  (n)             (n)
              |                                               M ki      = ∆tn r ui , µ, tn−1 + ci ∆tn .
blank         | 
text          |    Given the gradient verification from the previous section, attention is turned directly to the
              | optimization problem in (D.49) for various values of the impulse constraint, q. The optimization
              | solver used in this section is SNOPT [70], a nonlinearly constrained SQP method. Figure D.11
              | contains the initial guess for the optimization problem in (D.44) as well as its solution under both
              | parametrization, FI and FII. The initial guess for the optimization problem is a pure heaving motion
              | at a fixed shape, i.e., c(t) = θ(t) = 0. The solution under parametrization PI freezes the shape at its
              | nominal configuration (NACA0012) and modifies the rigid body motion. Pitch is introduced for all
              | values of the impulse constraint and the amplitude of the heaving motion is decreased for q = 0.0, 1.0
              | and increased for q = 2.5. The solution under parametrization PII reduces the heaving amplitude
              | and slightly increases the pitch amplitude as compared to PI. It also introduces non-trivial camber.
              |    The instantaneous quantities of interest—W and Jx in this case—for the nominal motion and
              | shape and solution of (D.49) under parametrizations PI and PII are included in Figure D.12. It
              | is clear that the optimal solution under both parametrizations result in a time history of the total
              | power that is uniformly closer to 0 than that at the nominal trajectory, which is expected since W
              | is the objective function. It is also clear that larger values of the impulse constraint require more
              | power to complete the flapping motion. While it may not be clear from Figure D.12, the integration
              | of Fxh leads to an impulse that exactly conforms to the specified value of q. This can be seen more
              | clearly in Figure D.13. These observations can also be verified in Figure D.13 and Table D.5 that
              | summarizes the optimal values of the integrated quantities of interest.
              |    Figure D.13 shows the convergence of the integrated quantities of interest with iterations in
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                   266
blank         | 
              | 
              | 
              | 
text          |                                                                        1
              |         1
              |                                                                       0.5
              | y(t)
blank         | 
              | 
              | 
              | 
text          |                                                            θ(t)
              |         0                                                              0
blank         | 
text          |                                                                   −0.5
              |        −1
              |             10   11    12          13       14        15              −1
              |                                                                         10         11         12          13        14         15
              |                               t                                                                     t
blank         | 
text          |                                     0.4
blank         | 
text          |                                     0.2
              |                             c(t)
blank         | 
              | 
              | 
              | 
text          |                                         0
              |                                    −0.2
blank         | 
text          |                                    −0.4
              |                                        10        11        12          13         14         15
              |                                                                   t
blank         | 
              | 
text          | Figure D.11: Trajectories of y(t), θ(t), and c(t) at initial guess (     ), solution of (D.49) under
              | parametrization FI (q = 0.0:       , q = 1.0:      , q = 2.5:       ), and solution of (D.49) under
              | parametrization FII (q = 0.0:     , q = 1.0:    , q = 2.5:     ) from Table D.4.
blank         | 
              | 
              | 
              | 
text          |         0
              |                                                                             0
              |        −2
              |                                                                 Fxh
              | Ph
blank         | 
              | 
              | 
              | 
text          |                                                                       −0.5
blank         | 
text          |        −4
              |                                                                         −1
              |         10       11   12           13       14        15                     10         11         12          13        14         15
              |                            time                                                                         time
blank         | 
              | 
text          | Figure D.12: Time history of total power, P h (u, µ, t), and x-directed force, Fxh (u, µ, t), imparted
              | onto foil by fluid at initial guess (    ), solution of (D.49) under parametrization FI (q = 0.0:    ,
              | q = 1.0:        , q = 2.5:       ), and solution of (D.49) under parametrization FII (q = 0.0:       ,
              | q = 1.0:      , q = 2.5:      ) from Table D.4.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                           267
blank         | 
              | 
              | 
text          | the optimization solver. The aforementioned observations can be verified by inspection of the final
              | iteration: all impulse constraints are satisfied, larger values of q require more work to achieve,
              | and morphing the shape of the airfoil allows for a slight reduction in the required work. After 20
              | iterations, the impulse constraint is satisfied for q = 0.0, 1.0 and reduction of the work has essentially
              | ceased, implying the optimization could have been terminated at that point. The case with q = 2.5
              | requires an additional 15 - 20 iterations to settle to a converged solution.
blank         | 
text          |        0                                                         1
              |                                                                  0
blank         | 
              | 
              | 
              | 
text          |                                                            Jx
              | W
blank         | 
              | 
              | 
              | 
text          |      −5                                                         −1
              |                                                                 −2
              |     −10
              |            0         20               40         60                  0         20               40           60
              |                           iteration                                                 iteration
blank         | 
              | 
text          | Figure D.13: Convergence of quantities of interest, W and Jx , with optimization iteration for
              | parametrization FI (q = 0.0:       , q = 1.0:    , q = 2.5:     ) and FII (q = 0.0:     , q = 1.0:  ,
              | q = 2.5:       ) from Table D.4. Each optimization iteration requires the a primal flow computation—
              | to evaluate quantities of interest—and its corresponding adjoint—to evaluate the gradient of quan-
              | tities of interest.
blank         | 
text          |     The shape and motion of the airfoil and vorticity of the surrounding flow are shown in Figure D.14
              | (nominal), Figure D.15 (optimal solution under parametrization FI for q = 2.5), and Figure D.16
              | (optimal solution under parametrization FII for q = 2.5). The flow corresponding to the nominal
              | configuration experiences flow separation and vortex shedding, which results in the relatively large
              | amount of total energy to complete the flapping motion and does not satisfy the impulse constraint.
              | Fixing the shape and optimizing over the heaving and pitching motion (FI) dramatically reduces the
              | amount of shedding and consequently reduces the amount of work required. Optimizing the shape in
              | addition to the pitching and heaving motion (FII) further reduces the shedding and required work.
              | The solution of FI and FII both satisfy the impulse constraint to greater than 8 digits of accuracy.
              |     To conclude this section, a brief comparison of the optimal flapping motions found in this work are
              | compared to those found in the literature. From Figure D.11, the pitch of the foil leads its plunge
              | by approximately 90◦ in all optimal flapping motions, a result that was found in several works
              | that range from experimental and computational [191, 162, 158, 148]. The improved efficiency is
              | largely due to a dramatic reduction in leading edge vortex shedding characteristic of pure heaving
              | motions (Figure D.14) [191, 158]. The specific pitching and heaving amplitudes were determined by
              | the optimizer such that the thrust constraint is satisfied; as the thrust requirement is increased, the
              | magnitude of the pitch and plunge increase and leading edge shedding off the leading edge is induced
              | (Figure D.15) [148]. The time-dependent shape deformation slightly reduces the magnitude of the
              | vortices shedding off the leading edge, which can be seen by comparing Figures D.15 and D.16.
              | Table D.5: Table summarizing integrated quantities of interest at optimal solution of each optimization problem for each impulse level. In all
              | cases, the desired value of Jx is achieved to greater than 4 digits of accuracy. The optimal solution for larger values of the impulse constraint
              | require more total work to complete flapping motion, i.e., work monotonically increases in magnitude as value of impulse constraint increases.
              | Smaller values of total work are achievable if airfoil is allowed to morph its shape in addition its rigid body motion. The other integrated
              | quantities are included for completeness, but do not exhibit trends since they were not in the optimization problem.
blank         | 
text          |                           Initial                          FI                                           FII
              |                  q                         0.0            1.0            2.5            0.0            1.0             2.5
              |                  W     -9.4096e+00    -4.5695e-01    -2.1419e+00    -4.9476e+00     -4.3252e-01    -2.0271e+00    -4.6110e+00
              |                  Wx    0.0000e+00     0.0000e+00      0.0000e+00    0.0000e+00      0.0000e+00     0.0000e+00      0.0000e+00
              |                  Wy    -9.4096e+00    -4.2807e-01    -2.0642e+00    -4.7967e+00     -3.7363e-01    -1.5413e+00    -3.3712e+00
              |                  Wθ    0.0000e+00      2.8883e-02     7.7694e-02     1.5083e-01     1.7101e-02      6.7900e-03     1.8744e-01
              |                  Fx     -1.7660e-01   -4.0490e-11    -1.0000e+00    -2.5000e+00     1.6937e-10     -1.0000e+00    -2.5000e+00
              |                  Fy     3.5413e-01     1.5989e-02     5.0480e-02     9.7240e-02     -1.5292e-02     4.8657e-02     9.6440e-02
meta          |                                                                                                                                                     APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION
              |                                                                                                                                                     268
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                               269
blank         | 
              | 
              | 
              | 
text          | Figure D.14: Flow vorticity around flapping airfoil undergoing motion corresponding to initial
              | guess for optimization problem (D.49), i.e., pure heaving (    ). Flow separation off leading
              | edge implies a large amount of work required for flapping motion. Snapshots taken at times
              | t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                  270
blank         | 
              | 
              | 
              | 
text          | Figure D.15: Flow vorticity around flapping airfoil undergoing optimal rigid body motion corre-
              | sponding to the solution of (D.49) under parametrization FI. The x-directed impulse is Jx = 2.5.
              | The pitching motion greatly reduces the degree of flow separation and vortex shedding compared
              | to the initial guess, and requires less work to complete the flapping motion and generate desired
              | impulse. Snapshots taken at times t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                271
blank         | 
              | 
              | 
              | 
text          | Figure D.16: Flow vorticity around flapping airfoil undergoing optimal deformation and kine-
              | matic motion, corresponding to the solution of (D.49) under parametrization FII. The x-directed
              | impulse is Jx = 2.5. The morphing further reduces the flow separation and work required
              | to complete the flapping motion and generate desired impulse. Snapshots taken at times t =
meta          | 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                            272
blank         | 
              | 
              | 
title         | D.3        Computing Time-Periodic Solutions of Partial Differen-
              |            tial Equations
text          | This section is devoted to the solution of partial differential equations with time-periodicity con-
              | straints. This will largely be a review of existing work on the topic [131, 196, 7, 8, 201, 77], although
              | emphasis will be placed on equations that are parametrized and fully discretized. This will lead
              | to the main contribution of this work, the fully discrete adjoint equations corresponding to time-
              | periodic solutions of partial differential equations and their use in computing gradients of quantities
              | of interest along the manifold of time-periodic solutions.
              |    Consider the general, nonlinear, time-periodically constrained system of partial differential equa-
              | tions, parametrized by the vector µ ∈ RNµ ,
blank         | 
text          |                                  ∂U
              |                                       = L(U , µ, t)                   in         Ω(µ, t) × (0, T ]
              |                                  ∂t                                                                  (D.50)
              |                              U (x, 0) = U (x, T ),
blank         | 
text          | where L(·, µ, t) is a spatial differential operator on the parametrized, time-dependent domain Ω(µ, t) ⊂
              | Rnsd . The boundary conditions have not been explicitly stated for brevity. This work will only con-
              | sider temporally first-order partial differential equations, or those that have been recast as such.
              | Without loss of generality, consider a quantity of interest of the form
              |                                                  Z     T   Z
              |                                  F(U , µ) =                          f (U , µ, t) dS dt,             (D.51)
              |                                                    0       Γ(µ,t)
blank         | 
              | 
text          | where Γ(µ, t) ⊆ ∂Ω(µ, t). The generalization to other types of quantities of interest, such as volu-
              | metric integrals and instantaneous or pointwise quantities of interest, is immediate as the specific
              | form of the quantity of interest will be abstracted away at the fully discrete level. The form in
              | (D.51) will be used in the physical setup of the applications in subsequent sections. In subsequent
              | sections, this quantity of interest will correspond to either the objective function or a constraint of
              | an optimization problem governed by a partial differential equation and subject to a time-periodicity
              | requirement. After space-time discretization of (D.50) via the DG-ALE-DIRK scheme discussed in
              | Section D.1, the fully discrete equations are
              |                                                                s
              |                                                                            (n)
              |                                                                X
              |                                    u(n) = u(n−1) +                   bi ki
              |                                                                i=1                                   (D.52)
blank         |                                                                                            
text          |                                    (n)                      (n)
              |                                 M ki     = ∆tn r           ui ,      µ, tn−1 + ci ∆tn ,
blank         | 
text          |         (n)
              | where ui      is defined in (D.15) and the fully discrete quantity of interest is
blank         | 
text          |                                                                      (1)
              |                                     F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) ).               (D.53)
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          273
blank         | 
              | 
              | 
text          | Time-periodicity may then be expressed as the constraint
blank         | 
text          |                                                 u(0) = u(Nt ) ,                                   (D.54)
blank         | 
text          | where Nt is the time index of the cycle period.
              |    The next section discusses methods for solving the fully discrete, time-periodically constrained
              | partial differential equations. The periodicity constraint, i.e., u(0) = u(Nt ) , turns the problem into
              | a nonlinear two-point boundary value problem, which eliminates the possibility of using traditional
              | evolution methods (since the initial conditions is unknown).
blank         | 
              | 
title         | D.3.1      Numerical Solvers: Shooting Methods
text          | This section provides a brief, non-exhaustive review of methods which have been introduced for
              | solving time-periodic partial differential equations. A distinguishing feature of this work is that
              | we directly consider the fully discrete form of the governing equations, whereas previous work has
              | focused on the continuous [194] or semi-discrete [185] levels. The section will conclude with a
              | discussion of a Newton-Krylov shooting method using a purely matrix-free Krylov solver to solve
              | the linear systems of equations that arise, which extends the work in [77].
              |    Define u(Nt ) (u0 ; µ) as the solution of the following initial-value problem
blank         | 
text          |                                    u(0) = u0
              |                                                       s
              |                                                                  (n)
              |                                                       X
              |                                   u(n) = u(n−1) +            bi ki                                (D.55)
              |                                                        i=1
blank         |                                                                        
text          |                                     (n)               (n)
              |                                M ki       = ∆tn r ui , µ, tn−1 + ci ∆tn ,
blank         | 
text          | which can be solved using a traditional evolution algorithm that advances the solution from timestep
              | n to n + 1. Notice that this overloads the notation introduced in Section 2.1.3, which defines u(Nt )
              | as the discrete approximation of the time-periodic solution of the system of partial differential
              | equations at the final time. Here, it is a nonlinear function that maps a state u0 ∈ RNu to the state
              | u(Nt ) (u0 ; µ). It is clear that u0 is the time-periodic initial condition of the fully discrete partial
              | differential equation if it is a fixed point of u(Nt ) (·; µ), namely
blank         | 
text          |                                              u(Nt ) (u0 ; µ) = u0 .                               (D.56)
blank         | 
text          | Then, provided the mapping u0 → u(Nt ) (u0 ; µ) is a contraction mapping, the Banach Fixed Point
              | Theorem implies the existence of the fixed point and provides a convergent algorithm for finding
              | it, see Algorithm 21. This is a convenient algorithm as it only relies on solution of the nonlinear
              | evolution equation (D.55), but is known to suffer from poor convergence rates and lack of convergence
              | if the mapping under consideration is not a contraction.
              |    Another class of solvers for time-periodically constrained partial differential equations rely on
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                               274
blank         | 
              | 
              | 
text          | Algorithm 21 Fixed Point Iteration Time-Periodic Solutions of PDE
              | Input: Initial guess for periodic initial condition, u0 ; parameter configuration, µ
              | Output: Periodic initial condition, u(0)
              |  1: while u(Nt ) (u0 ; µ) − u0 2 >  do
              |  2:  Update
              |                                             u0 ← u(Nt ) (u0 ; µ)
              |  3:   end while
              |  4:   Define periodic initial condition
              |                                                              u(0) = u0
blank         | 
              | 
text          | unconstrained, gradient-based optimization techniques. Define the function
blank         | 
text          |                                                         1                        2
              |                                             j(u0 ) =      u(Nt ) (u0 ; µ) − u0                        (D.57)
              |                                                         2                        2
blank         | 
              | 
text          | and consider the unconstrained optimization problem
blank         | 
text          |                                                        minimize j(u0 ),                               (D.58)
              |                                                        u0 ∈RNu
blank         | 
              | 
text          | which can be solved using gradient-based optimization techniques such as steepest descent, the
              | Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, or the limited-memory version of BFGS (L-
              |                                                     dj
              | BFGS) [71, 215, 143]. The gradient of (D.57),          , is usually computed using the adjoint method
              |                                                    du0
              | since the large number of optimization variables, Nu , renders the finite differences method or the
              |                                                                                    d(·)
              | linearized forward method impractical [78]. Throughout this work, the notation          will be used to
              |                                                                                     dµ
              | denote the total derivative of a quantity of interest with respect to parameters—including the explicit
              | dependence as well as the implicit dependence through the solution of the governing equation—and
              |                                 ∂(·)
              | the partial derivative notation      will be used elsewhere. The adjoint equations for the fully discrete
              |                                 ∂µ
              | evolution equations in (D.55) corresponding to the quantity of interest, j(u0 ), with parameter u0
              | are
              |                       λ(Nt ) = u(Nt ) (u0 ; µ) − u0
              |                                                 s
              |                          (n−1)        (n)
              |                                                 X         ∂r  (n)                 T
              |                                                                                       (n)
              |                      λ           =λ         +       ∆tn       ui , µ, tn−1 + ci ∆tn κi
              |                                                 i=1
              |                                                           ∂u                                          (D.59)
              |                                                    s
              |                            (n)
              |                                                   X      ∂r  (n)                 T
              |                                                                                      (n)
              |                    M T κi        = bi λ(n) +     aji ∆tn     uj , µ, tn−1 + cj ∆tn κj
              |                                              j=i
              |                                                          ∂u
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. The gradient of j(u0 ) is reconstructed from the dual variables as
blank         | 
text          |                                             dF       T
              |                                                = λ(0) + u0 − u(Nt ) (u0 ; µ).                         (D.60)
              |                                             dµ
              | See [211] for the derivation. These methods have been used with considerable success to solve a
              | variety of time-periodic partial differential equations, including the Benjamin-Ono equation [7], a
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                  275
blank         | 
              | 
              | 
text          | wave-guide array mode-locked laser system [202], and the vortex sheet with surface tension [8]. Un-
              | fortunately, the underlying optimization algorithms suffer from relatively slow convergence, requiring
              | many line-searches before becoming superlinear, and never achieve quadratic convergence.
              |    An attractive alternative is to recast the fixed point iteration as a nonlinear system of equations
              | and use the Newton-Raphson method to reap the benefits of quadratic convergence. To this end,
              | define the nonlinear system of equations
blank         | 
text          |                                      R(u0 ) = u(Nt ) (u0 ; µ) − u0 = 0                                    (D.61)
blank         | 
text          | with Jacobian matrix
              |                                              ∂R          ∂u(Nt )
              |                                 J (u0 ) =        (u0 ) =         (u0 ; µ) − I                             (D.62)
              |                                              ∂u0          ∂u0
              | where I is the Nu × Nu identity matrix. The crucial component of the Newton-Raphson method is
              | the solution of a linear system of equations with the Jacobian (D.62), i.e., the solution of J (u0 )x = b,
              |                                                                            ∂u(Nt )
              | given u0 ∈ RNu and b ∈ RNu . A linear evolution equation defining                  , i.e., the sensitivity of
              |                                                                             ∂u0
              | the final state with respect to perturbations in the initial state, is introduced by linearizing the fully
              |                                                                                 (n)
              | discrete evolution equation in (D.55) about the primal state u(n) , ki                with respect to the initial
              | state u0 . Direct differentiation of (D.55) with respect to u0 leads to the forward sensitivity equations
blank         | 
text          |                   ∂u(0)
              |                         =I
              |                    ∂u0
              |                                          s         (n)
              |                   ∂u(n)   ∂u(n−1) X ∂ki
              |                         =        +     bi
              |                    ∂u0     ∂u0            ∂u0                                                             (D.63)
              |                                    i=1
              |                                                                                                    
              |                      (n)                               ∂u(n−1) X                     i       (n)
              |                   ∂ki        ∂r  (n)                                    ∂kj
              |               M        = ∆tn     ui , µ, tn−1 + ci ∆tn        +     aij      .
              |                    ∂u0       ∂u                          ∂u0     j=1
              |                                                                           ∂u0
blank         | 
text          |             ∂u(Nt )
              | In general,         is a large (Nu × Nu ), dense matrix that requires the solution of Nu linear
              |              ∂u0
              | evolution equations to form. While it is true that the columns of the matrix can be solved in
              | parallel, formation and storage of this matrix may be impractical, particularly for the large-scale
              | computational fluid dynamics problems that motivate this work. For non-dissipative problems such
              | as standing waves in the free-surface Euler equations [201, 176], this is worth the expense since all
              | perturbation directions have to be explored (as opposed to letting the evolution over a cycle damp
              | out high frequency transients). But for viscous problems such as those studied in the numerical
              | experiments, solving the Newton-Raphson equations by Krylov subspace methods requires many
              | fewer iterations than there are columns of the Jacobian.
              |                                ∂u(Nt )
              |     Formation and storage of           can be completely avoided if a matrix-free Krylov method [106]
              |                                  ∂u0
              | is used to solve the linear systems arising in the Newton-Raphson method, i.e., J (u0 )x = b. In this
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                               276
blank         | 
              | 
              | 
text          | case, only matrix-vector products of the form
blank         | 
text          |                                            ∂R           ∂u(Nt )
              |                               J (u0 )v =       (u0 )v =         (u0 ; µ)v − v                         (D.64)
              |                                            ∂u0           ∂u0
blank         | 
text          | for any v ∈ RNu , are required. For efficiency, these must be computed without explicitly forming the
              |         ∂u(Nt )
              | matrix          . This is accomplished by considering the forward sensitivity equations in (D.63) in
              |           ∂u0
              | the direction defined by v. Multiplying (D.63) by the vector v leads to the system of linear evolution
              | equations
blank         | 
text          |             ∂u(0)
              |                   v=v
              |              ∂u0
              |                                  s       (n)
              |             ∂u(n)    ∂u(n−1)    X     ∂k
              |                   v=         v+     bi i v
              |              ∂u0      ∂u0              ∂u0                                                            (D.65)
              |                                 i=1
              |                                                                                           
              |               (n)                                   ∂u(n−1)     i        (n)
              |             ∂ki           ∂r  (n)                              X       ∂kj
              |         M         v = ∆tn     ui , µ, tn−1 + ci ∆tn         v+     aij       v .
              |              ∂u0          ∂u                          ∂u0       j=1
              |                                                                          ∂u0
blank         | 
text          |                                              (n)
              |                         ∂u(n)          ∂ki
              | These can be solved for       · v and        · v directly, only requiring one linear evolution for each
              |                          ∂u0            ∂u0
              | v. Since the equations in (D.65) are linear, the underlying linear solver must be converged to high
              | accuracy if accurate sensitivities are to be obtained. This mitigates the speedup with respect to the
              | nonlinear, primal solves whose linear systems are usually solved to low precision. For the problems
              | considered in Section D.4.4–D.4.5, the primal equations were, on average, 2 times more expensive
              | than the sensitivity equations, even though 5 nonlinear iterations were required for convergence.
              | This implies the cost of evaluating R(u0 ) is approximately 2 times as expensive as a Jacobian-
              | vector product J (u0 )v. The Newton-Krylov method, with Jacobian-vector products computed as
              | the solution of (D.65), is summarized in Algorithm 22. If the linear system of equations arising
              | at each iteration is solved to sufficient accuracy, this algorithm will converge quadratically. The
              | starting guess can be obtained by fixed point iteration (Algorithm 21), or, if solutions come in
              | families parametrized by an amplitude, by numerical continuation [76, 103, 53, 7, 8, 200, 176]. The
              | latter approach is particularly useful if the system is not dissipative and externally driven, as fixed
              | point iteration relies on transient modes being damped by the evolution equations, i.e., on the
              | periodic solution being stable and attracting.
              |    Given this exposition on methods for computing time-periodic solutions of partial differential
              | equations, we turn our attention to determining the stability of the corresponding periodic orbit.
blank         | 
              | 
text          | D.3.2       Stability of Periodic Orbits of Fully Discrete Partial Differential
              |             Equations
              | In this section, the concept of stability of a periodic orbit of fully discrete partial differential equations
              | is introduced and a method for determining the stability of a periodic solution presented. Recall
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                      277
blank         | 
              | 
              | 
text          | Algorithm 22 Newton-Krylov Shooting Method for Time-Periodic Solutions of PDE
              | Input: Initial guess for periodic initial condition, u0 ; parameter configuration, µ
              | Output: Periodic initial condition, u(0)
              |  1: while u(Nt ) (u0 ; µ) − u0 2 >  do
              |  2:  Solve unsymmetric linear system of equations
blank         | 
text          |                                      ∂u(Nt )
              |                                              (u0 ; µ) · ∆u = u(Nt ) (u0 ; µ) − u0
              |                                       ∂u0
              |       using a matrix-free Krylov method with matrix-vector products
blank         | 
text          |                                                   ∂u(Nt )
              |                                                           (u0 ; µ) · v
              |                                                    ∂u0
              |       computed as the solution of the directional sensitivity equations (D.65)
              |  3:     Update solution
              |                                               u0 ← u0 − ∆u
              |  4:   end while
              |  5:   Define periodic initial condition
              |                                                        u(0) = u0
blank         | 
              | 
text          | the interpretation of u(Nt ) as a function that propagates an initial condition u0 to its final state
              | u(Nt ) (u0 ; µ). Let u∗0 (µ) be the time-periodic solution of the fully discrete partial differential
              | equation in (D.52), (D.54) at parameter configuration µ, i.e., u∗0 (µ) = u(Nt ) (u∗0 ; µ). A periodic
              | orbit is stable if there is a δ > 0 such that
blank         | 
text          |                                lim    u(n·Nt ) (u∗0 (µ) + ∆u; µ) − u∗0 (µ) = 0                (D.66)
              |                               n→∞
blank         | 
              | 
text          | if k∆uk < δ, where
              |                            u(n·Nt ) (u0 ; µ) = u(Nt ) (·; µ) ◦ · · · ◦ u(Nt ) (u0 ; µ).       (D.67)
blank         | 
text          | A Taylor expansion of u(Nt ) about the periodic solution leads to
blank         | 
text          |                                                       ∂u(Nt ) ∗                       2
              |                    u(Nt ) (u∗0 (µ); µ) = u∗0 (µ) +           (u0 (µ); µ) · ∆u + O(k∆uk )      (D.68)
              |                                                        ∂u0
blank         | 
text          | where time-periodicity of u∗0 (µ) was used. Repeated application of (D.68) leads to
              |                                                                         n
              |                                                       ∂u(Nt ) ∗
blank         |                                                   
text          |                                                                                     n+1
              |          u(n·Nt ) (u∗0 (µ) + ∆u; µ) = u∗0 (µ) +              (u0 (µ); µ) ∆u + O(k∆uk    ).    (D.69)
              |                                                        ∂u0
blank         | 
text          |                                                                              ∂u(Nt ) ∗
              | Taking δ < 1, the stability criteria in (D.66) is satisfied if all eigenvalues of   (u0 (µ); µ) have
              |                                                                                ∂u0
              | modulus strictly less than 1. In Section D.4.4, the stability of the periodic flow around a flapping
              | airfoil is verified using this method.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                      278
blank         | 
              | 
              | 
title         | D.4         Fully Discrete, Time-Periodic Adjoint Method
text          | In this section, the adjoint equations corresponding to the fully discrete time-periodically constrained
              |                                                                                                                          (1)
              | partial differential equations (D.52), (D.54) and quantity of interest F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ),
              |                                                                                                 (1)
              | will be derived. For the remainder of this section, u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) will be taken as
              | the time-periodic solution of the fully discrete partial differential equations (D.52), (D.54) at pa-
              | rameter µ. The adjoint equations will be derived by linearizing the fully discrete equations about
              | this periodic solution. This highlights the importance of an efficient periodic solver—the subject of
              | Section D.3.1—as it is a prerequisite for the adjoint method.
              |     Before proceeding to the derivation of the adjoint equations, the following definitions are in-
              | troduced for the fully discrete time-periodic constraint and Runge-Kutta stage equations and state
              | updates
blank         | 
text          |                                r̃ (0) (u(0) , u(Nt ) ) = u(0) − u(Nt ) = 0
              |                                                                                      s
              |                                 (n)                                                           (i)
              |                                                                                      X
              |      r̃ (n) (u(n−1) , u(n) , k1 , . . . , ks(n) , µ) = u(n) − u(n−1) −                     bi ki = 0                           (D.70)
              |                                                                                      i=1
blank         |                                                                                                  
text          |                 (n)              (n)         (n)               (n)            (n)
              |               Ri (u(n−1) , k1 , . . . , ki , µ) = M ki               − ∆tn r ui , µ, tn−1 + ci ∆tn = 0
blank         | 
text          | for n = 1, . . . , n and i = 1, . . . , s.
blank         | 
              | 
title         | D.4.1        Derivation
text          | The derivation of the fully discrete adjoint equations corresponding to the output functional, F ,
              | begins with the introduction of test variables
blank         | 
text          |                                                                      (n)
              |                                                    λ(0) , λ(n) , κi        ∈ RNu                                               (D.71)
blank         | 
text          |                                                                                (1)
              | for n = 1, . . . , Nt and i = 1, . . . , s. Since u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) are taken as the solution of
              | the fully discrete time-periodic problem in (D.70), the following identity holds, for any µ ∈ RNµ ,
blank         | 
text          |                                                               Nt                           Nt X
              |                                                                                               s
              |                                                     T                      T                          (n) T    (n)
              |                                                               X                            X
              |                       F = F + 0 = F − λ(0) r̃ (0) −                  λ(n) r̃ (n) −                   κi       Ri               (D.72)
              |                                                               n=1                          n=1 i=1
blank         | 
text          |                                                               (n)
              | for any value of the test functions λ(n) and κi . In (D.72), arguments have been dropped for
              | brevity; it is understood that all terms are evaluated at the periodic solution of (D.52), (D.54) at
              |                                                                                                                    (1)
              | parameter µ. Since (D.70) holds for any µ ∈ RNµ , provided u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) is the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                         279
blank         | 
              | 
              | 
text          | corresponding periodic solution, differentiation with respect to µ leads to
blank         | 
text          |            Nt                     Nt X   s           (n)
              |                ∂F ∂u(n) X
              |                                                                       (0)
              |                                                                                  ∂ r̃ (0) ∂u(0)     ∂ r̃ (0) ∂u(Nt )
blank         |                                                                                                                      
text          | dF   ∂F   X                                  ∂F ∂ki            (0) T ∂ r̃
              |    =    +                     +                           −  λ                +                 +
              | dµ   ∂µ n=0 ∂u(n) ∂µ             n=1 i=1 ∂ki
              |                                                (n) ∂µ                 ∂µ         ∂u(0) ∂µ          ∂u(Nt ) ∂µ
              |            Nt
              |                       "                                                        s
              |                                                                                                   #
              |                            (n)                                                                (n)
              |           X
              |                 (n) T ∂ r̃         ∂ r̃ (n) ∂u(n)     ∂ r̃ (n) ∂u(n−1) X ∂ r̃ (n) ∂kp
              |         −     λ                 +                 +                        +
              |           n=1
              |                           ∂µ       ∂u(n) ∂µ         ∂u(n−1) ∂µ                p=1 ∂kp
              |                                                                                         (n) ∂µ
blank         | 
text          |                                                                                       
              |            Nt Xs                 (n)          (n)                  i      (n)    (n)
              |           X          (n) T  ∂Ri           ∂Ri     ∂u(n−1) X ∂Ri ∂kj 
              |         −         κi                  +                       +                          .
              |           n=1 i=1
              |                                ∂µ         ∂u(n−1) ∂µ             j=1 ∂k
              |                                                                           (n) ∂µ
              |                                                                                         j
              |                                                                                                                   (D.73)
              | Re-arrangement of terms in (D.73) such that the state variable sensitivities are isolated leads to the
              |                          dF
              | following expression for
              |                          dµ
blank         | 
text          |                                      (Nt )                  (0)                 Nt                      Nt Xs           (n)
              |                                                                     ∂u(Nt ) X                    (n)
blank         |                                                                 
text          | dF   ∂F       ∂F        (Nt ) T ∂ r̃             (0) T ∂ r̃                           (n) T ∂ r̃
              |                                                                                                        X
              |                                                                                                                 (n) T ∂Rp
              |    =    +           − λ                     −  λ                             −      λ                −         κp
              | dµ   ∂µ     ∂u(Nt )             ∂u(Nt )               ∂u(Nt )        ∂µ        n=0
              |                                                                                              ∂µ        n=1 p=1
              |                                                                                                                        ∂µ
              |            Nt
              |               "                                                                  s
              |                                                                                                        #
              |                                               (n−1)                    (n)                         (n)
              |           X        ∂F           (n−1) T ∂ r̃                (n) T ∂ r̃
              |                                                                                X      (n) T ∂Ri          ∂u(n−1)
              |         +                −   λ                        −  λ                   −      κ
              |           n=1
              |                 ∂u(n−1)                    ∂u(n−1)                 ∂u(n−1) i=1 i            ∂u(n−1)         ∂µ
              |                                                                           
              |            Nt Xs                            (n)       s               (n)       (n)
              |                    ∂F − λ(n) ∂ r̃                           (n) T ∂Ri  ∂kp
              |           X                           T              X
              |         +              (n)                  (n)
              |                                                   −      κ   i        (n)
              |                                                                                     .
              |           n=1 p=1 ∂ki                   ∂kp          i=p           ∂kp        ∂µ
              |                                                                                                                 (D.74)
              |                                     (n)
              | The dual variables, λ(n) and κi , which have remained arbitrary to this point, are chosen such that
              | the bracketed terms in (D.74) vanish
blank         | 
text          |                                 T                    T
              |                         ∂ r̃ (0)   (0)   ∂ r̃ (Nt )   (Nt )    ∂F T
              |                                  λ     +            λ       =
              |                        ∂u(Nt )           ∂u(Nt )              ∂u(Nt )
              |                             T                    T                                          s             (n) T
              |                     ∂ r̃ (n)   (n)   ∂ r̃ (n−1)   (n−1)     ∂F T X ∂Ri           (n)
              |                              λ     +            λ       =         −             κi                                (D.75)
              |                    ∂u(n−1)           ∂u(n−1)              ∂u(n−1)   i=1
              |                                                                         ∂u(n−1)
blank         | 
text          |                                               (n)        T                                     T
              |                                           s
              |                                           X ∂Rj               (n)        ∂F         ∂ r̃ (n)
              |                                                 (n)
              |                                                              κj     =     (n)
              |                                                                                 −     (n)
              |                                                                                                    λ(n)
              |                                           j=i ∂ki                       ∂ki         ∂ki
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. These are the fully discrete adjoint equations corresponding to the
              | time-periodic primal evolution equations in (D.70), discrete quantity of interest F , and parameter
              | µ. Defining the dual variables as the solution of the adjoint equations in (D.75), the expression for
              | dF
              |     in (D.74) reduces to
              | dµ
blank         | 
text          |                                              N                            N     s                    (n)
              |                                               T ∂ r̃ (n)
              |                                        t                     t X
              |                             dF   ∂F   X                    X            T ∂Rp
              |                                =    −    λ(n)            −         κ(n)
              |                                                                     p         .                                   (D.76)
              |                             dµ   ∂µ n=0          ∂µ        n=1 p=1
              |                                                                            ∂µ
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                280
blank         | 
              | 
              | 
text          |                                                             dF
              | This provides a means of computing the total derivative         without explicitly computing the large,
              |                                                             dµ
              | dense state sensitivities since the expression in (D.76) is independent of them. Direct differentiation
              |                (n)
              | of r̃ (n) and Ri         from their definitions in (D.70) leads to the final form of the adjoint equations of
              | the fully discrete, time-periodically constrained partial differential equations in (D.52), (D.54)
blank         | 
text          |                                    ∂F T
              |             λ(Nt ) = λ(0) +
              |                                   ∂u(Nt )
              |                                               s
              |                                     ∂F T X           ∂r  (n)                 T
              |                                                                                  (n)
              |            λ(n−1)        = λ(n) +    (n−1)
              |                                            +     ∆tn     ui , µ, tn−1 + ci ∆tn κi                        (D.77)
              |                                   ∂u         i=1
              |                                                      ∂u
              |                                      T                 s
              |                (n)            ∂F                       X               ∂r  (n)                 T
              |                                                                                                    (n)
              |          M T κi          =     (n)
              |                                          + bi λ(n) +         aji ∆tn       uj , µ, tn−1 + cj ∆tn κj
              |                              ∂ki                       j=i
              |                                                                        ∂u
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. Similarly, the total derivative of F , independent of state sensi-
              | tivities, takes the form
blank         | 
text          |                                        Nt      s
              |                              dF   ∂F   X      X    (n) T ∂r   (n)
              |                                 =    +    ∆tn     κi        (ui , µ, tn−1 + ci ∆tn ).                    (D.78)
              |                              dµ   ∂µ n=1      i=1
              |                                                          ∂µ
blank         | 
text          | From (D.77), it can be seen that the fully discrete adjoint equations take the form of a linear, two-
              | point boundary-value problem and cannot be solved directly as an evolution equation. D.6 proves
              | existence and uniqueness of solutions to (D.77). The next section will discuss solvers for the discrete
              | time-periodic adjoint equations in (D.77).
blank         | 
              | 
title         | D.4.2      Numerical Solver: Matrix-Free Krylov Method
text          | As the adjoint equations corresponding to the fully discrete time-periodic partial differential equation
              | are linear, this section will consider matrix-free Krylov methods to solve them. Alternatively, any
              | of the methods discussed in Section D.3.1 could be used.
              |    Define λ(0) (λNt ; µ, t) as the solution of the linear, backward evolution equations
blank         | 
text          |              λ(Nt ) = λNt
              |                                                          s
              |                                            ∂F T X           ∂r  (n)                 T
              |                                                                                         (n)
              |             λ(n−1) = λ(n) +                 (n−1)
              |                                                   +     ∆tn     ui , µ, tn−1 + ci ∆tn κi
              |                                          ∂u         i=1
              |                                                             ∂u                                           (D.79)
              |                                                    s
              |                    (n)        ∂F                   X               ∂r  (n)                 T
              |                                                                                                (n)
              |          M T κi          =     (n)
              |                                      + bi λ(n) +         aji ∆tn       uj , µ, tn−1 + cj ∆tn κj ,
              |                              ∂ki                   j=i
              |                                                                    ∂u
blank         | 
text          | which can be directly evolved, backward-in-time. Similar to Section D.3.1 this constitutes a notation
              | overload since λ(0) ∈ RNu is the initial solution of the adjoint equations corresponding to the fully
              | discrete periodic partial differential equations, as well as the linear function that takes a state λNt
              | to λ(0) (λNt ; µ). Then, λ(0) (λNt ; µ) is the initial solution of (D.77) if the following linear equation
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          281
blank         | 
              | 
              | 
text          | is satisfied
              |                                                                     ∂F T
              |                                        λ(0) (λNt ; µ, t) = λNt −           .                      (D.80)
              |                                                                    ∂u(Nt )
              | This is a linear system of equations of the form, Ax = b where
blank         | 
text          |                                                       ∂λ(0)
              |                                                 A=          − I.                                  (D.81)
              |                                                       ∂λNt
blank         | 
text          | The columns of the linear operator A can be formed by considering perturbations of (D.79) with
              | respect to the final state λNt . Differentiation of (D.79) with respect to λNt leads to the adjoint
              | sensitivity equations
blank         | 
text          |                       ∂λ(Nt )
              |                               =I
              |                        ∂λNt
              |                                            s                                       (n)
              |                      ∂λ(n−1)   ∂λ(n) X        ∂r  (n)                  T ∂κ
              |                                                                              i
              |                              =      +     ∆tn     ui , µ, tn−1 + ci ∆tn                           (D.82)
              |                       ∂λNt     ∂λNt   i=1
              |                                               ∂u                           ∂λ Nt
blank         | 
text          |                        (n)              s                                                (n)
              |                      ∂κi
              |                      T          ∂λ(n) X            ∂r  (n)                  T ∂κ
              |                                                                                   j
              |                    M       = bi      +     aji ∆tn     uj , µ, tn−1 + cj ∆tn          .
              |                      ∂λNt       ∂λNt   j=i
              |                                                    ∂u                           ∂λ Nt
blank         | 
              | 
              | 
text          |                                                                     ∂λ(0)
              | Similar to the situation for the primal problem, the matrix                is an Nu × Nu dense matrix
              |                                                                      ∂λNt
              | that requires Nu         linear evolution equations to form. As this is impractical for large problems, a
              | matrix-free Krylov method is used to solve (D.80), which only requires matrix-vector products of
              | the form
              |                                                       ∂λ(0)
              |                                                Av =         v − v.                                (D.83)
              |                                                       ∂λNt
              | The first term in this matrix-vector product can be computed directly by considering the adjoint
              | sensitivity equations in a given direction v
blank         | 
text          |                 ∂λ(Nt )
              |                         v=v
              |                  ∂λNt
              |                                     s                                T ∂κ(n)
              |                ∂λ(n−1)    ∂λ(n)    X       ∂r  (n)                       i
              |                        v=       v+     ∆tn     ui , µ, tn−1 + ci ∆tn          v                   (D.84)
              |                 ∂λNt      ∂λNt     i=1
              |                                            ∂u                           ∂λ Nt
blank         | 
text          |              (n)                  s                                                  (n)
              |            ∂κi T        ∂λ(n)    X           ∂r  (n)                  T ∂κ
              |                                                                             j
              |          M       v = bi       v+     aji ∆tn     uj , µ, tn−1 + cj ∆tn          v.
              |            ∂λNt         ∂λNt     j=i
              |                                              ∂u                           ∂λ Nt
blank         | 
              | 
              | 
text          |                                             ∂λ(0)
              | The equations in (D.84) can be solved for         · v at the cost of one linear evolution solution for
              |                                             ∂λNt
              | each v. The adjoint sensitivity equations in (D.84) are independent of the quantity of interest, F . If
              | there are multiple quantities of interest, fast multiple right-hand side solvers [182, 38, 79] could be
              | used to solve Ax = b as the matrix A will be fixed and only the right-hand side varied. Furthermore,
              | the adjoint sensitivity equations in (D.84) and the adjoint equations in (D.79) are identical, with the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                         282
blank         | 
              | 
              | 
text          |                          ∂F           ∂F
              | exception of the terms          and         . Therefore, the adjoint sensitivities are less expensive to
              |                        ∂u(n−1)       ∂ki
              |                                         (n)
blank         | 
text          | compute than the adjoint states and the savings becomes substantial when the number of parameters
              |                       ∂F          ∂F
              | in µ is large since         and         become expensive to compute. Algorithm 23 below details
              |                     ∂u(n−1)      ∂ki
              |                                     (n)
blank         | 
text          | the use of a matrix-free GMRES method to solve (D.80) with matrix-vector products defined by
              | (D.84).
blank         | 
text          | Algorithm 23 GMRES for Solution of Fully Discrete, Time-Periodic Adjoint PDE
              | Input: Initial guess for periodic adjoint final condition, λNt ,0 ; parameter configuration, µ; periodic
              |                                               (1)          (N )
              |     primal solution, u(0) , . . . , u(Nt ) , k1 , . . . , ks t
              |                                                          (Nt )
              | Output: Periodic adjoint final condition, λ
              |  1: Compute
              |                                                                       ∂F T
              |                                        r0 = λ(0) (λNt , µ) +                 − λNt ,0
              |                                                                      ∂u(Nt )
              |  2: Set β = kr0 k2 , v1 = r0 /β, and λNt = λNt ,0
blank         | 
text          |              (0)                   ∂F T
              |  3: while λ (λNt , µ) +                      − λNt >  do
              |                                ∂u(Nt )
              |                                                        2
              |  4:   for j = 1, 2, . . . , m do
              |  5:     Compute
              |                                                                ∂λ(0)
              |                                                     wj =             vj − vj
              |                                                                ∂λNt
              |       as the solution of the adjoint sensitivity equations (D.84)
              |  6:        for i = 1, . . . , j do
              |  7:          hij = (wj , vi )
              |  8:          wj = wj − hij vi
              |  9:        end for
              | 10:        hj+1,j = kwj k2
              | 11:        vj+1 = wj /hj+1,j
              | 12:     end for
              | 13:     Compute
              |                                         ym = arg min kβe1 − Hm yk2 ,
              |     where e1 is the first canonical until vector in RNu and H = {hij }1≤i≤m+1,1≤j≤m
              | 14:   Update solution
              |                                            λNt = λNt ,0 + Vm ym
              |       where                                                        
              |                                             Vm = v1    ···     vm
              | 15:   end while
              | 16:   Define adjoint periodic final condition
blank         | 
text          |                                                 λ(Nt ) = λNt
blank         | 
              | 
              | 
text          |       With the solution of the fully discrete primal and dual time-periodic problems fully specified,
              | from numerical discretization to solution algorithms, we close this section with an algorithm that
              | uses the fully discrete adjoint method to compute the gradient of the quantity of interest on the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                        283
blank         | 
              | 
              | 
text          | manifold of periodic solutions. First, the fully discrete time-periodic solution (D.52), (D.54) must be
              |                                                                                                            (1)
              | computed, e.g., using a matrix-free Newton-Krylov method, to yield u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) .
              | Next, the corresponding fully discrete adjoint equations are defined about this periodic solution and
              |                                                                                                (1)
              | solved, e.g., using a matrix-free Krylov method, for λ(0) , . . . , λ(Nt ) , κ1 , . . . , κ(N
              |                                                                                            s
              |                                                                                               t)
              |                                                                                                  . Finally, (D.78)
              |                                             dF
              | is used to reconstruct the desired gradient    . This procedure is summarized in Algorithm 24.
              |                                             dµ
blank         | 
text          | Algorithm 24 Gradients on Manifold of Time-Periodic Solutions of PDEs
              | Input: Parameter               configuration,      µ,   and    fully    discrete quantity of interest,
              |                                 (1)          (N )
              |     F (u(0) , . . . , u(Nt ) , k1 , . . . , ks t )
              |                             dF
              | Output: Gradient,                , on manifold of time-periodic solutions
              |                             dµ
              |  1: For parameter µ, compute time-periodic solution of fully discrete PDE in (D.52), (D.54), e.g.,
              |     using the Newton-Krylov shooting method in Algorithm 22
              |                                                                          (1)
              |                                                 u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt )
              |                                                                       (1)              (N )
              |  2:   For fully discrete functional F (u(0) , . . . , u(Nt ) , k1 , . . . , ks t ), compute adjoint solution of fully
              |       discrete time-periodic PDE in (D.77), e.g., using GMRES shooting method in Algorithm 23
              |       with matrix-vector products computed from the backward evolution of the adjoint sensitivity
              |       equations in (D.84)
              |                                                                   (1)
              |                                         λ(0) , . . . , λ(Nt ) , κ1 , . . . , κs(Nt )
              |                      dF
              |  3:   Reconstruct       using dual variables according to (D.78)
              |                      dµ
blank         | 
              | 
              | 
title         | D.4.3        Generalized Reduced-Gradient Method for PDE Optimization with
text          |              Time-Periodicity Constraints
              | Consider the fully discrete time-dependent PDE-constrained optimization problem
blank         | 
text          |                                                                                      (1)
              |                              minimize            F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ)
              |                        (0)
              |                      u , ...,  u(Nt ) ∈RNu ,
              |                       (1)
              |                      k1 , ..., ks(Nt ) ∈RNu ,
              |                             µ∈RNµ
              |                                                                                      (1)
              |                      subject to                  c(u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ) ≥ 0
              |                                                  u(0) = u0                                                       (D.85)
              |                                                                             s
              |                                                                                        (n)
              |                                                                             X
              |                                                  u(n) = u(n−1) +                  bi ki
              |                                                                             i=1
blank         |                                                                                                     
text          |                                                     (n)                        (n)
              |                                                  M ki       = ∆tn r ui , µ, tn−1 + ci ∆tn
blank         | 
text          | where F is a fully discrete output functional of the partial differential equation and c is a vector
              | of such output functionals. The nested or Generalized Reduced-Gradient (GRG) approach to solve
              | (D.85) explicitly enforces the PDE constraint at each optimization iteration. The implicit function
              | theorem states that the solution of the discretized PDE, can be considered an implicit function of the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                           284
blank         | 
              | 
              | 
text          |                                                   (n)      (n)
              | parameter µ, i.e., u(n) = u(n) (µ) and ki               = ki (µ). Strict enforcement of the discretized partial
              | differential equation allows the PDE variables and equations to be removed from the optimization
              | problem
              |                                                                       (1)
              |                  minimize       F (u(0) (µ), . . . , u(Nt ) (µ), k1 (µ), . . . , ks(Nt ) (µ), µ)
              |                   µ∈RNµ
              |                                                                                                                  (D.86)
              |                                                                       (1)
              |                  subject to c(u(0) (µ), . . . , u(Nt ) (µ), k1 (µ), . . . , ks(Nt ) (µ), µ) ≥ 0.
blank         | 
text          |                                                                                       dF       dc
              | To solve this optimization problem using gradient-based techniques, the terms              and    —
              |                                                                                       dµ       dµ
              | gradients of quantities of interest along the manifold of solutions of the PDE—are required. Depend-
              | ing on the relative number of variables in µ to the number of constraints in c, the direct or adjoint
              | method can be efficiently used to compute these gradients without relying on finite differences.
              |    Now consider the optimization problem in (D.85) with the time-periodicity constraint added
blank         | 
text          |                                                                               (1)
              |                             minimize           F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ)
              |                       (0)
              |                     u , ...,  u(Nt ) ∈RNu ,
              |                      (1)
              |                     k1 , ..., ks(Nt ) ∈RNu ,
              |                            µ∈RNµ
              |                                                                              (1)
              |                     subject to                 c(u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ) ≥ 0
              |                                                u(0) = u(Nt )                                                     (D.87)
              |                                                                     s
              |                                                                                 (n)
              |                                                                     X
              |                                                u(n) = u(n−1) +              bi ki
              |                                                                     i=1
blank         |                                                                                      
text          |                                                   (n)             (n)
              |                                                M ki      = ∆tn r ui , µ, tn−1 + ci ∆tn .
blank         | 
text          | Strict enforcement of the time-periodic partial differential equations leads to an application of the
              |                                                                                              (n)      (n)
              | implicit function theorem, similar to that above, i.e., u(n) = u(n) (µ) and ki                     = ki (µ), where u(n)
              |       (n)
              | and ki      are the time-periodic solution of the discrete partial differential equations. This results in
              |                                                                                                                 (n)
              | an optimization problem identical to that in (D.86) with this new definition of u(n) (µ) and ki (µ).
              | The novel periodic adjoint method, derived in Section D.4.1, can be used to compute gradients along
              |                                                                         dF       dc
              | the manifold of time-periodic solutions of the fully discrete PDE, i.e.      and    , for the use in
              |                                                                         dµ       dµ
              | gradient-based optimization.
blank         | 
              | 
title         | D.4.4       Numerical Experiment: Time-Periodic Solutions of the Compress-
              |             ible Navier-Stokes Equations
text          | In this section, the various solvers discussed in this document for determining primal and dual time-
              | periodic solutions of partial differential equations are compared for a flapping airfoil in an isentropic,
              | viscous flow. The stability of the periodic orbit is verified by performing an eigenvalue analysis of
              | ∂u(Nt )
              |         . The section closes with validation of the adjoint method, introduced for efficient gradient
              |   ∂u0
              | computation of quantities of interest, against a second-order finite difference approximation.
              |    Consider the NACA0012 airfoil in Figure D.27 immersed in an isentropic, viscous flow with
              | Reynolds and Mach number set to 1000 and 0.2, respectively. The kinematic motion of the foil is
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                           285
blank         | 
              | 
              | 
text          |                 1                                                                     0.2
blank         | 
              | 
text          |      h(µ, t)
blank         | 
              | 
              | 
              | 
text          |                                                                            θ(µ, t)
              |                 0                                                                       0
blank         | 
              | 
text          |                −1                                                                    −0.2
              |                      0                2.5                 5                                 0                    2.5              5
              |                                      time                                                                       time
blank         | 
text          | Figure D.17: Trajectories of h(µ, t) and θ(µ, t) that define the motion of the airfoil in Figure D.27
              | and will be used to study primal and dual time-periodic solvers.
blank         | 
              | 
text          | parametrized with a single Fourier mode, i.e.,
blank         | 
text          |                                                h(µ, t) = Ah sin(ωh t + φh ) + ch
              |                                                                                                                                  (D.88)
              |                                                 θ(µ, t) = Aθ sin(ωθ t + φθ ) + cθ .
blank         | 
text          | The vector of parameters is fixed for the remainder of this section
              |     h                                                      i h                                                                i
              |  µ = Ah                                                                                           π                π
              |                     ωh    φh   ch    Aθ      ωθ   φθ     cθ = 1.0          0.4π       0.0   0.0   15       0.4π    2       0.0 , (D.89)
blank         | 
text          | and corresponds to the motion in Figure D.17 with period T = 5. The mapping G(X, t) from the
              | fixed reference domain V to the physical domain Ω(µ, t) takes the form of a parametrized rigid body
              | motion
              |                                          G(X, t) = v(µ, t) + Q(µ, t)(X − x0 ) + x0 ,                                             (D.90)
blank         | 
text          | where x0 is the location of pitching axis in the reference configuration (the 1/3 chord) and
              |                                      "                                 #                               "          #
              |                                           cos θ(µ, t)    sin θ(µ, t)                                        0
              |                          Q(µ, t) =                                                      v(µ, t) =                      .
              |                                          − sin θ(µ, t)   cos θ(µ, t)                                   h(µ, t)
blank         | 
text          | The isentropic Navier-Stokes equations are discretized with the discontinuous Galerkin scheme of
              | Section D.1.2 using 978 triangular p = 3 elements. No-slip boundary conditions are imposed on
              | the airfoil wall and characteristic free-stream boundary conditions at the far-field. The temporal
              | discretization uses a third-order diagonally implicit Runge-Kutta solver with 100 equally spaced
              | steps to discretize a single period of the motion. The airfoil and surrounding fluid vorticity field
              | are shown in Figures D.18 and D.19 with the flow field initialized from steady-state flow and the
              | time-periodic initial condition, respectively. It is clear that the flow in Figure D.19 will seamlessly
              | transition between periods. The initialization from the steady-state solution in Figure D.18 will
              | introduce non-physcial transients into the flow as discussed in the next section.
              |    First, the solvers introduced in Section D.3.1 are compared for different initial guesses for the
              | time-periodic initial condition. In the absence of any a priori information regarding the time-periodic
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                      286
blank         | 
              | 
              | 
              | 
text          | Figure D.18: Flow vorticity around heaving/pitching airfoil for simulation initialized from steady
              | state flow. Non-physical transients are introduced at the beginning of the time interval that re-
              | sult in non-trivial errors in integrated quantities of interests. Snapshots taken at times t =
              | 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
blank         | 
              | 
              | 
              | 
text          | Figure D.19: Time-periodic flow vorticity around heaving/pitching airfoil, i.e., initialized from pe-
              | riodic initial condition. The time-periodic initial condition ensures transients are not introduced
              | at the beginning of the simulation; the result is a seamless transition between periods, as would
              | be experienced in-flight, and trusted integrated quantities of interest. Snapshots taken at times
              | t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                              287
blank         | 
              | 
              | 
text          |                 102
blank         | 
text          |     2
              | u(Nt ) − u0
blank         | 
text          |               10−2
blank         | 
              | 
text          |               10−6
blank         | 
              | 
text          |               10−10
              |                        100         101        102    100         101        102    100         101        102
              |                       iterations (primal solves)    iterations (primal solves)    iterations (primal solves)
blank         | 
              | 
text          | Figure D.20: Convergence comparison for numerical solvers for fully discrete time-periodically con-
              | strained partial differential equations (D.52), (D.54), nonlinearly preconditioned with m fixed point
              | iterations. Left: m = 0, middle: m = 1, right: m = 5. Solvers: fixed point iteration (       ), steep-
              | est decent (     ), L-BFGS (      ), Newton-GMRES: ∆ = 10−2 (          ), ∆ = 10−3 (     ), ∆ = 10−4
              | (    ), where ∆ is the GMRES convergence tolerance. The optimization algorithms (steepest decent
              | and L-BFGS) were not included in the m = 0 study due to lack of convergence issues.
blank         | 
              | 
text          | solution, a reasonable initial guess is the steady-state flow. Since the problem under consideration
              | is being forced by an input—the periodic motion of the foil—a mechanism for improving the initial
              | guess is to simulate the flow field for m periods of the foil motion and use the final state of the
              | final period as the initial guess. This corresponds to using m iterations of fixed point iteration
              | (Algorithm 21) as a nonlinear preconditioner for the nonlinear system of equations (D.56) that
              | enforces time-periodicity of the flow.
              |       Figure D.20 and Table D.6 compare the solvers under consideration for different levels of nonlinear
              | preconditioning. Regardless of nonlinear preconditioning, the Newton-GMRES solver converges most
              | rapidly for a range of linear system tolerances from 10−2 to 10−4 and the optimization algorithms
              | (steepest decent and L-BFGS) converge most slowly. In fact, without any nonlinear preconditioning
              | the optimization algorithms fail to make progress toward the optimal solution and were not included
              | in the figure. Nonlinear preconditioning helps the Newton-GMRES algorithm most substantially,
              | particularly with m = 5, as this appears to place the initial guess close enough to the solution that
              | quadratic convergence is obtained from the outset. This causes the number of Newton iterations to
              | be reduced from 8 or 9 to 3 or 4. From Table D.6, this does not save many primal solvers—since
              | the nonlinear preconditioning requires primal solves—but requires far fewer linear system solves and
              | therefore fewer sensitivity solutions. Figure D.21 isolates the Newton-GMRES solver (for m = 0, i.e.,
              | the case without preconditioning) to highlight convergence rates for different GMRES tolerances. It
              | also shows the convergence of GMRES for each nonlinear iteration and each tolerance considered.
              | As expected, more GMRES iterations are required near convergence as it becomes more difficult to
              | reduce the linear residual the prescribed orders of magnitude.
              | Table D.6: Table summarizing performance of numerical solvers for fully discrete time-periodic partial differential equations, considering
              | nonlinear preconditioning via m fixed point iterations.
blank         | 
text          |             m=0                       u(Nt ) − u0   2
              |                                                         Primal Solves (D.55)   Sensitivity Solves (D.63)   Adjoint Solves (D.25)
              |             Fixed Point Iteration       8.10e-07                90                        0                         0
              |             Newton-Krylov (10−2 )       4.41e-08                9                        128                        0
              |             Newton-Krylov (10−3 )       1.60e-08                8                        156                        0
              |             Newton-Krylov (10−4 )       4.85e-10                8                        220                        0
              |             m=1                       u(Nt ) − u0   2
              |                                                         Primal Solves (D.55)   Sensitivity Solves (D.63)   Adjoint Solves (D.25)
              |             Fixed Point Iteration      8.10e-07                  90                       0                         0
              |             Steepest Decent            6.09e+00                 121                       0                        121
              |             L-BFGS                     1.36e+00                 121                       0                        121
              |             Newton-Krylov (10−2 )      1.96e-08                  8                       104                        0
              |             Newton-Krylov (10−3 )      2.69e-08                  7                       116                        0
              |             Newton-Krylov (10−4 )      1.77e-09                  7                       149                        0
              |             m=5                       u(Nt ) − u0   2
              |                                                         Primal Solves (D.55)   Sensitivity Solves (D.63)   Adjoint Solves (D.25)
              |             Fixed Point Iteration       8.10e-07                 90                       0                         0
              |             Steepest Decent             4.65e-01                125                       0                        125
              |             L-BFGS                      7.40e-02                125                       0                        125
meta          |             Newton-Krylov (10−2 )       3.50e-08                 10                      92                         0
              |             Newton-Krylov (10−3 )       7.18e-08                 9                       88                         0
              |             Newton-Krylov (10−4 )       5.61e-09                 9                       121                        0
              |                                                                                                                                              APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION
              |                                                                                                                                              288
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                             289
blank         | 
              | 
              | 
              | 
text          |                 102                                                                 102
blank         | 
              | 
text          |               10−1                                                                10−1
              |     2
blank         | 
              | 
              | 
              | 
text          |                                                                      kJ x − rk2
              | u(Nt ) − u0
blank         | 
              | 
              | 
              | 
text          |               10−4                                                                10−4
blank         | 
              | 
text          |               10−7                                                                10−7
blank         | 
              | 
text          |               10−10                                                               10−10
              |                           0        2       4      6      8                                0         20          40
              |                               iterations (primal solves)                                  iterations (linearized solves)
blank         | 
              | 
text          | Figure D.21: Linear and nonlinear convergence of Newton-GMRES method for determining fully
              | discrete time-periodic solutions with various linear system tolerances, ∆, i.e., kJ x − Rk < ∆, where
              | r and J are defined in (D.61) and (D.62). Tolerances considered: ∆ = 10−2 (         ), ∆ = 10−3 (  ),
              |         −4
              | ∆ = 10 (        ).
blank         | 
              | 
text          |       The time history of the instantaneous quantities of interest in Figure D.22 illustrate the non-
              | physical transients that result from initializing the flow with the steady-state solution. While the
              | transients mostly vanish after a single Newton iteration, the trajectories of these quantities of inter-
              | est do not coincide with those of the true time-periodic solution. The error between the integrated
              | quantities of interest—W and Jx —at the time-periodic flow versus intermediate iterations is shown
              | in Figure D.23. Comparing Figures D.20 and D.23, it can be seen that a tolerance of 10−8 on
              |  u(Nt ) − u(0)            2
              |                               leads to an accuracy of 10−6 in the integrated quantities of the time-periodic solu-
              | tion.
              |                                                                                                                     ∂u(Nt )
              |       Next, the stability of the periodic orbit is verified by considering the eigenvalues of                               ,
              |                                                                                                                      ∂u0
blank         | 
              | 
text          |                0                                                                     0
blank         | 
text          |               −2                                                                  −0.5
              |                                                                      Fxh
              |    Ph
blank         | 
              | 
              | 
              | 
text          |                                                                                    −1
              |               −4
              |                                                                                   −1.5
              |                       0             2           4                                         0         2           4
              |                                      time                                                            time
blank         | 
text          | Figure D.22: Time history of power, Fxh (u, µ, t), and x-directed force, P h (u, µ, t), after k Newton-
              | GMRES iterations (linear system convergence tolerance ∆ = 10−2 ) starting from steady-state.
              | Values of k: 0 (  ), 1 (    ), and 8 (    ).
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                             290
blank         | 
              | 
              | 
              | 
text          |                                                                        10−1
              |          10−1
              | |W − W ∗ |
blank         | 
              | 
              | 
              | 
text          |                                                              |Jx − Jx∗ |
              |          10−3                                                          10−3
blank         | 
              | 
text          |          10−5                                                          10−5
blank         | 
              | 
text          |          10−7                                                          10−7
              |                 0      2         4        6                                     0   2         4     6
              |                            iteration                                                    iteration
blank         | 
text          | Figure D.23: Convergence of fully discrete quantities of interest to their values at the time-periodic
              | solution, W ∗ and Jx∗ , for various solvers, without nonlinear preconditioning. Solvers: Newton-
              | GMRES: ∆ = 10−2 (         ), ∆ = 10−3 (     ), ∆ = 10−4 (     ), where ∆ is the GMRES convergence
              | tolerance.
blank         | 
              | 
text          | evaluated at the time-periodic solution. As discussed in Section D.3.2 and many prior works [47, 112],
              | the periodic orbit is stable if all eigenvalues of this matrix have modulus less than unity. Figure D.24
              | shows that the 200 eigenvalues of largest modulus lie within the unit circle in the complex plane;
              | thus, the periodic orbit is stable for this problem.
blank         | 
text          |                                           1
blank         | 
              | 
text          |                                         0.5
              |                                 =(λ)
blank         | 
              | 
              | 
              | 
text          |                                           0
blank         | 
              | 
text          |                                        −0.5
blank         | 
              | 
text          |                                         −1
              |                                               −1      −0.5       0            0.5   1
              |                                                              <(λ)
blank         | 
              | 
text          |                                                    (Nt )
              | Figure D.24: First 200 eigenvalues ( ) of ∂u∂u0 —evaluated at periodic solution—with largest mag-
              | nitude. All eigenvalues lie in unit circle, thus the periodic orbit is stable.
blank         | 
text          |       This completes the discussion of the primal time-periodic problem and attention is turned to the
              | dual, or adjoint, problem. First, a brief comparison of two potential solvers—fixed point iteration
              | and GMRES—for the periodic adjoint equation is provided. In contrast to the primal problem, there
              | is a less pronounced difference between the convergence of fixed point iteration and the Krylov solver
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                       291
blank         | 
              | 
              | 
              | 
text          |                     100                                                       100
              |      kAx − b1 k2
blank         | 
              | 
              | 
              | 
text          |                                                                kAx − b2 k2
              |                    10−3                                                      10−3
blank         | 
text          |                    10−6                                                      10−6
blank         | 
text          |                    10−9                                                      10−9
              |                           0        50        100        150                         0        50         100       150
              |                       iterations (adjoint linearized solves)                    iterations (adjoint linearized solves)
blank         | 
              | 
text          | Figure D.25: GMRES convergence for determining solution of adjoint equations corresponding to
              | fully discrete time-periodic partial differential equation, i.e., a linear two-point boundary value prob-
              |                                      ∂W                  ∂Jx
              | lem. A defined in (D.81), b1 =              , and b2 =            from (D.80), where W is fully discrete
              |                                    ∂u(Nt )              ∂u(Nt )
              | approximation of the total work done by fluid on airfoil and Jx is the x-directed impulse. Solvers:
              | fixed point iteration (     ) and GMRES (           ). The linearization is performed about the time-
              | periodic solution obtained with Newton-Krylov (∆ = 10−4 ) method.
blank         | 
              | 
text          | in the dual problem. Figure D.25 shows the convergence history for two different right-hand sides
              | of Ax = b, each corresponding to the adjoint method for a different quantity of interest. However,
              | it should be noted that the iterations for the GMRES solver are cheaper than those of the fixed
              |                           ∂F
              | point solver as the terms    —which may be expensive if µ is a large vector—are not computed.
              |                           ∂µ
              | Therefore, the GMRES algorithm is superior to fixed point iterations as there are fewer required
              | iterations, each of which is cheaper.
              |    Finally, the adjoint method for computing gradients of quantities of interest on the manifold of
              | time-periodic solutions of the partial differential equations is verified against a second-order finite
              | difference approximations. The finite difference approximation to gradients on the aforementioned
              | manifold requires finding the time-periodic solution of the governing equations at perturbations about
              | the nominal parameter configuration in (D.89). Figure D.26 shows the relative error between the
              | gradients computed via the adjoint method in Algorithm 24 to this finite difference approximation
              | for a sweep of finite difference intervals, τ . To realize the sub-10−6 finite difference errors in the
              | time-periodic gradient, tolerances of 10−12 were used for the primal and dual time-periodic solutions.
              | As expected, the error starts to increase after τ drops too small due to the trade-off between finite
              | difference accuracy and round-off error.
              |    Given this exposition on solvers for time-periodically constrained partial differential equations,
              | we turn our attention to deriving the corresponding fully discrete adjoint equations.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                      292
blank         | 
              | 
              | 
              | 
text          |        10−3
blank         | 
              | 
              | 
              | 
text          |                                                              2
              |    2
blank         | 
              | 
              | 
              | 
text          |                                                         ∆Jx
              |  ∆W
blank         | 
              | 
              | 
              | 
text          |                                                         ∆µ
              |  ∆µ
blank         | 
              | 
              | 
              | 
text          |                                                                  10−4
              |             −4
              |        10
blank         | 
              | 
              | 
              | 
text          |                                                         /
              |  /
blank         | 
              | 
              | 
              | 
text          |                                                              2
              |    2
blank         | 
              | 
              | 
              | 
text          |                                                         ∆Jx
              |  ∆W
              |  ∆µ
blank         | 
              | 
              | 
              | 
text          |                                                         ∆µ
              |        10−5                                                      10−5
              |  −
blank         | 
              | 
              | 
              | 
text          |                                                         −
              |                                                         dJx
              |  dW
              |  dµ
blank         | 
              | 
              | 
              | 
text          |                                                         dµ
              |        10−6
              |                                                                  10−6
              |                  10−810−710−610−510−410−310−2                           10−810−710−610−510−410−310−2
              |                                τ                                                      τ
blank         | 
              | 
text          | Figure D.26: Verification of periodic adjoint-based gradient with second-order centered finite dif-
              | ference approximation, for a range of finite intervals, τ . The computed gradient match the finite
              | difference approximation to nearly 7 digits before round-off errors degrade the accuracy.
blank         | 
              | 
              | 
              | 
text          | Table D.7: Comparison of non-zero derivatives of total energy, W , and x-impulse, Jx , computed
              | with the adjoint method and a second-order finite difference approximation with step size τ = 10−6 .
blank         | 
text          |                              ∂W                 ∂W                      ∂W               ∂W
              |                              ∂Ah                ∂ωh                     ∂Aθ              ∂cθ
blank         | 
text          |  Adjoint                -2.30919016e+01   -2.593579090e+01       -7.99568107e+00   5.881595017e-01
              |  Finite difference      -2.30919013e+01   -2.593579395e+01       -7.99568151e+00   5.881594917e-01
              |                              ∂Jx                ∂Jx                     ∂Jx              ∂Jx
              |                              ∂Ah                ∂ωh                     ∂Aθ              ∂cθ
blank         | 
meta          |  Adjoint                -1.85436790e-01   -1.029830753e-01       6.72970822e+00    1.270106907e-02
              |  Finite difference      -1.85436774e-01   -1.029834126e-01       6.72970891e+00    1.270112956e-02
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                     293
blank         | 
              | 
              | 
title         | D.4.5       Numerical Experiment: Energetically Optimal Flapping with Thrust
              |             and Time-Periodicity Constraints
text          | This section will apply the novel, fully discrete, periodic adjoint method to solve an optimal con-
              | trol problem governed by the time-periodically constrained isentropic compressible Naiver-Stokes
              | equations. The system of PDEs is discretized using a nodal discontinuous Galerkin (DG) method
              | on unstructured meshes of triangles, with polynomial degrees 3 within each element. The viscous
              | fluxes are chosen according to the compact DG method [150] method, and our implementation
              | is fully implicit with exact Jacobian matrices and a range of parallel iterative solvers [153]. The
              | resulting semi-discrete system has the form of our general system of ODEs (D.13). All partial
              | derivatives of the semi-discrete governing equations and corresponding quantities of interest, namely
              | ∂r ∂r ∂fh ∂fh
              |     ,    ,      ,     are computed via automatic symbolic differentiation at the element-level with
              | ∂u ∂µ ∂u ∂µ
              | the MAPLE software [126] and subsequent
              |                                    Z           assembly. The semi-discrete quantity of interest fh is
              | defined as the approximation of           f (U , µ, t) dS in (D.51) using the DG shape functions and
              |                                              Γ(µ, t)
              | required, along with the temporal discretization scheme, to compute the discrete output functional
              | F in (D.53). Additional details regarding computation of the partial derivatives with respect to µ
              | in the case of a parametrized, deforming domain are provided in Section D.2.4 and [211].
blank         | 
              | 
text          |                                                                            l
              |                                                                l/3
blank         | 
text          |                                                θ(t)
blank         | 
text          |                                                       x(t)
              |                                                               y(t)
blank         | 
              | 
text          |     Figure D.27: Kinematic description of body under consideration, NACA0012 airfoil (right).
blank         | 
text          |     The remainder of this section will consider the time-periodic solution and optimization of a
              | flapping NACA0012 airfoil, shown in Figure D.27. Two quantities of interest that will be considered
              | are the total work exerted by the fluid on the airfoil, W, and the impulse in the x-direction imparted
              | on the airfoil by the fluid, Jx , which take the form
              |                Z     T   Z                                                          Z       T   Z
              |  W(U , µ) =                  f (U , µ, t)· ẋ dS dt          and     Jx (U , µ) =                   f (U , µ, t)·e1 dS dt (D.91)
              |                  0       Γ                                                              0       Γ
blank         | 
              | 
text          | In this case, Γ is the surface of the airfoil, e1 ∈ Rnsd is the 1st canonical unit vector, f (U , µ, t) ∈ Rnsd
              | is the instantaneous force that the fluid exerts on the airfoil, and ẋ is the pointwise velocity of airfoil.
              | The solver-consistent discretization, discussed in Section 2.1.4 and [211], of these quantities results in
              |                                                                      (1)       (Nt )                                         (1)         (Nt )
              | the fully discrete approximations W (u(0) , . . . , u(Nt ) , k1 , . . . , ks           , µ) and Jx (u(0) , . . . , u(Nt ) , k1 , . . . , ks      , µ).
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                 294
blank         | 
              | 
              | 
text          | The instantaneous quantities of interest corresponding to those in (D.91) are the power and x-
              | directed force the fluid exerts on the airfoil, which take the form
              |                        Z                                                   Z
              |          P(U , µ, t) =    f (U , µ, t) · ẋ dS   and       Fx (U , µ, t) =   f (U , µ, t) · e1 dS.
              |                           Γ                                                             Γ
blank         | 
              | 
text          | Define P h (u, µ, t) and Fxh (u, µ, t) as the solver-consistent semi-discretization of these instantaneous
              | quantities of interest.
              |    In this section, the periodic adjoint method is used to solve an optimal control problem with time-
              | periodicity constraints using gradient-based optimization techniques. The optimization problem is
              | to determine the energetically optimal flapping motion of the NACA0012 airfoil in isentropic, viscous
              | flow—over a single representative, in-flight period—such that the x-directed impulse on the body is
              | identically 0. The continuous form of the optimal control problem is given as
blank         | 
text          |                             minimize          W(U , µ)
              |                                U, µ
blank         | 
text          |                             subject to Jx (U , µ) = 0
              |                                                                                                           (D.92)
              |                                               U (x, 0) = U (x, T )
              |                                               ∂U
              |                                                  + ∇ · F (U , ∇U ) = 0            in Ω(µ, t).
              |                                               ∂t
blank         | 
text          | After spatial and temporal discretization via the high-order discontinuous Galerkin and diagonally
              | implicit Runge-Kutta schemes in Section 2.1.3, the continuous optimization problem in (D.92) is
              | replaced with its fully discrete counterpart
blank         | 
text          |                                                                            (1)
              |                         minimize              W (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ)
              |                   u(0) , ..., u(Nt ) ∈RNu ,
              |                    (1)
              |                   k1 , ..., ks(Nt ) ∈RNu ,
              |                            µ∈RNµ
              |                                                                            (1)
              |                   subject to                  Jx (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , µ) = 0
              |                                               u(0) = u(Nt )                                               (D.93)
              |                                                                   s
              |                                                                             (n)
              |                                                                   X
              |                                               u(n) = u(n−1) +           bi ki
              |                                                                   i=1
blank         |                                                                                    
text          |                                                  (n)            (n)
              |                                               M ki     = ∆tn r ui , µ, tn−1 + ci ∆tn .
blank         | 
text          | The physical and numerical setup are identical to that in Section D.4.4 with the exception of the
              | kinematic parametrization. Instead of a single Fourier mode, the kinematic motion is parametrized
              | by cubic splines with 5 equally spaced knots and boundary conditions that enforce
blank         | 
text          |                                               h(µ, t) = −h(µ, t + T /2)
              |                                                                                                           (D.94)
              |                                                θ(µ, t) = −θ(µ, t + T /2)
blank         | 
text          | where t is time and T = 5 is the fixed period of the flapping motion. The vector of parameters,
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                     295
blank         | 
              | 
              | 
text          | µ—used as optimization parameters—are the knots of the cubic splines. This leads to Nµ = 8 pa-
              | rameters; 4 knots for the motion of h(µ, t) and θ(µ, t)2 . Notice that (D.94) enforces the trajectories
              | of h(µ, t) and θ(µ, t) in [T /2, T ] to be the mirror of those in [0, T /2], which implicitly enforces
              | periodicity with period T . The mapping G from the reference to physical domain required for the
              | DG-ALE formulation is defined in (D.90) with the new definition of h(µ, t) and θ(µ, t) with periodic
              | cubic splines.
              |     The optimization problem in (D.93) is solved using the extension of the nested framework for
              | PDE-constrained optimization, or generalized reduced-gradient method, introduced in Section D.4.3.
              | The solvers introduced in Section D.3.1 will be used to determine the time-periodic flow around the
              | airfoil. Given the results in the previous section, the Newton-GMRES method with a tolerance of
              | ∆ = 10−3 , warm-started from m = 5 fixed-point iterations is employed. The flow is deemed to be
              | periodic if
              |                                              u(0) − u(Nt )       ≤ 10−10 .                                   (D.95)
              |                                                              2
blank         | 
text          | The periodic flow is used to compute quantities of interest—the total work and x-impulse. Then,
              | the periodic adjoint method will be used to compute gradients of the quantities of interest along
              | the manifold of time-periodic solutions of the governing equation. GMRES is used to solve the dual
              | linear, periodic adjoint equations with a tolerance of ∆ = 10−4 . Since there are two quantities of
              | interest, two periodic adjoint solves must be performed at each optimization iteration. Finally, the
              | quantities of interest and their gradients are passed to an optimization solver—SNOPT [70] is used
              | in this work—and progress is made toward a local minimum.
              |     The initial condition for the optimization solver is shown in Figure D.28; the heaving motion is
              | a sinusoid with amplitude 1 and there is no pitch—pure heaving motion. The vorticity snapshots in
              | Figure D.31 show this motion induces a fairly violent flow with shedding vortices. The corresponding
              | time history of the power, P h (u, µ, t), and x-directed force, Fxh (u, µ, t), imparted onto the airfoil
              | by the fluid are shown in Figure D.29. After 16 periodic optimization iterations, the first-order
              | optimality conditions have been reduced by two orders of magnitude. From Figure D.28, the optimal
              | airfoil motion is a combination of heaving and pitching. From the initial guess, the amplitude of
              | the heaving motion has been reduced by more than a factor of two and the pitching amplitude
              | increased to 18.7◦ . The convergence history for the optimization solver is given in Figure D.30. At
              | the optimal solution, the total work required to perform the flapping motion is more than an order of
              | magnitude smaller than at the initial guess (pure heaving). Figures D.31 and D.32 show snapshots
              | of the flow in time at the initial, purely heaving motion and the optimal flapping motion. From
              | these figures, it is clear that the flow corresponding to the optimal motion is relatively benign with
              | no shedding vortices, which explains the reduction in required work. The efficiency of combined
              | pitching and heaving has been repeatedly observed experimentally [191, 162, 158] and the phase
              | angle of approximately 90◦ between pitching and heaving, as observed in Figure D.28, has also been
              | observed in experiments [191, 162, 158, 148]. The specific pitching and heaving amplitudes were
              |    2 There are only 4 degrees of freedom since the mirror boundary condition in (D.94) prescribes the value of one of
blank         | 
text          | the knots given the other four.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                     296
blank         | 
              | 
              | 
text          |            1                                                         0.4
              | h(µ, t)                                                              0.2
blank         | 
              | 
              | 
              | 
text          |                                                           θ(µ, t)
              |            0                                                             0
blank         | 
text          |                                                                     −0.2
              |           −1
              |                                                                     −0.4
              |                    0       2        4                                            0           2            4
              |                             time                                                              time
blank         | 
text          | Figure D.28: Trajectories of h(µ, t) and θ(µ, t) at initial guess (                  ) and optimal solution (     )
              | for optimization problem in (D.93).
blank         | 
text          |            0.1                                                       0
blank         | 
text          |                0
              |                                                                     −2
blank         | 
              | 
              | 
              | 
text          |                                                           Fxh
              | Ph
blank         | 
              | 
              | 
              | 
text          |           −0.1
blank         | 
text          |           −0.2                                                      −4
blank         | 
text          |                        0    2           4                                    0             2          4
              |                              time                                                           time
blank         | 
text          | Figure D.29: Time history of the power, P h (u, µ, t), and x-directed force, Fxh (u, µ, t), imparted onto
              | foil by fluid at initial guess ( ) and optimal solution (       ) for optimization problem in (D.93).
blank         | 
              | 
text          | determined by the optimizer such that the thrust constraint is satisfied; if the thrust requirement
              | was increased, these magnitudes would increase and result in a more violent flow field, eventually
              | leading to vortex shedding [191, 211].
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                       297
blank         | 
              | 
              | 
              | 
text          |       0                                                       0.2
blank         | 
text          |                                                                 0
blank         | 
              | 
              | 
              | 
text          |                                                         Jx
              | W
blank         | 
              | 
              | 
              | 
text          |      −5                                                      −0.2
blank         | 
text          |                                                              −0.4
blank         | 
text          |     −10                                                      −0.6
              |            0       5       10       15                              0       5       10       15
              |                optimization iteration                                   optimization iteration
blank         | 
text          | Figure D.30: Convergence of quantities of interest, W and Jx , with optimization iteration. Each
              | optimization iteration requires a periodic flow computation and its corresponding adjoint to evaluate
              | the quantities of interest and their gradients.
blank         | 
              | 
              | 
              | 
text          | Figure D.31: Trajectory of airfoil and flow vorticity at initial guess for optimization (pure heaving
              | motion, see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        298
blank         | 
              | 
              | 
              | 
text          | Figure D.32: Trajectory of airfoil and flow vorticity at energetically optimal, zero-impulse flapping
              | motion (see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
blank         | 
              | 
title         | D.5      Conclusion
text          | This appendix discussed a fully discrete framework for computing time-periodic solutions of partial
              | differential equations. The discussion included the spatio-temporal discretization of the governing
              | equations and a slew of time-periodic shooting solvers, including optimization-based and Newton-
              | Krylov methods. These shooting methods consider the state at the final time to be a nonlinear
              | function of the initial condition and solve u(Nt ) (u0 ) = u0 using Newton-Raphson iterations or
              | optimization techniques to minimize its norm. The linear system of equations, arising in the Newton-
              | Raphson iterations, were solved using matrix-free GMRES with matrix-vector products computed
              | as the solution of the linearized, sensitivity equations (with appropriate initial condition). The
              | adjoint method was used to compute the gradients in the gradient-based optimization solvers. These
              | periodic solvers were used to compute the time-periodic flow around a flapping airfoil in isentropic,
              | compressible, viscous flow, and their performance compared. The Newton-Krylov solver exhibits
              | superior convergence to the optimization-based shooting methods, even when inexact tolerances
              | were used on the linear system solves, and fully leverages quality starting guesses. An eigenvalue
              | analysis is provided to show the periodic orbit of the flapping problem is stable.
              |    The main contribution of the document is the derivation of the adjoint equations corresponding
              | to the fully discrete time-periodically constraint partial differential equations. As opposed to the
              | backward-in-time evolution equations, these equations constitute a linear, two-point boundary value
              | problem that is provably solvable. The corresponding adjoint method was introduced for computing
              | exact gradients of quantities of interest along the manifold of time-periodic solutions of the discrete
              | conservation law. The gradients were verified against a second-order finite difference approxima-
              | tion. These quantities of interest and their gradients were used in the context of gradient-based
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                         299
blank         | 
              | 
              | 
text          | optimization to solve an optimal control problem with time-periodicity constraints, among others.
              | In particular, the energetically optimal flapping motion of a 2D airfoil in time-periodic, isentropic,
              | compressible, viscous flow that generates a prescribed time-averaged thrust is sought. The proposed
              | framework improves the nominal flapping motion by reducing the flapping energy nearly an order
              | of magnitude and exactly satisfies the thrust constraint.
              |    While this work is an initial step toward problems of engineering and scientific relevance, addi-
              | tional development will be required to solve truly impactful problems. One extension of this work is
              | the development of robust solvers for determining nearly time-periodic solutions of problems where
              | a time-periodic solution does not exist, but exhibits quasi-cyclic behavior. An example of such a
              | problem is the 3D turbulent flow around periodically driven bodies such as helicopter and windmill
              | blades. Another extension will be the development of faster numerical solvers to reduce the cost
              | of computing time-periodic solutions or solving optimization problems with time-periodicity con-
              | straints. For example, economical, matrix-free preconditioners could result in non-trivial speedups
              | for the Newton-Krylov time-periodicity solver and Krylov solver for the periodic adjoint equations.
              | Model order reduction techniques could dramatically reduce the cost of computing the solution of
              | the primal partial differential equations, and consequently the entire time-periodic solver.
blank         | 
              | 
title         | D.6      Existence and Uniqueness of Solutions of the Adjoint
text          |          Equations of the Fully Discrete, Time-Periodically Con-
              |          strained Partial Differential Equations
              | This section proves existence and uniqueness of solutions of the adjoint equations of the fully dis-
              | crete, time-periodically constrained partial differential equation. The strategy is to show the linear
              | operator that encapsulates them is the transpose of the linear operator that defines the fully discrete,
              | sensitivity equations, which is assumed non-singular at a time-periodic solution.
              |    Consider the initial-value problem (D.55), with the initial condition parametrized by µ,
blank         | 
text          |                                  u(0) = u0 (µ)
              |                                                    s
              |                                                              (n)
              |                                                    X
              |                                  u(n) = u(n−1) +         bi ki                                   (D.96)
              |                                                    i=1
blank         |                                                                     
text          |                                   (n)            (n)
              |                               M ki      = ∆tn r ui , µ, tn−1 + ci ∆tn .
blank         | 
text          | The fully discrete adjoint equations corresponding to the primal equation in (D.96) and the discrete
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                           300
blank         | 
              | 
              | 
text          |                                                             (1)            (Nt )
              | quantity of interest, F (u(0) , . . . , u(Nt ) , k1 , . . . , ks                   , µ) are
blank         | 
text          |                                 ∂F T
              |               ν (Nt ) =
              |                                ∂u(Nt )
              |                                                              s
              |                  (n−1)          (n)       ∂F T X           ∂r  (n)                     T
              |                                                                                             (n)
              |              ν           =ν           +         +     ∆t n     ui   , µ, tn−1 + ci ∆t n    τi                      (D.97)
              |                                         ∂u(n−1)   i=1
              |                                                            ∂u
              |                                         T                  s
              |                    (n)          ∂F                         X               ∂r  (n)                 T
              |                                                                                                        (n)
              |           MT τi          =        (n)
              |                                             + bi ν (n) +         aji ∆tn       uj , µ, tn−1 + cj ∆tn τ j ,
              |                                ∂ki                         j=i
              |                                                                            ∂u
blank         | 
text          | and the gradient of the quantity of interest can be reconstructed as
blank         | 
text          |                                             Nt      s
              |                    dF   ∂F         T ∂u0   X       X    (n) T ∂r   (n)
              |                       =    + ν (0)       +     ∆tn     τi        (ui , µ, tn−1 + ci ∆tn ),                         (D.98)
              |                    dµ   ∂µ           ∂µ    n=1     i=1
              |                                                               ∂µ
blank         | 
text          |                          (n)
              | where ν (n) and τ i            are the Lagrange multipliers. These equations can be obtained using an identical
              | derivation to that in Section D.4.1; see [211]. At this point, take F = v T u(Nt ) and µ = u0 for a
              | fixed, arbitrary vector v ∈ RNu . For this selection of F and µ, the above equations reduce to
blank         | 
text          |                                ν (Nt ) = v
              |                                                     s
              |                                                     X             ∂r  (n)                 T
              |                                                                                               (n)
              |                            ν (n−1) = ν (n) +               ∆tn        ui , µ, tn−1 + ci ∆tn τ i
              |                                                     i=1
              |                                                                   ∂u                                               (D.99)
              |                                                        s
              |                                  (n)
              |                                                       X                  ∂r  (n)                 T
              |                                                                                                      (n)
              |                          MT τi          = bi ν (n) +         aji ∆tn         uj , µ, tn−1 + cj ∆tn τ j
              |                                                        j=i
              |                                                                          ∂u
blank         | 
text          | and
              |                                                                                T
              |                                                        dF T   ∂u(Nt )
              |                                                             =         v = ν (0) .                                 (D.100)
              |                                                        dµ      ∂u0
blank         | 
text          |                                                                                                     ∂λ(0)
              | The equations in (D.99) defining ν (0) are identical to those in (D.79) defining                          , which leads to
              |                                                                                                     ∂λNt
              | the relation
              |                                                                      T
              |                                                            ∂u(Nt )    ∂λ(0)
              |                                                                    v=       v                                     (D.101)
              |                                                             ∂u0       ∂λNt
              | for any v. Thus, it can be concluded that
blank         | 
text          |                                                                                        T
              |                                                              ∂λ(0)   ∂u(Nt )
              |                                                                    =         .                                    (D.102)
              |                                                              ∂λNt     ∂u0
blank         | 
text          |                                                     ∂u(Nt )
              | Since the Jacobian of the time-periodic residual,           − I, is non-singular at a time-periodic
              |                                                      ∂u0
              |                                                                                ∂λ(0)
              | solution, the matrix defining the linear, two-point boundary value problem,          − I must also
              |                                                                                ∂λNt
              | be non-singular. Thus, a solution of the linear, two-point boundary value problem exists and is
              | unique.
title         | Bibliography
blank         | 
ref           |  [1] Anshul Agarwal and Lorenz T Biegler. A trust-region framework for constrained optimization
              |     using reduced order modeling. Optimization and Engineering, 14(1):3–35, 2013.
blank         | 
ref           |  [2] Volkan Akcelik, George Biros, Omar Ghattas, Judith Hill, David Keyes, and Bart van Bloe-
              |     men Waanders. Parallel algorithms for PDE-constrained optimization. Parallel Processing for
              |     Scientific Computing, 20:291, 2006.
blank         | 
ref           |  [3] Roger Alexander. Diagonally implicit Runge-Kutta methods for stiff ODE’s. SIAM Journal
              |     on Numerical Analysis, 14(6):1006–1021, 1977.
blank         | 
ref           |  [4] Natalia M Alexandrov, John E Dennis Jr, Robert Michael Lewis, and Virginia Torczon. A
              |     trust-region framework for managing the use of approximation models in optimization. Struc-
              |     tural Optimization, 15(1):16–23, 1998.
blank         | 
ref           |  [5] Natalia M Alexandrov and Robert Michael Lewis. An overview of first-order model manage-
              |     ment for engineering optimization. Optimization and Engineering, 2(4):413–430, 2001.
blank         | 
ref           |  [6] Natalia M Alexandrov, Robert Michael Lewis, Clyde R Gumbert, Lawrence L Green, and
              |     Perry A Newman. Approximation and model management in aerodynamic optimization with
              |     variable-fidelity models. Journal of Aircraft, 38(6):1093–1101, 2001.
blank         | 
ref           |  [7] David M Ambrose and Jon Wilkening.          Computation of time-periodic solutions of the
              |     Benjamin-Ono equation. arXiv preprint arXiv:0804.3623, 2008.
blank         | 
ref           |  [8] David M Ambrose and Jon Wilkening. Computation of symmetric, time-periodic solutions
              |     of the vortex sheet with surface tension. Proceedings of the National Academy of Sciences,
              |     107(8):3361–3366, 2010.
blank         | 
ref           |  [9] George R Anderson, Michael J Aftosmis, and Marian Nemec. Parametric deformation of
              |     discrete geometry for aerodynamic shape design. AIAA Paper, 965, 2012.
blank         | 
ref           | [10] Eyal Arian, Marco Fahl, and Ekkehard W Sachs. Trust-region proper orthogonal decomposi-
              |     tion for flow control. Technical report, DTIC Document, 2000.
blank         | 
              | 
              | 
              | 
meta          |                                               301
              | BIBLIOGRAPHY                                                                                       302
blank         | 
              | 
              | 
ref           | [11] Douglas N Arnold, Franco Brezzi, Bernardo Cockburn, and L Donatella Marini. Unified
              |     analysis of discontinuous Galerkin methods for elliptic problems. SIAM Journal on Numerical
              |     Analysis, 39(5):1749–1779, 2002.
blank         | 
ref           | [12] Ivo Babuška, Fabio Nobile, and Raúl Tempone. A stochastic collocation method for elliptic
              |     partial differential equations with random input data. SIAM Review, 52(2):317–355, 2010.
blank         | 
ref           | [13] Ivo Babuska, Raúl Tempone, and Georgios E Zouraris. Galerkin finite element approximations
              |     of stochastic elliptic partial differential equations. SIAM Journal on Numerical Analysis,
              |     42(2):800–825, 2004.
blank         | 
ref           | [14] Maciej Balajewicz and Earl H Dowell. Stabilization of projection-based reduced order models
              |     of the navier–stokes. Nonlinear Dynamics, 70(2):1619–1632, 2012.
blank         | 
ref           | [15] Afonso S Bandeira, Katya Scheinberg, and Luı́s N Vicente. Convergence of trust-region meth-
              |     ods based on probabilistic models. SIAM Journal on Optimization, 24(3):1238–1264, 2014.
blank         | 
ref           | [16] Jernej Barbič and Doug L James. Real-time subspace integration for St. Venant-Kirchhoff
              |     deformable models. In ACM transactions on graphics (TOG), volume 24, pages 982–990.
              |     ACM, 2005.
blank         | 
ref           | [17] Maxime Barrault, Yvon Maday, Ngoc Cuong Nguyen, and Anthony T Patera. An empirical
              |     interpolation method: application to efficient reduced-basis discretization of partial differential
              |     equations. Comptes Rendus Mathematique, 339(9):667–672, 2004.
blank         | 
ref           | [18] Volker Barthelmann, Erich Novak, and Klaus Ritter. High dimensional polynomial interpola-
              |     tion on sparse grids. Advances in Computational Mathematics, 12(4):273–288, 2000.
blank         | 
ref           | [19] Ted Belytschko, Wing Kam Liu, Brian Moran, and Khalil Elkhodary. Nonlinear Finite Ele-
              |     ments for Continua and Structures. John wiley & sons, 2013.
blank         | 
ref           | [20] Martin Philip Bendsoe and Ole Sigmund. Topology Optimization: Theory, Methods, and
              |     Applications. Springer Science & Business Media, 2013.
blank         | 
ref           | [21] Gal Berkooz, Philip Holmes, and John L Lumley. The proper orthogonal decomposition in the
              |     analysis of turbulent flows. Annual Review of Fluid Mechanics, 25(1):539–575, 1993.
blank         | 
ref           | [22] A Borzı̀, V Schulz, C Schillings, and G Von Winckel. On the treatment of distributed uncer-
              |     tainties in PDE-constrained optimization. GAMM-Mitteilungen, 33(2):230–246, 2010.
blank         | 
ref           | [23] Alfio Borzı̀. Multigrid and sparse-grid schemes for elliptic control problems with random
              |     coefficients. Computing and Visualization in Science, 13(4):153–160, 2010.
blank         | 
ref           | [24] Alfio Borzı̀ and G von Winckel. Multigrid methods and sparse-grid collocation techniques
              |     for parabolic optimal control problems with random coefficients. SIAM Journal on Scientific
              |     Computing, 31(3):2172–2192, 2009.
meta          | BIBLIOGRAPHY                                                                                   303
blank         | 
              | 
              | 
ref           | [25] Matthew Brand. Incremental singular value decomposition of uncertain data with missing
              |     values. In European Conference on Computer Vision, pages 707–720. Springer, 2002.
blank         | 
ref           | [26] Matthew Brand. Fast low-rank modifications of the thin singular value decomposition. Linear
              |     Algebra and its Applications, 415(1):20–30, 2006.
blank         | 
ref           | [27] Martin Dietrich Buhmann. Radial basis functions. Acta Numerica 2000, 9:1–38, 2000.
blank         | 
ref           | [28] T. Bui-Thanh, K. Willcox, and O. Ghattas. Model reduction for large-scale systems with high-
              |     dimensional parametric input space. SIAM Journal on Scientific Computing, 30(6):3270–3288,
              |     2008.
blank         | 
ref           | [29] Hans-Joachim Bungartz and Michael Griebel. Sparse grids. Acta Numerica, 13:147–269, 2004.
blank         | 
ref           | [30] Yanzhao Cao, MY Hussaini, and HONGTAO Yang. Numerical optimization of radiated engine
              |     noise with uncertain wavenumbers. International Journal of Numerical Analysis and Modeling,
              |     4(3-4):392–401, 2007.
blank         | 
ref           | [31] Kevin Carlberg, Charbel Bou-Mosleh, and Charbel Farhat. Efficient non-linear model reduc-
              |     tion via a least-squares petrov–galerkin projection and compressive tensor approximations.
              |     International Journal for Numerical Methods in Engineering, 86(2):155–181, 2011.
blank         | 
ref           | [32] Kevin Carlberg and Charbel Farhat. A low-cost, goal-oriented compact proper orthogonal
              |     decompositionbasis for model reduction of static systems. International Journal for Numerical
              |     Methods in Engineering, 86(3):381–402, 2011.
blank         | 
ref           | [33] Kevin Thomas Carlberg. Model reduction of nonlinear mechanical systems via optimal projec-
              |     tion and tensor approximation. PhD thesis, Stanford University, 2011.
blank         | 
ref           | [34] Richard G Carter. Numerical optimization in Hilbert space using inexact function and gradient
              |     evaluations. 1989.
blank         | 
ref           | [35] Richard G Carter. On the global convergence of trust region algorithms using inexact gradient
              |     information. SIAM Journal on Numerical Analysis, 28(1):251–265, 1991.
blank         | 
ref           | [36] Richard G Carter. Numerical experience with a class of algorithms for nonlinear optimization
              |     using inexact function and gradient information. SIAM Journal on Scientific Computing,
              |     14(2):368–388, 1993.
blank         | 
ref           | [37] ASL Chan. The design of Michell optimum structures. Technical report, College of Aeronautics
              |     Cranfield, 1960.
blank         | 
ref           | [38] Tony F Chan and Wing Lok Wan. Analysis of projection methods for solving linear systems
              |     with multiple right-hand sides. SIAM Journal on Scientific Computing, 18(6):1698–1721, 1997.
blank         | 
ref           | [39] Peter C Chang and S Chi Liu. Recent research in nondestructive evaluation of civil infrastruc-
              |     tures. Journal of Materials in Civil Engineering, 15(3):298–304, 2003.
meta          | BIBLIOGRAPHY                                                                                 304
blank         | 
              | 
              | 
ref           | [40] I Charpentier. Checkpointing schemes for adjoint codes: Application to the meteorological
              |     model Meso-NH. SIAM Journal on Scientific Computing, 22(6):2135–2151, 2001.
blank         | 
ref           | [41] Saifon Chaturantabut and Danny C Sorensen. Nonlinear model reduction via discrete empirical
              |     interpolation. SIAM Journal on Scientific Computing, 32(5):2737–2764, 2010.
blank         | 
ref           | [42] Peng Chen and Alfio Quarteroni. Weighted reduced basis method for stochastic optimal control
              |     problems with elliptic PDE constraint. SIAM/ASA Journal on Uncertainty Quantification,
              |     2(1):364–396, 2014.
blank         | 
ref           | [43] Peng Chen and Alfio Quarteroni. A new algorithm for high-dimensional uncertainty quan-
              |     tification based on dimension-adaptive sparse grid approximation and reduced basis methods.
              |     Journal of Computational Physics, 298:176–193, 2015.
blank         | 
ref           | [44] Peng Chen, Alfio Quarteroni, and Gianluigi Rozza. Multilevel and weighted reduced basis
              |     method for stochastic optimal control problems constrained by Stokes equations. Numerische
              |     Mathematik, pages 1–36, 2013.
blank         | 
ref           | [45] Jintai Chung and GM Hulbert. A time integration algorithm for structural dynamics with
              |     improved numerical dissipation: the generalized-α method. Journal of applied mechanics,
              |     60(2):371–375, 1993.
blank         | 
ref           | [46] Bernardo Cockburn and Chi-Wang Shu. Runge–Kutta discontinuous Galerkin methods for
              |     convection-dominated problems. Journal of Scientific Computing, 16(3):173–261, 2001.
blank         | 
ref           | [47] Earl A Coddington and Norman Levinson. Theory of Ordinary Differential Equations. Tata
              |     McGraw-Hill Education, 1955.
blank         | 
ref           | [48] Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust Region Methods, volume 1. SIAM,
              |     2000.
blank         | 
ref           | [49] Arnaud Debussche, Marco Fuhrman, and Gianmario Tessitore. Optimal control of a stochastic
              |     heat equation with boundary-noise and boundary-control. ESAIM: Control, Optimisation and
              |     Calculus of Variations, 13(01):178–205, 2007.
blank         | 
ref           | [50] Jean-Antoine Désidéri and Ales Janka. Multilevel shape parameterization for aerodynamic
              |     optimization: Application to drag and noise reduction of transonic/supersonic business jet.
              |     In Proceedings of the European Congress on Computational Methods in Applied Sciences and
              |     Engineering, ECCOMAS 2004, 2004.
blank         | 
ref           | [51] Benoı̂t Desjardins, Emmanuel Grenier, P-L Lions, and Nader Masmoudi. Incompressible limit
              |     for solutions of the isentropic Navier–Stokes equations with dirichlet boundary conditions.
              |     Journal de Mathématiques Pures et Appliquées, 78(5):461–471, 1999.
meta          | BIBLIOGRAPHY                                                                                   305
blank         | 
              | 
              | 
ref           | [52] Markus A Dihlmann and Bernard Haasdonk. Certified PDE-constrained parameter optimiza-
              |     tion using reduced basis surrogate models for evolution problems. Computational Optimization
              |     and Applications, 60(3):753–787, 2015.
blank         | 
ref           | [53] Eusebius Doedel, Herbert B Keller, and Jean Pierre Kernevez. Numerical analysis and con-
              |     trol of bifurcation problems (II): Bifurcation in infinite dimensions. International Journal of
              |     Bifurcation and Chaos, 1(04):745–772, 1991.
blank         | 
ref           | [54] Arne Drud. CONOPT: A GRG code for large sparse dynamic nonlinear optimization problems.
              |     Mathematical Programming, 31(2):153–191, 1985.
blank         | 
ref           | [55] Thomas D Economon, Francisco Palacios, and Juan J Alonso. Unsteady continuous adjoint
              |     approach for aerodynamic design on dynamic meshes. AIAA Journal, 53(9):2437–2453, 2015.
blank         | 
ref           | [56] R. Everson and L. Sirovich. Karhunen–Loève procedure for gappy data. JOSA A, 12(8):1657–
              |     1664, 1995.
blank         | 
ref           | [57] Marco Fahl and Ekkehard W Sachs. Reduced order modelling approaches to PDE-constrained
              |     optimization based on proper orthogonal decomposition. In Large-scale PDE-constrained op-
              |     timization, pages 268–280. Springer, 2003.
blank         | 
ref           | [58] C. Farhat, C. Degand, B. Koobus, and M. Lesoinne. Torsional springs for two-dimensional
              |     dynamic unstructured fluid meshes. Computer Methods in Applied Mechanics and Engineering,
              |     163(1–4):231–245, 1998.
blank         | 
ref           | [59] Charbel Farhat, Philip Avery, Todd Chapman, and Julien Cortial. Dimensional reduction of
              |     nonlinear finite element dynamic models with finite rotations and energy-based mesh sampling
              |     and weighting for computational efficiency. International Journal for Numerical Methods in
              |     Engineering, 98(9):625–662, 2014.
blank         | 
ref           | [60] Charbel Farhat, Philippe Geuzaine, and Céline Grandmont. The discrete geometric conser-
              |     vation law and the nonlinear stability of ALE schemes for the solution of flow problems on
              |     moving grids. Journal of Computational Physics, 174(2):669–694, 2001.
blank         | 
ref           | [61] Gerald Farin. Curves and Surfaces for Computer-Aided Geometric Design: A Practical Guide.
              |     Elsevier, 2014.
blank         | 
ref           | [62] Alexander IJ Forrester, Neil W Bressloff, and Andy J Keane. Optimization using surrogate
              |     models and partially converged computational fluid dynamics simulations. In Proceedings of
              |     the Royal Society of London A: Mathematical, Physical and Engineering Sciences, volume 462,
              |     pages 2177–2204. The Royal Society, 2006.
blank         | 
ref           | [63] Alexander IJ Forrester and Andy J Keane. Recent advances in surrogate-based optimization.
              |     Progress in Aerospace Sciences, 45(1):50–79, 2009.
meta          | BIBLIOGRAPHY                                                                                  306
blank         | 
              | 
              | 
ref           | [64] Bradley M Froehle. High-order discontinuous Galerkin fluid-structure interaction methods.
              |     PhD thesis, University of California, Berkeley, 2013.
blank         | 
ref           | [65] Michel Géradin and Daniel J Rixen. Mechanical Vibrations: Rheory and Application to Struc-
              |     tural Dynamics. John Wiley & Sons, 2014.
blank         | 
ref           | [66] Thomas Gerstner and Michael Griebel. Numerical integration using sparse grids. Numerical
              |     Algorithms, 18(3-4):209–232, 1998.
blank         | 
ref           | [67] Thomas Gerstner and Michael Griebel. Dimension–adaptive tensor–product quadrature. Com-
              |     puting, 71(1):65–87, 2003.
blank         | 
ref           | [68] P. Geuzaine, G. Brown, C. Harris, and C. Farhat. Aeroelastic dynamic analysis of a full F-16
              |     configuration for various flight conditions. AIAA Journal, 41:363–371, 2003.
blank         | 
ref           | [69] Omar Ghattas and Jai-Hyeong Bark. Optimal control of two-and three-dimensional incom-
              |     pressible Navier–Stokes flows. Journal of Computational Physics, 136(2):231–244, 1997.
blank         | 
ref           | [70] Philip E Gill, Walter Murray, and Michael A Saunders. SNOPT: An SQP algorithm for
              |     large-scale constrained optimization. SIAM Review, 47(1):99–131, 2005.
blank         | 
ref           | [71] Philip E Gill, Walter Murray, and Margaret H Wright. Practical optimization. 1981.
blank         | 
ref           | [72] Victor Giurgiutiu and Adrian Cuc. Embedded non-destructive evaluation for structural health
              |     monitoring, damage detection, and failure prevention. Shock and Vibration Digest, 37(2):83,
              |     2005.
blank         | 
ref           | [73] Tuhfe Göçmen and Barış Özerdem. Airfoil optimization for noise emission problem and aero-
              |     dynamic performance criterion on small scale wind turbines. Energy, 46(1):62–71, 2012.
blank         | 
ref           | [74] Jedidiah Gohlke. Reduced Order Modeling for Optimization of Large Scale Dynamical Systems.
              |     PhD thesis, Rice University, 2013.
blank         | 
ref           | [75] Gene H Golub and Charles F Van Loan. Matrix Computations, volume 3. JHU Press, 2012.
blank         | 
ref           | [76] Willy JF Govaerts. Numerical Methods for Bifurcations of Dynamical Equilibria, volume 66.
              |     Siam, 2000.
blank         | 
ref           | [77] Sanjay Govindjee, Trevor Potter, and Jon Wilkening. Cyclic steady states of treaded rolling
              |     bodies. International Journal for Numerical Methods in Engineering, 99(3):203–220, 2014.
blank         | 
ref           | [78] Max D Gunzburger. Perspectives in Flow Control and Optimization, volume 5. SIAM, 2003.
blank         | 
ref           | [79] Martin H Gutknecht. Block Krylov space methods for linear systems with multiple right-hand
              |     sides: an introduction. 2006.
blank         | 
ref           | [80] Raphael T Haftka and Z Mroz. First-and second-order sensitivity analysis of linear and non-
              |     linear structures. AIAA Journal, 24(7):1187–1192, 1986.
meta          | BIBLIOGRAPHY                                                                                   307
blank         | 
              | 
              | 
ref           | [81] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness:
              |     Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
              |     53(2):217–288, 2011.
blank         | 
ref           | [82] Kathryn Harriman, DJ Gavaghan, and Endre Suli. The importance of adjoint consistency in
              |     the approximation of linear functionals using the discontinuous Galerkin finite element method.
              |     Technical report, 2004.
blank         | 
ref           | [83] Kathryn Harriman, Paul Houston, Bill Senior, and Endre Suli. hp-version discontinuous
              |     Galerkin methods with interior penalty for partial differential equations with nonnegative
              |     characteristic form. Technical report, 2002.
blank         | 
ref           | [84] Ralf Hartmann. Adjoint consistency analysis of discontinuous Galerkin discretizations. SIAM
              |     Journal on Numerical Analysis, 45(6):2671–2696, 2007.
blank         | 
ref           | [85] Alexander Hay, Imran Akhtar, and Jeff T Borggaard. On the use of sensitivity analysis
              |     in model reduction to predict flows for varying inflow conditions. International Journal for
              |     Numerical Methods in Fluids, 68(1):122–134, 2012.
blank         | 
ref           | [86] Alexander Hay, Jeff Borggaard, Imran Akhtar, and Dominique Pelletier. Reduced-order models
              |     for parameter dependent geometries based on shape sensitivity analysis. Journal of Compu-
              |     tational Physics, 229(4):1327–1352, 2010.
blank         | 
ref           | [87] Alexander Hay, Jeffrey T Borggaard, and Dominique Pelletier. Local improvements to reduced-
              |     order models using sensitivity analysis of the proper orthogonal decomposition. Journal of
              |     Fluid Mechanics, 629:41–72, 2009.
blank         | 
ref           | [88] Beichang He, Omar Ghattas, and James F Antaki. Computational strategies for shape opti-
              |     mization of time–dependent Navier–Stokes flows. Technical report, Carnegie Mellon University,
              |     1997.
blank         | 
ref           | [89] J He and LJ Durlofsky. Constraint reduction procedures for reduced-order subsurface flow
              |     models based on pod–tpwl. International Journal for Numerical Methods in Engineering,
              |     103(1):1–30, 2015.
blank         | 
ref           | [90] Patrick Heimbach, Chris Hill, and Ralf Giering. An efficient exact adjoint of the parallel
              |     MIT general circulation model, generated via automatic differentiation. Future Generation
              |     Computer Systems, 21(8):1356–1371, 2005.
blank         | 
ref           | [91] Matthias Heinkenschloss. Formulation and analysis of a sequential quadratic programming
              |     method for the optimal dirichlet boundary control of Navier-Stokes flow. In Optimal Control,
              |     pages 178–203. Springer, 1998.
blank         | 
ref           | [92] Matthias Heinkenschloss and Denis Ridzal. A matrix-free trust-region SQP method for equality
              |     constrained optimization. SIAM Journal on Optimization, 24(3):1507–1541, 2014.
meta          | BIBLIOGRAPHY                                                                                  308
blank         | 
              | 
              | 
ref           |  [93] Matthias Heinkenschloss and Luis N Vicente. Analysis of inexact trust-region SQP algorithms.
              |      SIAM Journal on Optimization, 12(2):283–302, 2002.
blank         | 
ref           |  [94] William S Hemp. Optimum Structures. Clarendon Press, 1973.
blank         | 
ref           |  [95] Vincent Heuveline and Andrea Walther. Online checkpointing for parallel adjoint computation
              |      in PDEs: Application to goal-oriented adaptivity and flow control. In Euro-Par 2006 Parallel
              |      Processing, pages 689–699. Springer, 2006.
blank         | 
ref           |  [96] Michael Hinze, René Pinnau, Michael Ulbrich, and Stefan Ulbrich. Optimization with PDE
              |      Constraints, volume 23. Springer Science & Business Media, 2008.
blank         | 
ref           |  [97] Paul Houston and Endre Süli. hp-adaptive discontinuous galerkin finite element methods
              |      for first-order hyperbolic problems. SIAM Journal on Scientific Computing, 23(4):1226–1252,
              |      2001.
blank         | 
ref           |  [98] Sergio R Idelsohn and Alberto Cardona. A reduction method for nonlinear structural dynamic
              |      analysis. Computer Methods in Applied Mechanics and Engineering, 49(3):253–279, 1985.
blank         | 
ref           |  [99] M Hasan Imam. Three-dimensional shape optimization. International Journal for Numerical
              |      Methods in Engineering, 18(5):661–673, 1982.
blank         | 
ref           | [100] Antony Jameson. Aerodynamic design via control theory. Journal of Scientific Computing,
              |      3(3):233–260, 1988.
blank         | 
ref           | [101] Ian Jolliffe. Principal Component Analysis. Wiley Online Library, 2002.
blank         | 
ref           | [102] Martin Jones and Nail K Yamaleev. Adjoint based shape and kinematics optimization of
              |      flapping wing propulsive efficiency. In 43rd AIAA Fluid Dynamics Conference. San Diego,
              |      CA, 2013.
blank         | 
ref           | [103] HB KELLER. Numerical Methods in Bifurcation Problems. Springer-Verlag, 1987.
blank         | 
ref           | [104] CT Kelley and David E Keyes. Convergence analysis of pseudo-transient continuation. SIAM
              |      Journal on Numerical Analysis, 35(2):508–523, 1998.
blank         | 
ref           | [105] CT Kelley, Li-Zhi Liao, Liqun Qi, Moody T Chu, JP Reese, and C Winton. Projected pseudo-
              |      transient continuation. 2007.
blank         | 
ref           | [106] Dana A Knoll and David E Keyes. Jacobian-free Newton–Krylov methods: a survey of ap-
              |      proaches and applications. Journal of Computational Physics, 193(2):357–397, 2004.
blank         | 
ref           | [107] Drew P Kouri. An approach for the adaptive solution of optimization problems governed by
              |      partial differential equations with uncertain coefficients. Technical report, DTIC Document,
              |      2012.
meta          | BIBLIOGRAPHY                                                                                  309
blank         | 
              | 
              | 
ref           | [108] Drew P Kouri, Matthias Heinkenschloss, Denis Ridzal, and Bart G van Bloemen Waanders.
              |      A trust-region algorithm with adaptive stochastic collocation for PDE optimization under
              |      uncertainty. SIAM Journal on Scientific Computing, 35(4):A1847–A1879, 2013.
blank         | 
ref           | [109] Drew P Kouri, Matthias Heinkenschloss, Denis Ridzal, and Bart G van Bloemen Waanders.
              |      Inexact objective function evaluations in a trust-region algorithm for PDE-constrained op-
              |      timization under uncertainty. SIAM Journal on Scientific Computing, 36(6):A3011–A3029,
              |      2014.
blank         | 
ref           | [110] Drew Philip Kouri and Thomas M Surowiec. Risk-averse PDE-constrained optimization us-
              |      ing the conditional value-at-risk. Technical report, Sandia National Laboratories (SNL-NM),
              |      Albuquerque, NM (United States), 2014.
blank         | 
ref           | [111] J-P Kruth, Ming-Chuan Leu, and T Nakagawa. Progress in additive manufacturing and rapid
              |      prototyping. CIRP Annals-Manufacturing Technology, 47(2):525–540, 1998.
blank         | 
ref           | [112] Peter A Kuchment. Floquet Theory for Partial Differential Equations, volume 60. Birkhäuser,
              |      2012.
blank         | 
ref           | [113] Karl Kunisch and Stefan Volkwein. Proper orthogonal decomposition for optimality systems.
              |      ESAIM: Mathematical Modelling and Numerical Analysis, 42(01):1–23, 2008.
blank         | 
ref           | [114] Toni Lassila and Gianluigi Rozza. Parametric free-form shape design with PDE models and re-
              |      duced basis method. Computer Methods in Applied Mechanics and Engineering, 199(23):1583–
              |      1592, 2010.
blank         | 
ref           | [115] Patrick Allen LeGresley. Application of proper orthogonal decomposition (POD) to design
              |      decomposition methods. PhD thesis, Citeseer, 2005.
blank         | 
ref           | [116] Friedemann Leibfritz and Ekkehard W Sachs. Inexact SQP interior point methods and large
              |      scale optimal control problems. SIAM Journal on Control and Optimization, 38(1):272–293,
              |      1999.
blank         | 
ref           | [117] Chad Lieberman, Karen Willcox, and Omar Ghattas. Parameter and state model reduction for
              |      large-scale statistical inverse problems. SIAM Journal on Scientific Computing, 32(5):2523–
              |      2542, 2010.
blank         | 
ref           | [118] Chi-Kun Lin. On the incompressible limit of the compressible Navier-Stokes equations. Com-
              |      munications in Partial Differential Equations, 20(3-4):677–707, 1995.
blank         | 
ref           | [119] Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale opti-
              |      mization. Mathematical Programming, 45(1-3):503–528, 1989.
blank         | 
ref           | [120] Trent Lukaczyk, Francisco Palacios, Juan J Alonso, and P Constantine. Active subspaces for
              |      shape optimization. In Proceedings of the 10th AIAA Multidisciplinary Design Optimization
              |      Conference, pages 1–18, 2014.
meta          | BIBLIOGRAPHY                                                                                  310
blank         | 
              | 
              | 
ref           | [121] L Machiels, Y Maday, and AT Patera. Output bounds for reduced-order approximations of el-
              |      liptic partial differential equations. Computer Methods in Applied Mechanics and Engineering,
              |      190(26):3413–3426, 2001.
blank         | 
ref           | [122] Yvon Maday and Einar M Rønquist. A reduced-basis element method. Journal of scientific
              |      computing, 17(1-4):447–459, 2002.
blank         | 
ref           | [123] Charles A Mader, JR RA Martins, Juan J Alonso, and E Van Der Weide. ADjoint: An
              |      approach for the rapid development of discrete adjoint solvers. AIAA Journal, 46(4):863–873,
              |      2008.
blank         | 
ref           | [124] Karthik Mani and Dimitri J Mavriplis.      Unsteady discrete adjoint formulation for two-
              |      dimensional flow problems with deforming meshes. AIAA Journal, 46(6):1351–1364, 2008.
blank         | 
ref           | [125] Andrea Manzoni, Alfio Quarteroni, and Gianluigi Rozza. Shape optimization for viscous flows
              |      by reduced basis methods and free-form deformation. International Journal for Numerical
              |      Methods in Fluids, 70(5):646–670, 2012.
blank         | 
ref           | [126] V Maple. Waterloo MAPLE software. University of Waterloo, Version, 5, 1994.
blank         | 
ref           | [127] K Maute, M Nikbay, and C Farhat. Sensitivity analysis and design optimization of three-
              |      dimensional non-linear aeroelastic systems by the adjoint method. International Journal for
              |      Numerical Methods in Engineering, 56(6):911–933, 2003.
blank         | 
ref           | [128] K Maute and M Raulli. FEM—optimization module and SDESIGN user guides, 2006.
blank         | 
ref           | [129] Kurt Maute, Melike Nikbay, and Charbel Farhat. Coupled analytical sensitivity analysis and
              |      optimization of three-dimensional nonlinear aeroelastic systems. AIAA Journal, 39(11):2051–
              |      2061, 2001.
blank         | 
ref           | [130] Dimitri J Mavriplis. Discrete adjoint-based approach for optimization problems on three-
              |      dimensional unstructured meshes. AIAA Journal, 45(4):741–750, 2007.
blank         | 
ref           | [131] GN Mercer and AJ Roberts. Standing waves in deep water: Their stability and extreme form.
              |      Physics of Fluids A: Fluid Dynamics (1989-1993), 4(2):259–269, 1992.
blank         | 
ref           | [132] Asitav Mishra, Karthik Mani, Dimitri Mavriplis, and Jay Sitaraman. Time dependent adjoint-
              |      based optimization for coupled fluid–structure problems. Journal of Computational Physics,
              |      292:253–271, 2015.
blank         | 
ref           | [133] Jorge J Moré. Recent developments in algorithms and software for trust region methods.
              |      Springer, 1983.
blank         | 
ref           | [134] Matthias Morzfeld, Xuemin Tu, Jon Wilkening, and Alexandre Chorin. Parameter estimation
              |      by implicit sampling. Communications in Applied Mathematics and Computational Science,
              |      10(2):205–225, 2015.
meta          | BIBLIOGRAPHY                                                                                     311
blank         | 
              | 
              | 
ref           | [135] Siva Nadarajah and Antony Jameson. A comparison of the continuous and discrete adjoint
              |      approach to automatic aerodynamic optimization. AIAA Paper, 667:2000, 2000.
blank         | 
ref           | [136] Siva K Nadarajah and Antony Jameson. Optimum shape design for unsteady flows with
              |      time-accurate continuous and discrete adjoint method. AIAA Journal, 45(7):1478–1491, 2007.
blank         | 
ref           | [137] Guy Narkiss and Michael Zibulevsky. Sequential Subspace Optimization Method for Large-Scale
              |      Unconstrained Problems. Technion-IIT, Department of Electrical Engineering, 2005.
blank         | 
ref           | [138] James C Newman III, Arthur C Taylor III, Richard W Barnwell, Perry A Newman, and Gene
              |      J-W Hou. Overview of sensitivity analysis and shape optimization for complex aerodynamic
              |      configurations. Journal of Aircraft, 36(1):87–96, 1999.
blank         | 
ref           | [139] Nathan Mortimore Newmark. A method of computation for structural dynamics. In Proc.
              |      ASCE, volume 85, pages 67–94, 1959.
blank         | 
ref           | [140] Eric J Nielsen, Boris Diskin, and Nail K Yamaleev. Discrete adjoint-based design optimization
              |      of unsteady turbulent flows on dynamic unstructured grids. AIAA Journal, 48(6):1195–1206,
              |      2010.
blank         | 
ref           | [141] Fabio Nobile, Raul Tempone, and CG Webster. The analysis of a sparse grid stochastic
              |      collocation method for partial differential equations with high-dimensional random input data.
              |      Technical report, Technical report, Sandia National Laboratories, 2007. SAND REPORT,
              |      2007.
blank         | 
ref           | [142] Fabio Nobile, Raúl Tempone, and Clayton G Webster. A sparse grid stochastic collocation
              |      method for partial differential equations with random input data. SIAM Journal on Numerical
              |      Analysis, 46(5):2309–2345, 2008.
blank         | 
ref           | [143] Jorge Nocedal and Stephen Wright. Numerical Optimization. Springer Science & Business
              |      Media, 2006.
blank         | 
ref           | [144] Erich Novak and Klaus Ritter. High dimensional integration of smooth functions over cubes.
              |      Numerische Mathematik, 75(1):79–97, 1996.
blank         | 
ref           | [145] Erich Novak and Klaus Ritter. Simple cubature formulas with high polynomial exactness.
              |      Constructive Approximation, 15(4):499–522, 1999.
blank         | 
ref           | [146] Erich Novak and Henryk Woźniakowski. Tractability of Multivariate Problems: Standard in-
              |      formation for Functionals, volume 12. European Mathematical Society, 2010.
blank         | 
ref           | [147] Carlos E Orozco and ON Ghattas. Massively parallel aerodynamic shape optimization. Com-
              |      puting Systems in Engineering, 3(1):311–320, 1992.
blank         | 
ref           | [148] Akira Oyama, Yoshiyuki Okabe, Koji Shimoyama, and Kozo Fujii. Aerodynamic multiobjec-
              |      tive design exploration of a flapping airfoil using a Navier-Stokes solver. Journal of Aerospace
              |      Computing, Information, and Communication, 6(3):256–270, 2009.
meta          | BIBLIOGRAPHY                                                                                  312
blank         | 
              | 
              | 
ref           | [149] Anthony T Patera and Gianluigi Rozza. Reduced basis approximation and a posteriori error
              |      estimation for parametrized partial differential equations. Technical report, (C) MIT, Mas-
              |      sachusetts Institute of Technology, 2007.
blank         | 
ref           | [150] Jaime Peraire and P-O Persson. The Compact Discontinuous Galerkin (CDG) method for
              |      elliptic problems. SIAM Journal on Scientific Computing, 30(4):1806–1824, 2008.
blank         | 
ref           | [151] Ruben E Perez, Peter W Jansen, and Joaquim RRA Martins. pyOpt: a Python-based object-
              |      oriented framework for nonlinear constrained optimization. Structural and Multidisciplinary
              |      Optimization, 45(1):101–118, 2012.
blank         | 
ref           | [152] P-O Persson, J Bonet, and J Peraire. Discontinuous Galerkin solution of the Navier–Stokes
              |      equations on deformable domains. Computer Methods in Applied Mechanics and Engineering,
              |      198(17):1585–1595, 2009.
blank         | 
ref           | [153] P-O Persson and Jaime Peraire. Newton-GMRES preconditioning for discontinuous Galerkin
              |      discretizations of the Navier-Stokes equations.    SIAM Journal on Scientific Computing,
              |      30(6):2709–2733, 2008.
blank         | 
ref           | [154] Per-Olof Persson. Scalable parallel Newton-Krylov solvers for discontinuous Galerkin dis-
              |      cretizations. AIAA Paper, 606:2009, 2009.
blank         | 
ref           | [155] Per-Olof Persson and Jaime Peraire. Curved mesh generation and mesh refinement using
              |      Lagrangian solid mechanics. In Proceedings of the 47th AIAA Aerospace Sciences Meeting and
              |      Exhibit, volume 204, 2009.
blank         | 
ref           | [156] Knut Petras. On the smolyak cubature error for analytic functions. Advances in Computational
              |      Mathematics, 12(1):71–93, 2000.
blank         | 
ref           | [157] Knut Petras. Smolyak cubature of given polynomial degree with few nodes for increasing
              |      dimension. Numerische Mathematik, 93(4):729–753, 2003.
blank         | 
ref           | [158] Max F Platzer, Kevin D Jones, John Young, and JC S. Lai. Flapping wing aerodynamics:
              |      progress and challenges. AIAA Journal, 46(9):2136–2149, 2008.
blank         | 
ref           | [159] MJD Powell. Convergence properties of a class of minimization algorithms. Nonlinear Pro-
              |      gramming, 2(0):1–27, 1975.
blank         | 
ref           | [160] Alfio Quarteroni and Gianluigi Rozza. Optimal control and shape optimization of aorto-
              |      coronaric bypass anastomoses.     Mathematical Models and Methods in Applied Sciences,
              |      13(12):1801–1823, 2003.
blank         | 
ref           | [161] Louis B Rall. Automatic differentiation: Techniques and applications. 1981.
blank         | 
ref           | [162] Ravi Ramamurti and William Sandberg. Simulation of flow about flapping airfoils using finite
              |      element incompressible flow solver. AIAA Journal, 39(2):253–260, 2001.
meta          | BIBLIOGRAPHY                                                                                       313
blank         | 
              | 
              | 
ref           | [163] James Reuther, Juan Jose Alonso, Mark J Rimlinger, and Antony Jameson. Aerodynamic
              |      shape optimization of supersonic aircraft configurations via an adjoint formulation on dis-
              |      tributed memory parallel computers. Computers & Fluids, 28(4):675–700, 1999.
blank         | 
ref           | [164] James Reuther, Antony Jameson, James Farmer, Luigi Martinelli, and David Saunders. Aero-
              |      dynamic shape optimization of complex aircraft configurations via an adjoint formulation.
              |      AIAA paper, 94, 1996.
blank         | 
ref           | [165] Michal Rewienski and Jacob White. A trajectory piecewise-linear approach to model order
              |      reduction and fast simulation of nonlinear circuits and micromachined devices. IEEE Trans-
              |      actions on computer-aided design of integrated circuits and systems, 22(2):155–170, 2003.
blank         | 
ref           | [166] Denis Ridzal. Trust-region SQP methods with inexact linear system solves for large-scale
              |      optimization. PhD thesis, Citeseer, 2006.
blank         | 
ref           | [167] TD Robinson, MS Eldred, KE Willcox, and R Haimes. Surrogate-based optimization us-
              |      ing multifidelity models with variable parameterization and corrected space mapping. AIAA
              |      Journal, 46(11):2814–2822, 2008.
blank         | 
ref           | [168] Theresa Dawn Robinson. Surrogate-based optimization using multifidelity models with variable
              |      parameterization. PhD thesis, Massachusetts Institute of Technology, 2007.
blank         | 
ref           | [169] Philip L Roe. Approximate Riemann solvers, parameter vectors, and difference schemes.
              |      Journal of Computational Physics, 43(2):357–372, 1981.
blank         | 
ref           | [170] Sabrina Rogg. Trust Region POD for Optimal Boundary Control of a Semilinear Heat Equa-
              |      tion. PhD thesis, University of Trier, 2014.
blank         | 
ref           | [171] Gianluigi Rozza. On optimization, control and shape design of an arterial bypass. International
              |      Journal for Numerical Methods in Fluids, 47(10-11):1411–1419, 2005.
blank         | 
ref           | [172] Gianluigi Rozza. Shape design by optimal flow control and reduced basis techniques. PhD
              |      thesis, EPFL, 2005.
blank         | 
ref           | [173] Gianluigi Rozza, DBP Huynh, and Anthony T Patera. Reduced basis approximation and a
              |      posteriori error estimation for affinely parametrized elliptic coercive partial differential equa-
              |      tions. Archives of Computational Methods in Engineering, 15(3):229–275, 2008.
blank         | 
ref           | [174] Gianluigi Rozza and Andrea Manzoni. Model order reduction by geometrical parametrization
              |      for shape optimization in computational fluid dynamics. In Proceedings of the ECCOMAS
              |      CFD 2010, V European Conference on Computational Fluid Dynamics, number EPFL-CONF-
              |      148535, 2010.
blank         | 
ref           | [175] D. Ryckelynck. A priori hyperreduction method: an adaptive approach. Journal of Computa-
              |      tional Physics, 202(1):346–366, 2005.
meta          | BIBLIOGRAPHY                                                                                    314
blank         | 
              | 
              | 
ref           | [176] Chris H Rycroft and Jon Wilkening. Computation of three-dimensional standing water waves.
              |      Journal of Computational Physics, 255:612–638, 2013.
blank         | 
ref           | [177] Jamshid A Samareh. A survey of shape parameterization techniques. In NASA Conference
              |      Publication, pages 333–344. Citeseer, 1999.
blank         | 
ref           | [178] Claudia Schillings. Optimal aerodynamic design under uncertainties. PhD thesis, PhD thesis,
              |      Fb–IV, Mathematik, Universität Trier, D–54286 Trier, Germany, 2010.
blank         | 
ref           | [179] Thomas W Sederberg and Scott R Parry. Free-form deformation of solid geometric models.
              |      ACM SIGGRAPH computer graphics, 20(4):151–160, 1986.
blank         | 
ref           | [180] Ole Sigmund. Design of multiphysics actuators using topology optimization–part I: One-
              |      material structures. Computer Methods in Applied Mechanics and Engineering, 190(49):6577–
              |      6604, 2001.
blank         | 
ref           | [181] Ole Sigmund and Kurt Maute. Topology optimization approaches. Structural and Multidisci-
              |      plinary Optimization, 48(6):1031–1055, 2013.
blank         | 
ref           | [182] Valeria Simoncini and Efstratios Gallopoulos. An iterative method for nonsymmetric systems
              |      with multiple right-hand sides. SIAM Journal on Scientific Computing, 16(4):917–933, 1995.
blank         | 
ref           | [183] L. Sirovich. Turbulence and the dynamics of coherent structures. I-coherent structures. II-
              |      symmetries and transformations. III-dynamics and scaling. Quarterly of Applied Mathematics,
              |      45:561–571, 1987.
blank         | 
ref           | [184] Sergey A Smolyak. Quadrature and interpolation formulas for tensor products of certain classes
              |      of functions. In Dokl. Akad. Nauk SSSR, volume 4, page 123, 1963.
blank         | 
ref           | [185] Roman Srzednicki.     Periodic and bounded solutions in blocks for time-periodic nonau-
              |      tonomous ordinary differential equations. Nonlinear Analysis: Theory, Methods & Appli-
              |      cations, 22(6):707–737, 1994.
blank         | 
ref           | [186] Eka Suwartadi, Stein Krogstad, and Bjarne Foss. Adjoint-based surrogate optimization of oil
              |      reservoir water flooding. Optimization and Engineering, 16(2):441–481, 2015.
blank         | 
ref           | [187] Jeffrey P Thomas, Kenneth C Hall, and Earl H Dowell. Discrete adjoint approach for modeling
              |      unsteady aerodynamic design sensitivities. AIAA Journal, 43(9):1931–1936, 2005.
blank         | 
ref           | [188] Hanne Tiesler, Robert M Kirby, Dongbin Xiu, and Tobias Preusser. Stochastic collocation
              |      for optimal control problems with stochastic PDE constraints. SIAM Journal on Control and
              |      Optimization, 50(5):2659–2682, 2012.
blank         | 
ref           | [189] Ph L Toint. Global convergence of a class of trust-region methods for nonconvex minimization
              |      in Hilbert space. IMA Journal of Numerical Analysis, 8(2):231–252, 1988.
meta          | BIBLIOGRAPHY                                                                                     315
blank         | 
              | 
              | 
ref           | [190] Fredi Tröltzsch. Optimal control of partial differential equations. Graduate studies in mathe-
              |      matics, 112, 2010.
blank         | 
ref           | [191] Ismail H Tuncer and Mustafa Kaya. Optimization of flapping airfoils for maximum thrust and
              |      propulsive efficiency. AIAA Journal, 43(11):2329–2336, 2005.
blank         | 
ref           | [192] Nico P van Dijk, K Maute, M Langelaar, and F Van Keulen. Level-set methods for structural
              |      topology optimization: a review. Structural and Multidisciplinary Optimization, 48(3):437–
              |      472, 2013.
blank         | 
ref           | [193] Marnix P van Schrojenstein Lantman and Krzysztof Fidkowski. Adjoint-based optimization of
              |      flapping kinematics in viscous flows. In 21st AIAA Computaional Fluid Dynamics Conference,
              |      2013.
blank         | 
ref           | [194] Stefan Vandewalle and Robert Piessens.       Efficient parallel algorithms for solving initial-
              |      boundary value and time-periodic parabolic partial differential equations. SIAM Journal on
              |      Scientific and Statistical Computing, 13(6):1330–1346, 1992.
blank         | 
ref           | [195] Jean Virieux and Stéphane Operto. An overview of full-waveform inversion in exploration
              |      geophysics. Geophysics, 74(6):WCC1–WCC26, 2009.
blank         | 
ref           | [196] Divakar Viswanath. Recurrent motions within plane Couette turbulence. Journal of Fluid
              |      Mechanics, 580:339–358, 2007.
blank         | 
ref           | [197] Zhi Wang, IM Navon, FX Le Dimet, and X Zou. The second order adjoint analysis: theory
              |      and applications. Meteorology and Atmospheric Physics, 50(1-3):3–20, 1992.
blank         | 
ref           | [198] Kyle Washabaugh. Faster Fidelity For Better Design: A Scalable Model Order Reduction
              |      Framework For Steady Aerodynamic Design Applications. PhD thesis, Stanford University,
              |      2016.
blank         | 
ref           | [199] Clayton G Webster. Sparse grid stochastic collocation techniques for the numerical solution of
              |      partial differential equations with random input data. Flordia State University, 2007.
blank         | 
ref           | [200] Jon Wilkening. Breakdown of self-similarity at the crests of large-amplitude standing water
              |      waves. Physical Review Letters, 107(18):184501, 2011.
blank         | 
ref           | [201] Jon Wilkening and Jia Yu. Overdetermined shooting methods for computing standing water
              |      waves with spectral accuracy. Computational Science & Discovery, 5(1):014017, 2012.
blank         | 
ref           | [202] Matthew O Williams, Jon Wilkening, Eli Shlizerman, and J Nathan Kutz. Continuation of
              |      periodic solutions in the waveguide array mode-locked laser. Physica D: Nonlinear Phenomena,
              |      240(22):1791–1804, 2011.
blank         | 
ref           | [203] Kaufui V Wong and Aldo Hernandez. A review of additive manufacturing. ISRN Mechanical
              |      Engineering, 2012, 2012.
meta          | BIBLIOGRAPHY                                                                                 316
blank         | 
              | 
              | 
ref           | [204] Dongbin Xiu and Jan S Hesthaven. High-order collocation methods for differential equations
              |      with random inputs. SIAM Journal on Scientific Computing, 27(3):1118–1139, 2005.
blank         | 
ref           | [205] Nail K Yamaleev, Boris Diskin, and Eric J Nielsen. Adjoint-based methodology for time-
              |      dependent optimization. AIAA Paper, 5857:2008, 2008.
blank         | 
ref           | [206] Nail K Yamaleev, Boris Diskin, and Eric J Nielsen. Local-in-time adjoint-based method for
              |      design optimization of unsteady flows. Journal of Computational Physics, 229(14):5394–5407,
              |      2010.
blank         | 
ref           | [207] Ya-Xiang Yuan. Subspace methods for large scale nonlinear equations and nonlinear least
              |      squares. Optimization and Engineering, 10(2):207–218, 2009.
blank         | 
ref           | [208] Yao Yue and Karl Meerbergen. Accelerating optimization of parametric linear systems by
              |      model order reduction. SIAM Journal on Optimization, 23(2):1344–1370, 2013.
blank         | 
ref           | [209] Matthew J. Zahr, Kevin Carlberg, David Amsallem, and Charbel Farhat. Comparison of
              |      model reduction techniques on high-fidelity linear and nonlinear electrical, mechanical, and
              |      biological systems. Technical report, University of California, Berkeley, 2010.
blank         | 
ref           | [210] Matthew J. Zahr and Charbel Farhat. Progressive construction of a parametric reduced-order
              |      model for PDE-constrained optimization. International Journal for Numerical Methods in
              |      Engineering, 102(5):1111–1135, 2015.
blank         | 
ref           | [211] Matthew J. Zahr and Per-Olof Persson. An adjoint method for a high-order discretization of
              |      deforming domain conservation laws for optimization of flow problems. Journal of Computa-
              |      tional Physics, In review, 2016.
blank         | 
ref           | [212] Matthew J. Zahr, Per-Olof Persson, and John Wilkening. A fully discrete adjoint method
              |      for optimization of flow problems on deforming domains with time-periodicity constraints.
              |      Computers & Fluids, 2016.
blank         | 
ref           | [213] Tomás Zegard and Glaucio H Paulino. Bridging topology optimization and additive manufac-
              |      turing. Structural and Multidisciplinary Optimization, pages 1–18, 2015.
blank         | 
ref           | [214] Kemin Zhou, John Comstock Doyle, Keith Glover, et al. Robust and Optimal Control, vol-
              |      ume 40. Prentice Hall, New Jersey, 1996.
blank         | 
ref           | [215] Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B:
              |      Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on
              |      Mathematical Software (TOMS), 23(4):550–560, 1997.
blank         | 
ref           | [216] J Carsten Ziems and Stefan Ulbrich. Adaptive multilevel inexact SQP methods for PDE-
              |      constrained optimization. SIAM Journal on Optimization, 21(1):1–40, 2011.
blank         | 
